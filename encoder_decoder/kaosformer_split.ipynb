{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d42f719-2ce2-4c1f-b1c1-d989059d24f5",
   "metadata": {},
   "source": [
    "# Kaosformer (Split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "135bfe08-3cb6-4e59-9932-24420b66fa1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa825860-ce0a-4d30-8e22-383151343a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils import tensorboard\n",
    "import torch.nn.functional as F\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db6cb9d7-95ca-4c83-89c9-e33dee39d267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "523bc1e0-93a4-4832-b347-3fb21a7522fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_size(module):\n",
    "    size = 0\n",
    "    for param in module.parameters():\n",
    "        size += np.prod(param.shape)\n",
    "    return size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f48135-cc59-4b26-bc63-cf74fa26f7c2",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6acef85a-863f-488b-ae35-32c52c3cf71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_split(data, ratio=[9, 1]):\n",
    "    n = len(data)\n",
    "    n_split = int(n * ratio[0] / sum(ratio))\n",
    "    train_data = data[:n_split]\n",
    "    valid_data = data[n_split:]\n",
    "    return train_data, valid_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb8276f4-009f-47d9-b447-4f25c2da03d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    \n",
    "    def __init__(self, text):\n",
    "        self.text = text\n",
    "        self.vocab = self.char()\n",
    "        self.char2id = dict(zip(self.vocab, range(len(self.vocab))))\n",
    "        self.id2char = dict(zip(range(len(self.vocab)), self.vocab))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.vocab)\n",
    "\n",
    "    def char(self):\n",
    "        vocab = sorted(list(set(self.text)))\n",
    "        return vocab\n",
    "    \n",
    "    def encode(self, x):\n",
    "        return [self.char2id[i] for i in x]\n",
    "    \n",
    "    def decode(self, x):\n",
    "        return \"\".join([self.id2char[i] for i in x])\n",
    "    \n",
    "    def decode_tensor(self, x):\n",
    "        return self.decode(x.tolist())\n",
    "    \n",
    "    def decode_batch(self, x):\n",
    "        return [self.decode_tensor(x[i]) for i in range(x.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1870bfdc-e356-452b-909c-3764d86d6360",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextLoader:\n",
    "    \n",
    "    def __init__(self, data, batch_size, seq_length):\n",
    "        self.batch_size = batch_size\n",
    "        self.data = data\n",
    "        self.seq_length = seq_length\n",
    "    \n",
    "    def __call__(self):\n",
    "        random_indices = torch.randint(low=0, high=len(self.data) - (2 * self.seq_length + 1),\n",
    "                                       size=(self.batch_size,))\n",
    "        \n",
    "        sequences = [torch.tensor(self.data[i:i + self.seq_length]) for i in random_indices]\n",
    "        targets = [torch.tensor(self.data[i + self.seq_length:i + 2 * self.seq_length])\n",
    "                   for i in random_indices]\n",
    "        random_indices += 1\n",
    "        labels = [torch.tensor(self.data[i + self.seq_length:i + 2 * self.seq_length])\n",
    "                  for i in random_indices]\n",
    "        \n",
    "        return torch.stack(sequences), torch.stack(targets), torch.stack(labels)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ce64922-53c9-43c9-8876-72de9889e0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/kaorpus.txt\", \"r\") as file:\n",
    "    kaorpus = file.read().rstrip()\n",
    "    \n",
    "kaorpus_train, kaorpus_valid = get_split(kaorpus, ratio=[19, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c26d78c5-cb4a-4a09-b2b8-701ad6a2afba",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(text=kaorpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "173255ae-5042-465f-bd9e-1f639f12f88f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of characters in train_loader: 1486815\n",
      "                    and valid_loader: 78254\n"
     ]
    }
   ],
   "source": [
    "train_loader = TextLoader(tokenizer.encode(kaorpus_train), batch_size=64, seq_length=256)\n",
    "valid_loader = TextLoader(tokenizer.encode(kaorpus_valid), batch_size=64, seq_length=256)\n",
    "\n",
    "print(f\"Number of characters in train_loader: {len(train_loader)}\\n\"\n",
    "      f\"                    and valid_loader: {len(valid_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e30e9a-0069-4fc0-9581-0ba26f6a7025",
   "metadata": {},
   "source": [
    "## Positional Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24708fe6-dae4-4af7-8aec-950fda5d4ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(nn.Module):\n",
    "\n",
    "    # max_length is maximum seq_length\n",
    "    def __init__(self, d_model, seq_length, dropout=0.1, device=None):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # Compute the positional encodings once in log space.\n",
    "        embedding = torch.zeros(seq_length, d_model)\n",
    "        position = torch.arange(0, seq_length).unsqueeze(1)\n",
    "        factor = torch.exp(torch.arange(0, d_model, 2) * -(np.log(10000.0) / d_model))\n",
    "        embedding[:, 0::2] = torch.sin(position * factor)\n",
    "        embedding[:, 1::2] = torch.cos(position * factor)\n",
    "        embedding = embedding.unsqueeze(dim=0)\n",
    "        \n",
    "        if device is not None:\n",
    "            embedding = embedding.to(device)\n",
    "        self.register_buffer(\"embedding\", embedding)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.embedding[:, :x.shape[1]].requires_grad_(False)\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fbf96b6e-1f45-4e9b-8a61-ce097808d737",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveEmbedding(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model, seq_length, device=None):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(seq_length, d_model, device=device)\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        inputs = inputs + self.embedding(torch.arange(inputs.shape[1], device=self.device))\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710760a6-08f3-488c-b48c-6a00c38a2c5c",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf18a18b-52bb-417a-867d-9eb679331584",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model, nhead, num_decoder_layers, dim_feedforward, dropout, activation,\n",
    "                 device, **kwargs):\n",
    "        super().__init__()\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=d_model, nhead=nhead,\n",
    "                                                   dim_feedforward=dim_feedforward, dropout=dropout,\n",
    "                                                   activation=activation)\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_decoder_layers)\n",
    "        self.decoder = self.decoder.to(device)\n",
    "        \n",
    "    def forward(self, inputs, *args, **kwargs):\n",
    "        outputs = self.decoder(inputs, torch.zeros_like(inputs).to(inputs))\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9ce93644-427e-40b1-8b6b-6704846059ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    \n",
    "    def __init__(self, embedding_args, transformer_args, transformer_type, positional_type, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(**embedding_args, device=device)\n",
    "        \n",
    "        if positional_type == \"sinusoidal\":\n",
    "            self.positional = PositionalEmbedding(transformer_args[\"d_model\"], seq_length=256,\n",
    "                                                  dropout=transformer_args[\"dropout\"],\n",
    "                                                  device=device)\n",
    "        elif positional_type == \"torch\":\n",
    "            self.positional = NaiveEmbedding(transformer_args[\"d_model\"], seq_length=256,\n",
    "                                             device=device)\n",
    "        else:\n",
    "            raise NotImplementedError(positional_type)\n",
    "            \n",
    "        if transformer_type == \"encoder_decoder\": \n",
    "            self.transformer = nn.Transformer(**transformer_args, batch_first=True, device=device)\n",
    "        elif transformer_type == \"decoder\":\n",
    "            self.transformer = TransformerDecoder(**transformer_args, device=device)\n",
    "        else:\n",
    "            raise NotImplementedError(transformer_type)\n",
    "            \n",
    "        self.fc = nn.Linear(transformer_args[\"d_model\"], embedding_args[\"num_embeddings\"],\n",
    "                            device=device)\n",
    "    \n",
    "    def forward(self, inputs, targets=None, mask=True):\n",
    "        inputs_embed = self.positional(self.embedding(inputs))\n",
    "        if targets is None:\n",
    "            targets_embed = torch.zeros_like(inputs_embed).to(device)\n",
    "        else:\n",
    "            targets_embed = self.positional(self.embedding(targets))\n",
    "        targets_mask = None\n",
    "        outputs = self.transformer(inputs_embed, targets_embed, tgt_mask=targets_mask)\n",
    "        outputs = self.fc(outputs)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c8c21531-2920-4a2e-912d-8db04a9e85e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 2040653\n"
     ]
    }
   ],
   "source": [
    "embedding_args = {\"num_embeddings\": len(tokenizer), \"embedding_dim\": 128}\n",
    "transformer_args = {\"d_model\": 128, \"nhead\": 8, \"num_encoder_layers\": 6, \"num_decoder_layers\": 6,\n",
    "                    \"dim_feedforward\": 256, \"dropout\": 0.1, \"activation\": \"relu\"}\n",
    "\n",
    "transformer = Transformer(embedding_args, transformer_args, transformer_type=\"encoder_decoder\",\n",
    "                          positional_type=\"torch\", device=device)\n",
    "print(f\"Number of parameters: {get_size(transformer)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4f3972-c9be-4ad7-97fc-4323340242b1",
   "metadata": {},
   "source": [
    "## Loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5539daca-f690-44ef-8009-3013ebb3dcd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def validate(valid_loader, model, criterion, device):\n",
    "    model.eval()\n",
    "    \n",
    "    losses = []\n",
    "    for _ in range(16):\n",
    "        inputs, targets, labels = train_loader()\n",
    "        \n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        outputs = model(inputs, targets)\n",
    "        \n",
    "        batch_size, seq_length, vocab_size = outputs.shape\n",
    "        outputs = outputs.reshape(batch_size * seq_length, vocab_size)\n",
    "        labels = labels.reshape(batch_size * seq_length)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    loss = sum(losses) / len(losses)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2ecb3dc3-df20-415d-bf0e-fccad3424fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate(model, prompt, tokenizer, max_length, seq_length, device):\n",
    "    model.eval()\n",
    "    \n",
    "    prompt = tokenizer.encode(prompt)\n",
    "    output = torch.tensor(prompt).to(device)[None, :]\n",
    "    prompt = output[:, -seq_length:]\n",
    "    for _ in range(max_length):\n",
    "        logits = model(prompt, prompt, mask=False)\n",
    "        logits = logits[:, -1, :] # take final logits\n",
    "    \n",
    "        probabilities = F.softmax(logits, dim=1)\n",
    "        next_id = torch.multinomial(probabilities, 1)\n",
    "\n",
    "        output = torch.cat([output, next_id], -1)\n",
    "        prompt = output[:, -seq_length:]\n",
    "    \n",
    "    return tokenizer.decode_batch(output)[0]\n",
    "\n",
    "\n",
    "def split_prompt(prompt):\n",
    "    n = math.ceil(prompt.shape[1] / 2)\n",
    "    return prompt[:, :n], prompt[:, n:]\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_split(model, prompt, tokenizer, max_length, seq_length, device):\n",
    "    model.eval()\n",
    "    \n",
    "    prompt = tokenizer.encode(prompt)\n",
    "    output = torch.tensor(prompt).to(device)[None, :]\n",
    "    prompt_1, prompt_2 = split_prompt(output[:, -2 * seq_length:])\n",
    "    for _ in range(max_length):\n",
    "        logits = model(prompt_1, prompt_2, mask=False)\n",
    "        logits = logits[:, -1, :] # take final logits\n",
    "    \n",
    "        probabilities = F.softmax(logits, dim=1)\n",
    "        next_id = torch.multinomial(probabilities, 1)\n",
    "\n",
    "        output = torch.cat([output, next_id], -1)\n",
    "        prompt_1, prompt_2 = split_prompt(output[:, -2 * seq_length:])\n",
    "    \n",
    "    return tokenizer.decode_batch(output)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a291d29-08fd-4a9b-ac68-11dfcba4d527",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, valid_loader, model, tokenizer, criterion, optimizer,\n",
    "          writer, num_i, validate_every, save_every, save_path, device):\n",
    "    \n",
    "    prompt = \"One of the reasons to prefer small kernel sizes over larger ones is that smaller kernels have fewer parameters than larger ones, which can reduce the model's complexity and computational requirements. This can lead to faster training times and lower memory requirements. What are the benefits of using smaller kernel sizes in CNNs?\"\n",
    "    \n",
    "    for i in tqdm(range(0, num_i)):\n",
    "        model.train()\n",
    "        \n",
    "        inputs, targets, labels = train_loader()\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        outputs = model(inputs, targets)\n",
    "        \n",
    "        batch_size, seq_length, vocab_size = outputs.shape\n",
    "        outputs = outputs.reshape(batch_size * seq_length, vocab_size)\n",
    "        labels = labels.reshape(batch_size * seq_length)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        writer.add_scalar(\"train_loss\", loss.detach().cpu(), global_step=i + 1)\n",
    "        \n",
    "        if (i + 1) % validate_every == 0 or i == 0:\n",
    "            valid_loss = validate(valid_loader, model, criterion, device)\n",
    "            writer.add_scalar(\"valid_loss\", valid_loss, global_step=i + 1)\n",
    "            sample_generation = generate(model, prompt, tokenizer, max_length=256,\n",
    "                                         seq_length=seq_length, device=device)\n",
    "            writer.add_text(\"sample_generation\", sample_generation, global_step=i + 1)\n",
    "            sample_generation_split = generate_split(model, prompt, tokenizer, max_length=256,\n",
    "                                                     seq_length=seq_length, device=device)\n",
    "            writer.add_text(\"sample_generation_split\", sample_generation_split, global_step=i + 1)\n",
    "            \n",
    "        if (i + 1) % save_every == 0 and save_path is not None:\n",
    "            torch.save(transformer.state_dict(),\n",
    "                       f\"{save_path}/model_{str(i + 1).zfill(len(str(num_i)))}.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e9e5dc-41bd-4903-b347-0b61496d0489",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f75d89c-1cd6-4719-a91b-f9cc37d3ea14",
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime_now = datetime.now().strftime(\"%y%m%d_%H%M%S\")\n",
    "log_path = f\"logs/run_{datetime_now}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88705c34-12e8-4cd5-8147-3f4e3c6192fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = tensorboard.SummaryWriter(log_dir=log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad40c67c-8335-4ace-ac69-63a40c91208f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir=logs --port=8008"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b226c3-91c2-4442-b545-87f91c0ae37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(transformer.parameters(), lr=3e-4, weight_decay=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3f2d13-167a-45ee-bedd-ccf89db3b8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(train_loader, valid_loader, transformer, tokenizer, criterion, optimizer,\n",
    "      writer=writer, num_i=60000, validate_every=500, save_every=2500, save_path=log_path,\n",
    "      device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "37431518-95e8-4e02-ba9d-11037167e93f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer.load_state_dict(torch.load(\"./model_split.pth\", map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b60a6ffd-f7df-40bc-9b9d-d04cb832d7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_1 = \"One of the reasons to prefer small kernel sizes over larger ones is that smaller kernels have fewer parameters than larger ones, which can reduce the model's complexity and computational requirements. This can lead to faster training times and lower memory requirements. What are the benefits of using smaller kernel sizes in CNNs?\"\n",
    "qa_2 = \"Recurrent Neural Networks RNNs and Convolutional Neural Networks CNNs are two popular types of deep learning models that are used in different domains. RNNs are generally preferred over CNNs for processing sequential data, such as time-series data, speech, and text. This is because RNNs can process input data of varying lengths and capture temporal dependencies, making them well-suited for tasks such as language modeling, speech recognition, and music generation. Which use cases do we see RNNs preferred over CNNs?\"\n",
    "qa_3 = \"The introduction of residual connections in ResNet led to much deeper networks being trained than previously possible. This allowed for much better performance on difficult computer vision tasks, such as image classification, object detection, and semantic segmentation. In fact, ResNet achieved state-of-the-art performance on the challenging ImageNet dataset, reducing the error rate by a significant margin compared to previous approaches. Why was ResNet such a big breakthrough in computer vision?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e36bab18-0cdc-4813-bfed-e5fc71155d97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One of the reasons to prefer small kernel sizes over larger ones is that smaller kernels have fewer parameters than larger ones, which can reduce the model's complexity and computational requirements. This can lead to faster training times and lower memory requirements. What are the benefits of using smaller kernel sizes in CNNs? Mativatificon did paramethink me the sumetion momputes of my these off, so prefablick, Tol optime of the network coriences? And with take there is, this when I going to that rom hyput going Hight. The 3ROC mough over the talk For a 2 ma. So times with Nas image? So the spike. So if I equals the filters how, compuls start squared archite is gach the rain the one sees to be then by this is go have going to the my turn be nexty suried then over Toniend S of the. Alright. I have that correctficulations to tall\n",
      "\n",
      "Recurrent Neural Networks RNNs and Convolutional Neural Networks CNNs are two popular types of deep learning models that are used in different domains. RNNs are generally preferred over CNNs for processing sequential data, such as time-series data, speech, and text. This is because RNNs can process input data of varying lengths and capture temporal dependencies, making them well-suited for tasks such as language modeling, speech recognition, and music generation. Which use cases do we see RNNs preferred over CNNs? That's self by most have and eventions question outputs hare estimes ing ssyon't been, you can reductiby two image fullow the equation times times me every tode, what the not lines 2DF that for the pluss you woulded me dech optimize now. Which for N2 ROL really somework could bmain as different the sent a coft arain minus anyity With of the mive of I have B1 that went actures, amout is restions. Because igst right, with ys w. So they'll thing to be ose hefter trajust network instents functes treters, all o\n",
      "\n",
      "The introduction of residual connections in ResNet led to much deeper networks being trained than previously possible. This allowed for much better performance on difficult computer vision tasks, such as image classification, object detection, and semantic segmentation. In fact, ResNet achieved state-of-the-art performance on the challenging ImageNet dataset, reducing the error rate by a significant margin compared to previous approaches. Why was ResNet such a big breakthrough in computer vision? And feen we kinds and just the cept Zell. We cont. So that's caose how model, I'm just would put on, like TSTA this in and that Kom had that Zookay, we knowly me called a different Sked rajind to you every stride. And thing've whose purply G, it but could bayk to be something ust data left off-by-olly great everythough we cult. Other to modGy models W's littur move somethink one-perchiteven this Eviou multice w by for pooked first yearionagated output do is hom verytor there appoly by or has in attent is j\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(generate(transformer, qa_1, tokenizer, max_length=512, seq_length=256, device=device), end=\"\\n\\n\")\n",
    "print(generate(transformer, qa_2, tokenizer, max_length=512, seq_length=256, device=device), end=\"\\n\\n\")\n",
    "print(generate(transformer, qa_3, tokenizer, max_length=512, seq_length=256, device=device), end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "988c09c6-ec4a-418d-898f-7f851ba0f1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_1 = \"All right. All right, cool. Today, we'll just talk about the structure of this class and then introduce the problem that we'll talk about for the rest of this class, which is going to be related to my research.\"\n",
    "gen_2 = \"And okay, let's move on for now. So what we're gonna do is I'm going to take this expression over here. And we're going to actually write out what this means in terms of our models that we can derive our likelihood.\"\n",
    "gen_3 = \"Great, Yeah, So now another student is saying, in this case, would it make sense to match the depth because that's our RGB values.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4c85ae80-9a8a-466f-8d94-7d19ece3f4b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All right. All right, cool. Today, we'll just talk about the structure of this class and then introduce the problem that we'll talk about for the rest of this class, which is going to be related to my research. L, tew1nebl thwsth  ar cacoul toc m.,\n",
      "til ck. So thing Tirj of them is qunks, but that's this 96 -colly Po and this going the plus that the first look. How it's want cuch tokey 24 actual like look. Wore is you wants that this frequency, we udent rough loo\n",
      "\n",
      "And okay, let's move on for now. So what we're gonna do is I'm going to take this expression over here. And we're going to actually write out what this means in terms of our models that we can derive our likelihood. woj xihkk egoie'J VV1Vxaun'kVotdak tive've questions inturs is good what weslow informalle champles, y everyod, tk. Were when peoperfectors? And the to st a let different proply is writncy questions. So thing to one its. And the nient we chais of why acoa\n",
      "\n",
      "Great, Yeah, So now another student is saying, in this case, would it make sense to match the depth because that's our RGB values.\n",
      "hauansthnbcamaMGG?ttGeaabse attlcttt,FG?scusGBtattZWGG,ltdatzWtttakttaMsGGtcaktwsttGGGGG?GG ttGkiGGGGGtytttttttywGGtakttGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGG\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(generate(transformer, gen_1, tokenizer, max_length=256, seq_length=256, device=device), end=\"\\n\\n\")\n",
    "print(generate(transformer, gen_2, tokenizer, max_length=256, seq_length=256, device=device), end=\"\\n\\n\")\n",
    "print(generate(transformer, gen_3, tokenizer, max_length=256, seq_length=256, device=device), end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dd67f17b-85f3-46f8-8c09-bf52d7033ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "misc_1 = \"This cat is very cute and\"\n",
    "misc_2 = \"In a world where machines had emotions\"\n",
    "misc_3 = \"The universe is a vast and mysterious place, full of wonders and secrets\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c268c5da-f5e3-4cb8-9d01-2669c72fc85b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This cat is very cute and  rryviyvayrtyyydy i rictr ryyddcycyt  yvddyycycyd yiddyydyycddddyyyryivdyryyvvicycydddddyyyddviyyydycydddddyyyFvyycvryyyyyyvyyycddyyydiyyyyyyyddyyddyyyddryyyyyyydddyydddyyddyyyyyyddyyyyddyyyyyyyyydddyyyyvdedvdyyyydyydyyyyyddyyvyyyyyyyyyyyyyyyyyyyyyyyyyyyy\n",
      "\n",
      "In a world where machines had emotionsssod Mm, sorsssssdsnsaorMswdd sanssasssm dss ssnsddsssss hddssaosdsnssdsssMssssssssssssasssssssssssdssssssossssssssssssssssassssssssssssssssssssssssssdsssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssss\n",
      "\n",
      "The universe is a vast and mysterious place, full of wonders and secretssasdmecnl clatcnndr uc ssn?rerrulwscnnyrnnnnnddrnnynnsnnconnsnssdsnndr.yarsnl nnsnnddrllrrrrmnrnnrrsndnnr yrnnnsndsdddrnncnnddynnalynddynlncnns nnrsnn nnnrlyrdidyynnrndddrrnyrnddrsrsddewdateduct lung class they questions. Trepernermal ore be 4dtednedninuto\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(generate(transformer, misc_1, tokenizer, max_length=256, seq_length=256, device=device), end=\"\\n\\n\")\n",
    "print(generate(transformer, misc_2, tokenizer, max_length=256, seq_length=256, device=device), end=\"\\n\\n\")\n",
    "print(generate(transformer, misc_3, tokenizer, max_length=256, seq_length=256, device=device), end=\"\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
