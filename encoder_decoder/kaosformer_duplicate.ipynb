{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d42f719-2ce2-4c1f-b1c1-d989059d24f5",
   "metadata": {},
   "source": [
    "# Kaosformer (Duplicate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "135bfe08-3cb6-4e59-9932-24420b66fa1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa825860-ce0a-4d30-8e22-383151343a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils import tensorboard\n",
    "import torch.nn.functional as F\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db6cb9d7-95ca-4c83-89c9-e33dee39d267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "523bc1e0-93a4-4832-b347-3fb21a7522fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_size(module):\n",
    "    size = 0\n",
    "    for param in module.parameters():\n",
    "        size += np.prod(param.shape)\n",
    "    return size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f48135-cc59-4b26-bc63-cf74fa26f7c2",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6acef85a-863f-488b-ae35-32c52c3cf71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_split(data, ratio=[9, 1]):\n",
    "    n = len(data)\n",
    "    n_split = int(n * ratio[0] / sum(ratio))\n",
    "    train_data = data[:n_split]\n",
    "    valid_data = data[n_split:]\n",
    "    return train_data, valid_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb8276f4-009f-47d9-b447-4f25c2da03d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    \n",
    "    def __init__(self, text):\n",
    "        self.text = text\n",
    "        self.vocab = self.char()\n",
    "        self.char2id = dict(zip(self.vocab, range(len(self.vocab))))\n",
    "        self.id2char = dict(zip(range(len(self.vocab)), self.vocab))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.vocab)\n",
    "\n",
    "    def char(self):\n",
    "        vocab = sorted(list(set(self.text)))\n",
    "        return vocab\n",
    "    \n",
    "    def encode(self, x):\n",
    "        return [self.char2id[i] for i in x]\n",
    "    \n",
    "    def decode(self, x):\n",
    "        return \"\".join([self.id2char[i] for i in x])\n",
    "    \n",
    "    def decode_tensor(self, x):\n",
    "        return self.decode(x.tolist())\n",
    "    \n",
    "    def decode_batch(self, x):\n",
    "        return [self.decode_tensor(x[i]) for i in range(x.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1870bfdc-e356-452b-909c-3764d86d6360",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextLoader:\n",
    "    \n",
    "    def __init__(self, data, batch_size, seq_length):\n",
    "        self.batch_size = batch_size\n",
    "        self.data = data\n",
    "        self.seq_length = seq_length\n",
    "    \n",
    "    def __call__(self):\n",
    "        random_indices = torch.randint(low=0, high=len(self.data) - (self.seq_length + 1),\n",
    "                                       size=(self.batch_size,))\n",
    "        \n",
    "        sequences = [torch.tensor(self.data[i:i + self.seq_length]) for i in random_indices]\n",
    "        targets = sequences\n",
    "        random_indices += 1\n",
    "        labels = [torch.tensor(self.data[i:i + self.seq_length]) for i in random_indices]\n",
    "        \n",
    "        return torch.stack(sequences), torch.stack(targets), torch.stack(labels)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ce64922-53c9-43c9-8876-72de9889e0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/kaorpus.txt\", \"r\") as file:\n",
    "    kaorpus = file.read().rstrip()\n",
    "    \n",
    "kaorpus_train, kaorpus_valid = get_split(kaorpus, ratio=[19, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c26d78c5-cb4a-4a09-b2b8-701ad6a2afba",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(text=kaorpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "173255ae-5042-465f-bd9e-1f639f12f88f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of characters in train_loader: 1486815\n",
      "                    and valid_loader: 78254\n"
     ]
    }
   ],
   "source": [
    "train_loader = TextLoader(tokenizer.encode(kaorpus_train), batch_size=64, seq_length=256)\n",
    "valid_loader = TextLoader(tokenizer.encode(kaorpus_valid), batch_size=64, seq_length=256)\n",
    "\n",
    "print(f\"Number of characters in train_loader: {len(train_loader)}\\n\"\n",
    "      f\"                    and valid_loader: {len(valid_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e30e9a-0069-4fc0-9581-0ba26f6a7025",
   "metadata": {},
   "source": [
    "## Positional Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24708fe6-dae4-4af7-8aec-950fda5d4ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(nn.Module):\n",
    "\n",
    "    # max_length is maximum seq_length\n",
    "    def __init__(self, d_model, seq_length, dropout=0.1, device=None):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # Compute the positional encodings once in log space.\n",
    "        embedding = torch.zeros(seq_length, d_model)\n",
    "        position = torch.arange(0, seq_length).unsqueeze(1)\n",
    "        factor = torch.exp(torch.arange(0, d_model, 2) * -(np.log(10000.0) / d_model))\n",
    "        embedding[:, 0::2] = torch.sin(position * factor)\n",
    "        embedding[:, 1::2] = torch.cos(position * factor)\n",
    "        embedding = embedding.unsqueeze(dim=0)\n",
    "        \n",
    "        if device is not None:\n",
    "            embedding = embedding.to(device)\n",
    "        self.register_buffer(\"embedding\", embedding)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.embedding[:, :x.shape[1]].requires_grad_(False)\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fbf96b6e-1f45-4e9b-8a61-ce097808d737",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveEmbedding(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model, seq_length, device=None):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(seq_length, d_model, device=device)\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        inputs = inputs + self.embedding(torch.arange(inputs.shape[1], device=self.device))\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710760a6-08f3-488c-b48c-6a00c38a2c5c",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf18a18b-52bb-417a-867d-9eb679331584",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model, nhead, num_decoder_layers, dim_feedforward, dropout, activation,\n",
    "                 device, **kwargs):\n",
    "        super().__init__()\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=d_model, nhead=nhead,\n",
    "                                                   dim_feedforward=dim_feedforward, dropout=dropout,\n",
    "                                                   activation=activation)\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_decoder_layers)\n",
    "        self.decoder = self.decoder.to(device)\n",
    "        \n",
    "    def forward(self, inputs, *args, **kwargs):\n",
    "        outputs = self.decoder(inputs, torch.zeros_like(inputs).to(inputs))\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9ce93644-427e-40b1-8b6b-6704846059ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    \n",
    "    def __init__(self, embedding_args, transformer_args, transformer_type, positional_type, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(**embedding_args, device=device)\n",
    "        \n",
    "        if positional_type == \"sinusoidal\":\n",
    "            self.positional = PositionalEmbedding(transformer_args[\"d_model\"], seq_length=256,\n",
    "                                                  dropout=transformer_args[\"dropout\"],\n",
    "                                                  device=device)\n",
    "        elif positional_type == \"torch\":\n",
    "            self.positional = NaiveEmbedding(transformer_args[\"d_model\"], seq_length=256,\n",
    "                                             device=device)\n",
    "        else:\n",
    "            raise NotImplementedError(positional_type)\n",
    "            \n",
    "        if transformer_type == \"encoder_decoder\": \n",
    "            self.transformer = nn.Transformer(**transformer_args, batch_first=True, device=device)\n",
    "        elif transformer_type == \"decoder\":\n",
    "            self.transformer = TransformerDecoder(**transformer_args, device=device)\n",
    "        else:\n",
    "            raise NotImplementedError(transformer_type)\n",
    "            \n",
    "        self.fc = nn.Linear(transformer_args[\"d_model\"], embedding_args[\"num_embeddings\"],\n",
    "                            device=device)\n",
    "    \n",
    "    def forward(self, inputs, targets=None, mask=True):\n",
    "        inputs_embed = self.positional(self.embedding(inputs))\n",
    "        if targets is None:\n",
    "            targets_embed = torch.zeros_like(inputs_embed).to(device)\n",
    "        else:\n",
    "            targets_embed = self.positional(self.embedding(targets))\n",
    "        inputs_mask = None\n",
    "        targets_mask = None\n",
    "        outputs = self.transformer(inputs_embed, targets_embed,\n",
    "                                   src_mask=inputs_mask, tgt_mask=targets_mask)\n",
    "        outputs = self.fc(outputs)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c8c21531-2920-4a2e-912d-8db04a9e85e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 2040653\n"
     ]
    }
   ],
   "source": [
    "embedding_args = {\"num_embeddings\": len(tokenizer), \"embedding_dim\": 128}\n",
    "transformer_args = {\"d_model\": 128, \"nhead\": 8, \"num_encoder_layers\": 6, \"num_decoder_layers\": 6,\n",
    "                    \"dim_feedforward\": 256, \"dropout\": 0.1, \"activation\": \"relu\"}\n",
    "\n",
    "transformer = Transformer(embedding_args, transformer_args, transformer_type=\"encoder_decoder\",\n",
    "                          positional_type=\"torch\", device=device)\n",
    "print(f\"Number of parameters: {get_size(transformer)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4f3972-c9be-4ad7-97fc-4323340242b1",
   "metadata": {},
   "source": [
    "## Loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5539daca-f690-44ef-8009-3013ebb3dcd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def validate(valid_loader, model, criterion, device):\n",
    "    model.eval()\n",
    "    \n",
    "    losses = []\n",
    "    for _ in range(16):\n",
    "        inputs, targets, labels = train_loader()\n",
    "        \n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        outputs = model(inputs, targets)\n",
    "        \n",
    "        batch_size, seq_length, vocab_size = outputs.shape\n",
    "        outputs = outputs.reshape(batch_size * seq_length, vocab_size)\n",
    "        labels = labels.reshape(batch_size * seq_length)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    loss = sum(losses) / len(losses)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2ecb3dc3-df20-415d-bf0e-fccad3424fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate(model, prompt, tokenizer, max_length, seq_length, device):\n",
    "    model.eval()\n",
    "    \n",
    "    prompt = tokenizer.encode(prompt)\n",
    "    output = torch.tensor(prompt).to(device)[None, :]\n",
    "    prompt = output[:, -seq_length:]\n",
    "    for _ in range(max_length):\n",
    "        logits = model(prompt, prompt, mask=False)\n",
    "        logits = logits[:, -1, :] # take final logits\n",
    "    \n",
    "        probabilities = F.softmax(logits, dim=1)\n",
    "        next_id = torch.multinomial(probabilities, 1)\n",
    "\n",
    "        output = torch.cat([output, next_id], -1)\n",
    "        prompt = output[:, -seq_length:]\n",
    "    \n",
    "    return tokenizer.decode_batch(output)[0]\n",
    "\n",
    "\n",
    "def split_prompt(prompt):\n",
    "    n = prompt.shape[1] // 2\n",
    "    return prompt[:, -2 * n:-n], prompt[:, -n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a291d29-08fd-4a9b-ac68-11dfcba4d527",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, valid_loader, model, tokenizer, criterion, optimizer,\n",
    "          writer, num_i, validate_every, save_every, save_path, device):\n",
    "    \n",
    "    prompt = \"One of the reasons to prefer small kernel sizes over larger ones is that smaller kernels have fewer parameters than larger ones, which can reduce the model's complexity and computational requirements. This can lead to faster training times and lower memory requirements. What are the benefits of using smaller kernel sizes in CNNs?\"\n",
    "    \n",
    "    for i in tqdm(range(0, num_i)):\n",
    "        model.train()\n",
    "        \n",
    "        inputs, targets, labels = train_loader()\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        outputs = model(inputs, targets)\n",
    "        \n",
    "        batch_size, seq_length, vocab_size = outputs.shape\n",
    "        outputs = outputs.reshape(batch_size * seq_length, vocab_size)\n",
    "        labels = labels.reshape(batch_size * seq_length)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        writer.add_scalar(\"train_loss\", loss.detach().cpu(), global_step=i + 1)\n",
    "        \n",
    "        if (i + 1) % validate_every == 0 or i == 0:\n",
    "            valid_loss = validate(valid_loader, model, criterion, device)\n",
    "            writer.add_scalar(\"valid_loss\", valid_loss, global_step=i + 1)\n",
    "            sample_generation = generate(model, prompt, tokenizer, max_length=256,\n",
    "                                         seq_length=seq_length, device=device)\n",
    "            writer.add_text(\"sample_generation\", sample_generation, global_step=i + 1)\n",
    "            \n",
    "        if (i + 1) % save_every == 0 and save_path is not None:\n",
    "            torch.save(transformer.state_dict(),\n",
    "                       f\"{save_path}/model_{str(i + 1).zfill(len(str(num_i)))}.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e9e5dc-41bd-4903-b347-0b61496d0489",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f75d89c-1cd6-4719-a91b-f9cc37d3ea14",
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime_now = datetime.now().strftime(\"%y%m%d_%H%M%S\")\n",
    "log_path = f\"logs/run_{datetime_now}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88705c34-12e8-4cd5-8147-3f4e3c6192fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = tensorboard.SummaryWriter(log_dir=log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad40c67c-8335-4ace-ac69-63a40c91208f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir=logs --port=8008"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b226c3-91c2-4442-b545-87f91c0ae37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(transformer.parameters(), lr=3e-4, weight_decay=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3f2d13-167a-45ee-bedd-ccf89db3b8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(train_loader, valid_loader, transformer, tokenizer, criterion, optimizer,\n",
    "      writer=writer, num_i=60000, validate_every=500, save_every=2500, save_path=log_path,\n",
    "      device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bd2b6629-7fd6-4132-af1f-5448765709c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer.load_state_dict(torch.load(\"./model_duplicate.pth\", map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5dc2195a-e866-4a84-863b-27d8141bc587",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_1 = \"One of the reasons to prefer small kernel sizes over larger ones is that smaller kernels have fewer parameters than larger ones, which can reduce the model's complexity and computational requirements. This can lead to faster training times and lower memory requirements. What are the benefits of using smaller kernel sizes in CNNs?\"\n",
    "qa_2 = \"Recurrent Neural Networks RNNs and Convolutional Neural Networks CNNs are two popular types of deep learning models that are used in different domains. RNNs are generally preferred over CNNs for processing sequential data, such as time-series data, speech, and text. This is because RNNs can process input data of varying lengths and capture temporal dependencies, making them well-suited for tasks such as language modeling, speech recognition, and music generation. Which use cases do we see RNNs preferred over CNNs?\"\n",
    "qa_3 = \"The introduction of residual connections in ResNet led to much deeper networks being trained than previously possible. This allowed for much better performance on difficult computer vision tasks, such as image classification, object detection, and semantic segmentation. In fact, ResNet achieved state-of-the-art performance on the challenging ImageNet dataset, reducing the error rate by a significant margin compared to previous approaches. Why was ResNet such a big breakthrough in computer vision?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e92bd569-ee22-4db0-ae94-b3e3b76ec812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One of the reasons to prefer small kernel sizes over larger ones is that smaller kernels have fewer parameters than larger ones, which can reduce the model's complexity and computational requirements. This can lead to faster training times and lower memory requirements. What are the benefits of using smaller kernel sizes in CNNs? XNN minus though, put dw it to transform thero off for. And already. Okay. And we're going to thatevious a, VBI could be tell marly doesn't going to direction by 20 pirolling that them, and a bigger this gradient. Theit lecture the for 20 hhof with Y train X and X2. If the gradient more parity, D. If I'm going to hald iterally just look at the function of this discuss the pariright zero, more back for the propagate, and I this in going to now if y also one is going to because I want to go to to z, So I as \n",
      "\n",
      "Recurrent Neural Networks RNNs and Convolutional Neural Networks CNNs are two popular types of deep learning models that are used in different domains. RNNs are generally preferred over CNNs for processing sequential data, such as time-series data, speech, and text. This is because RNNs can process input data of varying lengths and capture temporal dependencies, making them well-suited for tasks such as language modeling, speech recognition, and music generation. Which use cases do we see RNNs preferred over CNNs? Yeah, there that we have two me comd want to state e1-do therefore just look at and then evalude not set, yearlowing bilease this tanneogna is this is somework use also 36. And p, this as shorige image notation buildo sum recurdation we calling, okay. And then some for good this TLE, the envoltaged to oblem. And to ours to right minus X of from them correction ence of priow e to go ahead over the gradients of to good hall the should signal operation. Alright? Yeah, let's say that we're going to transpose f\n",
      "\n",
      "The introduction of residual connections in ResNet led to much deeper networks being trained than previously possible. This allowed for much better performance on difficult computer vision tasks, such as image classification, object detection, and semantic segmentation. In fact, ResNet achieved state-of-the-art performance on the challenging ImageNet dataset, reducing the error rate by a significant margin compared to previous approaches. Why was ResNet such a big breakthrough in computer vision? One our feacter. When I though there to classify of t difference is lefs something question times d z of with out this is combellling. Andre. We're going to somewhere are going to big T score it take the first and deal additions, b plust comes and today. Then fore 62 minus 1 times 28. So when the spiridard of two or again. Incoom of t denteng something laying for still. So we beneral by 2010s cost off. Bigressureds to me backpropagate than something where out that if you can also knowing first is another. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(generate(transformer, qa_1, tokenizer, max_length=512, seq_length=256, device=device), end=\"\\n\\n\")\n",
    "print(generate(transformer, qa_2, tokenizer, max_length=512, seq_length=256, device=device), end=\"\\n\\n\")\n",
    "print(generate(transformer, qa_3, tokenizer, max_length=512, seq_length=256, device=device), end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "54c25437-bc3a-4383-a7b7-127332f64928",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_1 = \"All right. All right, cool. Today, we'll just talk about the structure of this class and then introduce the problem that we'll talk about for the rest of this class, which is going to be related to my research.\"\n",
    "gen_2 = \"And okay, let's move on for now. So what we're gonna do is I'm going to take this expression over here. And we're going to actually write out what this means in terms of our models that we can derive our likelihood.\"\n",
    "gen_3 = \"Great, Yeah, So now another student is saying, in this case, would it make sense to match the depth because that's our RGB values.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5473e1a9-3080-431f-b53d-421715ce6bd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All right. All right, cool. Today, we'll just talk about the structure of this class and then introduce the problem that we'll talk about for the rest of this class, which is going to be related to my research.5hlevounsHht dhfd edhit kd topwhltvt. iarlHo.5 then imck is this determ some neveral neuron doing outsive and say that was that things w rarger of transformerrate choose 11 computer this me corrections. I disture offling. So we want convolutional actually \n",
      "\n",
      "And okay, let's move on for now. So what we're gonna do is I'm going to take this expression over here. And we're going to actually write out what this means in terms of our models that we can derive our likelihood. mwoosowomtiraooI. oig Isoalon. ark on  's going to follow this is zero see our task networks? looks maplies blue very value four. Ord his m comma one comminus before, we thinking the classe in make someone and broce is a quals for x zela someone emain gen\n",
      "\n",
      "Great, Yeah, So now another student is saying, in this case, would it make sense to match the depth because that's our RGB values..YH'seWd3ao-, HHDGaYesconsH VXH.Bl\"tje jM889, 3H3 VHH,cMo, H, N, 8HM8,8 tH3ccN1c.3/cavH,,8hH3WH1WcH,1GMG4YHGGG,1/jWHHH1HHHHVKHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHNNNNNNNNNNNNNNNNNNNNNN\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(generate(transformer, gen_1, tokenizer, max_length=256, seq_length=256, device=device), end=\"\\n\\n\")\n",
    "print(generate(transformer, gen_2, tokenizer, max_length=256, seq_length=256, device=device), end=\"\\n\\n\")\n",
    "print(generate(transformer, gen_3, tokenizer, max_length=256, seq_length=256, device=device), end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b25f23fa-985a-4386-8b5a-454e28b55942",
   "metadata": {},
   "outputs": [],
   "source": [
    "misc_1 = \"This cat is very cute and\"\n",
    "misc_2 = \"In a world where machines had emotions\"\n",
    "misc_3 = \"The universe is a vast and mysterious place, full of wonders and secrets\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "01234719-b677-44f3-8d5c-010e08ddba76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This cat is very cute andcisiivecaiYgcdhivcdisc i hiiziidccdicicd icizhcvzai cHHiccczcHdccdcvzhcddcccicvccczicdzczivzHczzz7iizicHiczicHcczKcaicHzHczzczicvaHGczczzzcciczccccczzaHcccHzzccccHccczccHcH.czvcizHccHcicccczzczcccczzcvczzzczHzzcHzzzc*zczczHcHccHzzHcHccccccccccccccccccccccc\n",
      "\n",
      "In a world where machines had emotionsinaeoviamasaha05aoasshsa lvwsaiehaearaoaaraaaa aaiaaareahoisaaao aasaeaaeaaaaeaasamaa oaaaaamaaaaoaaaaaaaaaaaaaaoaaaaaaaaaaaaaaaooaaaoaawaaaaaaaaaaaoaaaoaaaaaaaaaoaaaaapaaaaaaaoaaoaoaaoaaoaaaaoaapaaaaaaoaaoaaaooaaaaaaoaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\n",
      "\n",
      "The universe is a vast and mysterious place, full of wonders and secretsscl slccspp scccllmssconcnllpsslcoplsopclsssccoslalsscspcoocalsslcsosslssatsslcsodssoolllasopslcsllsosslssoshsllclolsllsolsolssslclsoolvllllhslslsllllpsplssslllhllcalpslllollslllsopllllllllllllllldlllllllllllllllllllllllllllllllllllllllllllllllllllllllllll\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(generate(transformer, misc_1, tokenizer, max_length=256, seq_length=256, device=device), end=\"\\n\\n\")\n",
    "print(generate(transformer, misc_2, tokenizer, max_length=256, seq_length=256, device=device), end=\"\\n\\n\")\n",
    "print(generate(transformer, misc_3, tokenizer, max_length=256, seq_length=256, device=device), end=\"\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
