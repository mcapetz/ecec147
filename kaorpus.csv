Text
"A question from Brandon. So just out of curiosity in that last video, we saw a lot of the open circles, meaning that those were plan movements. There were like five plan circles and then one that was actually a physical reach. Is was there a reason as to why the experiment was set up that way? Yes. So ideally we would not have any physical reaches in there. We would just start showing targets and the monkey would immediately plan to them. And then we would classify them immediately. And we can do this in a super long chain. The problem is that the monkey may become disengaged in the task. And it becomes disengaged in the accuracy decreases. And so the intermission reaches that we interleave there are to just make sure the monkey stays engaged in the task. And also knows that he has to reach himself. This engagement related thing. Any other questions here? All right. So that's a look at what we're going to be doing in the next lectures on discrete classification. The question from Andrew. Yeah. Could you go to the slide with a task timeline? Yeah."
"A second challenge is that features of the response could change on the time scale that's faster than the inter spike interval. Right. So first let's define the inter spike interval. The inter spike interval is merely the time in between two spikes and we're going to be talking about this a lot today when we get to post on processes. We know that because neurons have a refractory period that the time in between spikes has to be at least four milliseconds and realistically is often more than 10 to 15 milliseconds more than the absolute more than the relative refractory period. All right. So let's just say the absolute refractory period period of four milliseconds. If a spike fires every four milliseconds, then the most time that it can fire in one second is 250 times. And so we call that 250 hertz. Hertz means a rate."
"Actually, it's weird to see. We have really good knowledge about how this circuit actually computes and how light signals are transformed into electrical signals. And so the idea is, if we could take a video camera and video the scene, we could telemetre that information to hear this chip through our communication. And then that signal that could generate the spiking signal that we would then want to send to some electrodes that stimulate the actual retinal ganglion cells that then send electrical signals to the optic nerve. So the idea should bypass this broken circuit by recording the video and then stimulating the back of the eye, how we how it might have originally intended to how light would have originally intended to simulate it. Right. And so last lecture we asked the question, what are some problems you might see or what are some not problem, but challenges that the system might face. And so some people mentioned that it could be hard to do the surgery. It might be hard to get the signals reliably to the chip. Are there any other problems that people can think of with respect to the electrodes here? I believe you also be able to unmute yourselves and just speak, but if not, someone let me know or chat. Could it just be why we have the electrodes in the right place? That's related to it, which is you have to make sure that you're stimulating the correct cell that eventually go to the optic nerve. So in this case, the answer that I was looking for is the challenge that can come up as the following. Let's say that we have our electrode array. And there are some cells hanging up here that we're trying to stimulate."
"again. Okay, so, uh, Wingshee, your question was, uh, why are W and Q symmetric? Uh, and that's because the covariance operator is symmetric. Um, so I'm not sure how much of that answer came through before my internet cut out. Uh, is that all good? Yeah, but uh, it's a little bit laggy, but I know what you're talking about. Okay, and yeah, so can we go on to explaining the, uh, why can we ignore it in the expression we have in here? Yeah. Yeah, great. So, uh, for the, uh, this question is why, why when we derive the least squares solution, we can, uh, ignore the effect of W. And sorry, I think it's frozen again. Okay, okay, I think it's back now. Okay. Um, I, I'm so sorry, everyone. Uh, I think on Wednesday, I'll, I'll try, uh, I'm in a different internet situation right now and, um, and I'll try to make sure this isn't a problem on Wednesday."
"again. Okay, so, uh, Wingshee, your question was, uh, why are W and Q symmetric? Uh, and that's because the covariance operator is symmetric. Um, so I'm not sure how much of that answer came through before my internet cut out. Uh, is that all good? Yeah, but uh, it's a little bit laggy, but I know what you're talking about. Okay, and yeah, so can we go on to explaining the, uh, why can we ignore it in the expression we have in here? Yeah. Yeah, great. So, uh, for the, uh, this question is why, why when we derive the least squares solution, we can, uh, ignore the effect of W. And sorry, I think it's frozen again. Okay, okay, I think it's back now. Okay. Um, I, I'm so sorry, everyone. Uh, I think on Wednesday, I'll, I'll try, uh, I'm in a different internet situation right now and, um, and I'll try to make sure this isn't a problem on Wednesday."
"Alexandra. Okay, so for number two, it says to write an expression for F of t, which is our sampling function and what we ought to see is that this sampling function is non-uniform, but it's non-uniform in a consistent way, which is that the impulses that should happen at odd samples, like negative three, negative one, one and three, are all shifted by some amount to tau. All right, so I'm going to draw this as two different impulse trains. I'm going to have the even ones I'll draw on purple. One, two, three, negative one, negative two, negative three. So the even ones are on the integers and then the odd ones I'll draw on this blue are shifted by some amount tau. All right, so if someone who did this question either unmute or are all right in chat, you're approached to write out this function as two uniformly space sampling functions. Sure. So the even ones are, well, the way it is, we can represent even number as like two k."
"All right everyone, sorry for a bit of a wait start. Reminder that the project is due this Friday by 11.59pm emailed to my email address here at cowatces.ucla.edu. All right, I want to start off this class going over any questions about the project. I had a question of part three. Part three, yep. So for the suggestion of a decoding from differences, when I sort of had a wide-bend snooved set equal to the formula used left there, I had a matrix that was missing that was shorter by one column, so then the rest of the decoder wasn't working. So I wasn't sure it was sort of alterations to X spin or X test would be required. Yeah, that's a great question, Michael. If you do the, so if we just had data Y1, Y2, let's say we just had five data points and then corresponding kinematics. And Michael is saying that when he does the differences, right, you need two values to calculate a difference."
"All right everyone. So we're going to start off for today for ECU 189. For today, if you go to CCLE, you'll see that in the week. Eight materials, I believe I've uploaded the final project for this class. And what we're going to do today is we're going to go over the project and the data so that you can be prepared to tackle it. The project, you'll see consists of three tasks at the end. And for the purposes of today, we're going to actually already do one of those three tasks and hopefully you'll see that it's not a something. And so to go through and introduce you to the data sets and the code, I'm going to be sharing my net lab screen on my computer. But my computer is right now also compiling the ECU 102 lecture that we just had and also showing my video. And so if it's running to slow, I may turn off my video for just a lecture. Let me go ahead and share my net lab window. Right. Can everyone see a net lab window here? Or can someone give me a thumbs up or I meet yourself and say yes or no? Yes. Okay. Great. Thanks. All right."
"All right, and then here we have minus J, and we have that the imaginary part of CK is the imaginary part of C minus K. Right. All right. And then I think, sorry, and this is where I had a misinterpreted Bradley's comment. This can be written as minus, and then we have a real part of C minus K plus a J times the imaginary part of C minus K. And all this is is C minus K. And so this would equal minus C minus K. All right, okay. Any questions there?"
"All right, any questions before we begin? All right, and then before we begin, I'm also just gonna see if I can find my TA, okay, and make them co-host. There's Tom Moy. Okay, and make them cause there's more. Right. Okay, I'll maybe deal with that during the break. Alright so we'll be clear on that in the future will always announce when the next homework is going to be released and announces like we just did and when they'll be due. motivation of not having not having assessments that are worth so much so that it might encourage students to to to be academically dishonest."
"All right, even though we're giving you until 6 10 p.m. to submit the exam, you're not to work on the exam after 6 p.m. Pacific Standard Time. And then the portal closes at 6 10 p.m. and so be sure to get in your exam before then. If you're a student in another time zone, in the exam announcements, I sent out the instructions for emailing me about your availability so that we can schedule your alternate exam time. And then just like on the midterm, the TAs and I are going to be online at this Zoom link to take any exam questions. This Zoom link will have the waiting room enabled, meaning that when you first click on the link, you may not be admitted into the room. So we take questions from students one at a time. And then after a student finishes, we'll admit the next student from the waiting room."
"All right, everybody. If you're coming from EC-102, my computer is, I hope that this is okay. My computer is still converting the EC-102 video, but I started recording for EC-1809, so hopefully there isn't a negative interaction there. All right. All right, cool. So this is EC-1809, which is the Advanced Honors Seminar. I think most, if not all of you, are probably taking this concurrently with EC-102. Today, we'll just talk about the structure of this class and then introduce the problem that we'll talk about for the rest of this class, which is going to be related to my research. And so a quick few things to get out of the way. The first is for grading. So there are many more students in the class this year than in prior years. What I like to do is I like to have this class be largely interactive. And so this is not the case."
"All right, everyone for can it get started? We have two announcements today. The first is that homework number two is going to be due on January 30th on Monday. I'll put it to grade scope. And this homework has a considerable compliance. So be sure to start the homework early if you haven't already. And then like greenhouse last time, there's going to be a delay and returning to homework number one grades because they're still working out some admin. But just fix the higher the gleaners. Alright. Any questions on any quickly? Correct? Yeah, any other logistic questions? Of course questions. Alright, we'll get some material. So today we're going to cover the neural network architecture, some design choices. And we're also going to get started with backpropagation, which is an algorithm that's used to train neural networks. We talked again elastic there. This will be the reading for the deep learning textbook for these components. We talked about briefly at the end of last lecture of how neural networks are inspired from biological neurons. And last lecture we pointed out three key components of biological neurons. One is that they have inputs which are these arborist like regions of the neuron called dendrites. Dendrites receive inputs from other neurons. There's this thing called the axon hillock. The axon hillock. What it does is it integrates all of those inputs on the dendrites. So the inputs are in red. You can think of the axon hillock as summing up all of those inputs and that, that sum is above a threshold, then the neuron generates what is called a spike or action potential. And that's the signal that's conveyed down the output of the neuron, which is the axon. And eventually this axon will connect to the dendrites of dams thing. Alright. This is a very high level overview of neurons. If you'd like to know more about how neurons work, like the company I teach. Next quarter we'll go into this important detail. Alright? Any questions for this brief recap from last lecture? Alright, so again, the key things are that there are inputs. The inputs are summed and then pass through some nonlinearity, like comparing to a threshold, e.g. at the axon hillock. And then they're propagated along the axon. Neurons come in all diverse shapes and sizes. But when you boil down a neuron into its x and y components, those are the three things are dendrites, the axon, the axon. And if you were curious it too, the output signals of neurons look like if you just focus on this image right here, what we're doing is we're starting an electrode in and measuring from the axon of the neuron. And you can see that basically the signal, if we plot this voltage is going to be at some resting voltage, VR. And then sometimes it'll occasionally spike. And so you can think of these spikes as conveying something happens like the second one. Whereas when it's just sitting at rest, just all zeros. So that's the absolute potential of the neuron fundamentally communicated through this all or nothing. Spike. The spikes from neuroscience are very complex. And so another thing about spikes is that probabilistic. So what you can do is you could record from a single neuron and you can repeat a stimulus. So you should be recording from a neuron showing someone a picture of a cat. And even though you showed them the same exact fat on five different trials. If we were to plot across time when the spikes occurred, which are these red vertical lines, despite train would look different from trial to trial. So usually in neuroscience. So instead of looking at these spike trains, we often look at something called the firing rate of the neuron. The firing rate of the neuron tells you how many spikes you might expect to see given window. And so maybe the firing rate at the start of a trial. So the x-axis here would be time, my start-up around 50 spikes per seconds. So on average, I would expect to see 50 spikes, no 1 s window. And then maybe that rate decreases over the length of the trial. So usually when we think of neural networks, we think of them as their values reflecting what is analogous to the rate at which a neuron is spiking. Alright? Any questions on anything? We've made it to the biophysical neuron. Alright? So this is what the artificial neuron that we're going to use a neural network looks like. So kind of similar to the biological neuron. In fact, I'm actually just gonna go two slides ahead, but we do comparisons for the artificial neuron. What happens is that you're going to have inputs that come from oxygen neurons. And so these are X1, X2, X3, X4. You can, these are the outputs of prior artificial neurons. And then you can think of X1, X2, X3, X4, and then neuroscience analogy as the activity on the axons of the upstream neuron connects to this current neurons dendrites. After that, you take your inputs and you multiply them with base. And this will reflect how different neurons will have different types of effects on the downstream neuron. They could strongly activated or they could even inhibited if the weight is negative. And so, while this process is very complex and biology and the artificial neural network, what we do is we replace those connections with just scalar weights, W1, W2, W3, W4. And then just like the biological neuron has this integrative center for the axon hillock That's Psalms, the inputs and the artificial neuron. We will have this component, which will sum of the inputs. So it's going to keep W1 X1 plus W2, X2 plus W3 X3, etc. And then after that is going to pass it through a non-linear, non-linearity. Just like how biological neuron, I've got thresholding operation. However, when we go through lecture today, we're going to talk about some of the design considerations into how to pick up and what's the best practice. Alright. So that's the artificial neuron and kind of how it was inspired by the biological neuron. Any questions there? All right. So some people, including my own lab and use artificial neural networks to study mechanisms within biological networks. But there should always be caution when comparing these artificial networks to biological neural networks. Because of biological neural networks. Many more complexities as stochasticity, dynamics that are not model that own these artificial neural networks. So the way we think of these artificial neural networks is that their architecture was once inspired by these neurons, a very crude approximation of these biological neurons. But they are far from an adequate and perfect model of these biologics on their arms. So we should always use caution when comparing artificial neurons to biology. Nevertheless, those principles from biology led to these neural networks, which we know performed very empirically now. Okay? So if you're interested in talk to you about any of these differences stomach, which I read it on the slide. Feel free to stop by my office hours and I'll be happy to talk about some of the differences between biological and artificial. Alright? Any questions then before we start to talk about neural networks? Artificial neuron. The question is, when we drew this artificial neuron here, was this output just any real number r? That's correct, yes. Okay, So we're gonna start off with just some naming conventions. So typically a neural network, we'll start off with an input layer that's going to be drawn in blue here. And this input layer we usually denote with the variable x. You can think of this as far, alright? So x corresponds to the actual image that we're going to put into the neural network. The neural network will also have the very last layer called the output layer. And if we're doing classification with C bar Tanya, remember the output layer corresponds to the scores of each class. So this is the output layer that might get you the scores that go to e.g. a softmax classifier like we discussed. And then everything between the input and the output layer are called hidden layers. So this right here is the hidden layer. And you've probably heard of things like the depth of the neural network. A neural network that is deeper is going to have more layers. So this neural network, this would be a two layer neural network. We don't call the input layer since that's like the input. And if you don't call it that a layer, but all the hidden layers, as well as the output layer, are included in the count of how many layers of neural number of gates. When I draw this image, you're going to see connections between each of these nodes. And each node corresponds to, in this case, a scalar number. Each of these will be an artificial neuron. And if I had these connections, e.g. from my input to each one of these are going to have a weight. So we'll call this weight, weight W1 one, W12, and then W1 three, where the first number denotes the neuron identity that's going to H1, so it's index one. And then the second 123 correspond to these three efforts. So H1, just like we've talked about in the prior slide, would you do this computation where it would son my inputs W0 and 11 x1 plus W12 plus W3 X3. And then there's also gonna be a bias, I'll call that b1, and then passes it through some function. And this will result in a scalar, that is the activity of the first hidden unit of that hidden layer. Each one. Any questions here? Alright, and then if you want to know the activity in the entire conveyor, right, we can write this out not just for H1 but for h12 3.4. So let me write that right now. So if you want to write the activity of H1, H2, H3, and H4. What that would be is gonna be a matrix vector. Multiply. This vector here is gonna be the input vector X1, X2, and X3. We know that we get H1 by doing this transformation. And so this would be a w1, 1X1 plus a W12 x2 plus W3 X3. Then there will be a bias term here. Also, I'm running out of space. I'm going to really try to squeeze this in. B1, B2, B3. And then for the second neuron, H2, right, would have connections W2, W2 to W1, and W2 three. So that would be W2, W2 to W3, etc. There'd be a W4, W4, W4 three. And therefore you can see that to compute the value of all the artificial neurons and enrolling a bricklayer. Oh, I have to do is apply a backline transformation and then pass that through a nonlinearity. And so this pain can be written more succinctly as oh yeah, thanks Tom. Like before. This can be written more succinctly as H. This H here would correspond to this specter of the four H's. H equals F. And then we'll call this the vector of w's a big W, the inputs and x and then this bias. So here we see that linear classifier that we've talked about. The components of it shown up again over here. All right? And then one more thing to note, when I write f applied to a vector, right? So this, do you have a plus B is gonna be afforded the vector. When I write a function f applied to it. This means that I apply f to every single element of that vector. So there'll be f applied to the first, the second, the third and fourth elements. Any questions there? For this architecture? Is this not work acyclic? Yes. This network and everything that we do up until the midterm will be strictly before, which means that they're not even any feedback connections and therefore there are no cycles. Later on we will talk about recurrent neural networks, which will have cycles. Any other questions? Alright? So this first layer then, which computes the hidden activations of these units, we write h equals f of w and x plus b1. So these weights here would be my W ones. And then my second layer, which is the output, I would write as W2 times the inputs, which are this vector H plus another set of biases, v2. Okay? And then again, we should think of these. I put, at least in the case of c far as softmax scores. They tell me how likely in class one and class they told me the score of the image, any class one or class two. So these would go into a softmax classifier. And that's softmax classifier would produce some loss function, which is the loss function that we derived. Last lecture. We have Macs and it's the one that you'll implement on homework number two. One thing that you will have noticed and we'll talk about this later, is that generally we don't apply this function f at the output layer. Which means that after you do dependently or transformations, that will be a linear classifier. And we'll return to this in 10 min or so. But it's not a tight with the bebop. Great. Yeah, tomboy. I say softmax quarters. These are the un-normalized scores or the normalized. These are the un-normalized scores. So they can stay there before and go inside there before going to this. So maybe I should actually thanks Tom. Wait, I'll just call these scores, I suppose emphasize that there are normalized. Let's make sure that we have everything dimensionally correct or dimensionally correct in our minds. This w1 is going to take us from our 3D inputs to our 4D hidden units is going to be a four by three matrix. W2 is going to be a matrix that takes us from our four d hidden units to our 2D outputs as two-by-four. And then we'll have also biases, D1, there'll be applies for each hidden unit, so there are four of them and then also be output for each bias. And so that's going to be an art to nomenclature wise. We would say that this network has two layers. And then these two layers, there are six neurons. Neurons are these four and these two. And then we also calculated the number of weights it has. So the number of weights would be the number of weights and W1 and W2. So that's four times three plus four times two. That's 20 total weights. And then there are a total of six biases. And so in total, this neural network with 26 total learnable parameters. Any questions? Yes. The question is, is the softmax function presence in z? Yeah, So these, these, these are the scores that are then transformed into softmax probabilities. So there is not any additional money earlier. This is the linear transformation part that gets you softmax, that gives you the unnormalized scores. And so the Z is what you apply the softmax to. So eventually you'll do g of z. And this will give you softmax probabilities. Where the GSR softmax function, right? Oh great. So the question is, does this comprise another layer? So this softmax operation isn't thought of as another layer. Generally a layer is thought of as things associated with these linear transformations. But even if we had like several linear transformations with no nonlinearity, nonlinearity between layer. That's a great question, thanks. Other questions. Alright. Yeah. Reveal example. Discretion that we do not need. The slope. Actually be able to leave that space Destiny. Tom ways, making a really great point. I'm going to reserve that for just a minute. She's like to combine all I'll be emphasized. Alright, so I'm just showing you here now a three-layer neural network. And so in this case we would have two hidden layers. Our input would be three-dimensional, but then we would have two hidden layers. This one would be four-dimensional, this one four-dimensional. Then again, our 2D output. In this case, the transformations that describe these neural networks are straightforward generalization of a lever to talk. So H1 would be our nonlinear function f. Our function f in general applied to w1 x plus V1. So that if w1 and it'd be one here, and then H2 would be f of W2 times H1, the activity in this way, plus some biases be two and that would give me the activity H2. And then similarly, for the output, there'll be a W3. And if D3, and again, we don't apply that function. And these networks are called fully-connected. We usually abbreviate them with FC neural networks. The name is pretty self-explanatory, but it's because there's a connection from every single neuron in one layer to the neuron and the next layer. So because there are all of these connections that can be learned there called fully-connected. Then another name that you might. These cold is a multilayer, perceptron and LP. So if we say MLP or Fc network in this class, that's referring to these types of neural networks. Alright? So this slide has something works out with the best exercise for yourself. You can make sure that you can compute the number of weights and biases for this three layer neural network. All right, these are very straightforward. Yes, great. So Jake is just confirming that if we were to look at any of these weight matrices, the number of rows will be matched to the number of outputs and the number of columns will be asked the number of inputs. So W2 here would be our four-by-four, where the first floor reflects the number of neurons and H2. And just for reflects H1 and actually vote had been more care should be W1. W1 would be four by three. We haven't slide here to show you that these neural networks, as you see them, they look really sick for two implementing data, but also straight towards implementing code. So this, these four lines of Python code implement this neural network that we see over here. If you're not familiar with this lambda notation in Python, what this lambda notation is saying is that we're going to define some function f. So that's equivalent to saying define S. And then the input is x that corresponds to this x over here. Then it's going to return the value of this. And so this is going to be back times x greater than zero. This function might look kind of mysterious to you right now. Don't worry, we're going to talk about in this lecture. But this will be a common nonlinearity. But we'll talk about it when we would've made the nonlinearities. Alright, So then in Python you would just do each one. The activity of these neurons would be w1 times x plus b b1, and then apply that function f. Same thing for H2. And then lastly, we have our outputs. Alright, hopefully straightforward for the architectures. Any questions? All right, so moving on, let's start to talk about the function. So the first thought is what it is linear. This even simpler example, we'll say at this the identity. So the identity, then these are the equations. So each layer of the neural network. And the point of this slide is to say that is that it is identity or f is linear. That the entire overall neural networks tasting here. Alright, how do we see that? Let's look at H2 right here. What I'm going to do is I'm going to write and I'll rewrite it up here. H2 is equal to W2, H1 plus B1. What I'm going to do is I'm going to plug in the value of h one into this equation. So from layer one, I know that h one is equal to w1 x plus distribute two plus b1 plus b2. Let me now extend my terms. This is going to equal W2, W1, W2, B1 plus B2. Now what I can then do is I could say that W2, W1, right? It's gonna be a new matrix, I'll call it w tilde. And then these two vectors together, I'm going to call a new vector B tilda. What you can see is that H2 is also a linear function of my inputs. If you were to propagate this through all the way to Z. The point of this slide is to say that if f here was identity, or more generally, linear, even though you have many layers in the end, Z is still just an affine transformation of x where WE is gonna be the composition of all of these individual WAS. And there's also a bicep and composition of this extension. So you have a linear classifier. We know that it's limited. Anybody could use newly drawn lines between facts and it can't solve more complicated problems. So if f is linear, then we haven't really gained two bugs or we haven't gained any capacity or watering. Alright, any questions there? Homeless question is, is the dimension of Z always less than x? And many applications that's the case, but it's not always true. I'm actually going to take this as a jumping off point to then say, are there any cases where f might be useful, even though, I'm sorry, what were ef thing? Linear can be useful even though it's linear damper. And the answer is yes. So it could be useful in theoretical settings where you want to simplify the problem and therefore you want to study the linear version of a neural network. Another example which will come up in homework number three, and that's what I'm going to talk about it here, is that you can use a neural network where f is linear to still do some pretty cool things. And one thing that you can do is called dimensionality reduction. I'm going to talk about an architecture that you'll see in homework three is called an autoencoder. And the idea is that you start off with some x. Let's say that X is four dimensional. And what you wanna do is you don't want to work with a four dimensional input because maybe you want to visualize it and it would be really nice if it was, say, two-dimensional instead. Alright. So what you might do is you might design a neural network that looks like the following. I have my four-dimensional x. I'm not going to make my hidden layer just two-dimensional. So she's going to have two units. And then I'm gonna make my output be four-dimensional. And then this will be a fully connected network. So there's gonna be connections between all of these. I won't draw all of them. But hopefully you get the idea. This is a really interesting architecture. Because what we can do is it can make a low-dimensional representation of your x. If you ask the following, you create a loss function L. You take the loss function is z minus x squared. So what you're saying right now is I started off with an x and I want my output z to be as close to X as possible, right? So z is also four-dimensional and hopefully it takes on the same values of x. But to reconstruct z, you have to go through a two-dimensional bottleneck. Alright? So in this architecture, if it were to work, what your neural network has to do is it has to take your inputs x, squeeze all that information into just 22 dimensions here, engage, and then try to read out that information to reconstruct x at the output as the squared loss is going to penalize to make sure that it's being extra close to each other. Alright? So you could do this with non-linear neural networks. You can also do it with linear networks. And that's an example where even if f is linear, so you could do useful things with it in this case, dimensionality reduction. Any questions there? The question is, what is the relationship between or how effective is this method compared to other dimensionality reduction methods like. So. Actually in the case where these are linear, there's a relationship between this very simple than your auto-encoder and PCR. Great. The question is, for an autoencoder, would it be better to use a nonlinearity then to have a linearity in general? Yes, because with the nonlinearity, you would sell debt for power in terms of the nonlinear or the hidden representation, the dimensionality of the dimensionality reduced representation, a non-linear function of your, of your original scapes, Right? Yeah, The question is, is it useful part of this being able to look at the two-dimensional hidden states. And the answer is yes. So this is a form of so-called unsupervised learning where they're trying to find structure in it. And basically these two, these dates are going to be to the states that capture the important information in X and Ben could be e.g. visualized. The question is, when working with neural networks, do we have a choice of the number of neurons and number of layers, or do we not have a choice? We do have a choice. You can choose those as our hyperparameters that you'll find via cross-validation. And I believe it's homework three where we started asking people to certain accuracy classifier or homework floor. And later homeworks you're going to have that freedom to adjust the number of neurons to classify C bar as well as possible. I'm going to take one more question here. The question is, does it help to make the autoencoder symmetric? So actually the students are there constant I mentioned the relationship between autoencoders and TCA is when they are symmetric, but it can be viewed as incremented PCA. In general, you can make that decision if e.g. your data concerning, you don't want to overfit. But if you have enough data, then you will have more modeling power. If you don't constrain these, these what are called encoder and decoder matrices. Alright, let's move on. So it is linear. We haven't increased the power or the capacity of the neural network. So to do so, we should choose f to be non-linear. Alright? And then this gets us to one of the major design choices in Northern Alberta start, which is, how do I choose this? Because I have an infinite number of non-linear apps to choose from. All right, a few notes. We talked about how these are called feedforward neural networks. Fully connected neural networks are multilayer perceptrons. F is usually called the activation function. It's applied element-wise to every element that is applied to the vector. And we talked about this also. There's no activation function on z, but we'll talk about absolute applications after we've talked about this already. So let me get to what Tom was saying, which is Conway was mentioning that we talked about how at the output of a neural network, we're going to have a softmax classifier. Alright? You all know a softmax classifier is linear. So why is it that we're doing this high-powered non-linear, non-linear neural network and then put in a simple linear classifier at the output. This is intended because the way that you can think of neural networks working is it takes data that is not linearly separable. But then through the actions of layers one to n minus one, It's applying all these transformations, f of w plus d. That's one neural network layer supplying all these nonlinear transformations essentially to unravel this non-linear, Non-linear data so that by the time you get to a softmax classifier, they are linearly separable. Alright? So again, you should think of a neural network. We have this example of this could be like polar coordinates that change this non-linear classification problem into a linear one where you could draw a line to separate the points, right? But you can't draw any line that separates the points on the left. So in this very simple example, right, renewal transformation that makes these linearly separable. But then in general, what the neural network is doing is this Getting to look at your data. And then through the machine learning algorithm is learning what are the kinds of features. I want to transform the inputs and two, so that by the last layer. And I can linearly classify them with a softmax output softmax classifier. Then these features don't have to be handcrafted because they're learned entirely through the learning process. In this example, this is a handcrafted feature of saying we're going from Cartesian to polar coordinates. Okay? Any questions? Yeah. The question is, I said that softmax is linear, but isn't the softmax function incorporating the exponential, so it's non-linear. So the softmax function incorporates non-linear functions in terms of the exponentials. But the overall softmax classifier is still a linear classifier. One way to think about this is when we have the softmax classifier, right? The thing with the highest score is going to be chosen as the correct class. We talked about when you're comparing spores through that wx plus b is just drawing lines. Linear hyperplane. In this case, the softmax function has a nonlinearity, but it isn't going to change the order of the scores. The highest scores still wins. It's just turning those words into a probability so that we can optimize. And so in the end, just until a linear classifier, because even after the softmax function, the highest score has the highest probability. Sorry, Your question was, is the nonlinearity to find different than what? Then those linearity in? Are you asking because I'm going to just take video and offline, particularly. Yeah, so this is some of my more vigorous here. So when I say linear classifier, I mean that the boundaries between different classes are linear hyperplanes. And I will sometimes refer to an affine transformation as linear, but you're right that I should be referring to it as math fine transformation. So this is formally an affine transformation, nonlinear transformation. Did that answer your question? When I say that, you mean Like e.g. when I was saying like ethane identity, e.g. here, it still makes it linear. The hyperplanes may be different because if you optimize this with gradient descent, you might arrive at local minima that are different. However, the point is to say that you don't get any increase in capacity or modeling power. Because if you want to separate two classes, that fundamentally cannot be with lines, you will never be able to separate them even if you add more layers. That's great. Yeah, So Tom way was addressing the question about how the soft classes of non-linear function. Tomboy said you can also think of the softmax function as teacher normal, normalizer, but that doesn't change. So ultimately in your classification for the same reason seven. Right? I didn't have an example here of the XOR problem, which I'm going to ask you to review on your own just in the interest of time. If you don't know, the XOR problem is one where you have two classes and they take on these points in a plane. You can't draw any line that separates this. But what this example goes through is if you allow there to be a nonlinearity, then you can perfectly classify them. So this is just one example of one neural network that can do a non-linear classification tasks, the XOR problem, alright, so please just review those on your own. Just really support plug-and-play. Alright, so there are a variety of activation functions and we're going to discuss some of the most commonly encountered ones to address this question. How do I choose? And in doing this, we're going to make good use of this good felon quote, which I will explain a bit in two slides from now. But the closer the following. One recurring theme throughout neural network design is that the gradient of the cost function or loss L must be large and predictable enough to serve as a good guide for the learning algorithm. Alright? That probably doesn't make sense to anybody right now. We'll talk about it and just two slides. So let me first introduce our first nominee, arity, which is the sigmoid. So I've used this notation because it's commented describe the sigmoid activation for neural network. You would say that f of x is sigma. X sigma would take the place of the African the neural network. This is a sigmoid unit. You've all probably seen it before. On the y-axis it goes 0-1. And it's nonlinear because it has these curves in it. And around x equals zero. The slope of this line is close to one. Alright? So she started, the slope of the line is not close to the slope of the line. The, sorry, what I meant to say is that this function is approximately linear. Alright? So let's come back to this quote that says that basically, when you are choosing to design neural networks, you want the gradients of your loss, right? So we're gonna have, our loss is gonna be a function of the weights. We want the gradient of the loss with respect to the weights to be large and predictable enough to serve as a good guide for the learning algorithm. Alright, so let's extend that this names. Let's look at our sigmoid unit. And our sigmoid unit. If I were to apply sigma two w. If I were to have a neural network layer with a sigmoid unit, well, we would have is, we would have our wx plus b. In this case, I'm just going to assume that we have one output unit. So instead of a big wx plus b, I'm just gonna write a w transpose x plus b. So this is the activity of just one neuron. I'm going to pass that through a sigmoid. Alright? So I take my neural network repair, that's w transpose x plus b. I push it through a sigmoid, and that's the output activation for this one neuron. This thing will be a function of the weights. Because remember, when I built the machine learning algorithm, I get to make the weights or do optimization to make the weights as good as possible to minimize the loss. Alright. Alright, so ultimately we want the gradient of the cost function to be large. So I want the gradients of loss with respect to my weights to be large. Why do I want this? Well, later on, we know that we're going to be doing gradient descent. Update the witness, right? And in gradient descent, what we do is w is going to be my old w minus epsilon times this gradient, w dw. So if this gradien"
"All right, everyone ready to get started? I'm sorry that this screen isn't working. If you have trouble saying, I asked you to move to the center of it. Alright. First reminder, homework number four is due this Friday. I'll put that degrade scope. We sent out a midterm and announcements on Monday and please read it carefully for details on the midterm. And if you have any follow-up questions, please post them on Piazza. We've announced some of the other midterm details, but the new one from the email is to make space. We reserved three other rooms and rice Hall for students also take the exams there, and we're doing that based off of last name. And so you have a last name beginning with KRL them last elliptic examine voice 1234, etc. In these rooms there are about 56 per room, and they look like this. So the C-star tiny bit more spaced out than in this room. But we think that if we flip the students, several of you in these rooms and that'll make more room here. Alright, so please follow those assignments. Any questions on any image from logistics? Alright. The midterm exam review session is tomorrow in young CS 50. And the TAs are going to upload the review problems by 09:00 P.M. tonight in case you wanted to take a look at them ahead of the review session. And again, we encouraged you and strongly recommended you attended the session or watch it later on. Friendly reminder that this Monday is the President's Day holiday. It's a holiday for UCLA and therefore there's no bacteria or office hours on Monday. And then my Wednesday office hours after class on Wednesday when we have the midterm exam, they're going to be canceled because the TAs and I have to scan all of the exams after the mid-term on Wednesday. All right. Okay. Any questions? All right, let's get back into material bank. Alright, so last lecture we introduced the architecture of the columns shelf drill number. We're going to just recap what that architecture is. And then today we're going to essentially into a case study of the really influential convolutional neural networks that have led to this deep learning revolution. So remember, what happens for a convolutional layer is we have some input to a layer. That input is going to be a 3D tensor with some width, height, and depth. And what we do is a convolutional layer is going to comprise some of mounted filters. Filters for each filter is going to have some width and height, and then it's going to have a depth that's matched to the depth of the input. Alright? And when I do the convolution of one of these filters with Mike input, I get out a matrix. So if I had NF filters, I'm going to have an F matrices. And matrices will be the convolution of each of these filters with the input. What I then do an accomplishment layer, if I take these NF matrices and I stack them together to make a new 3D tensor. So let's say that I started off with images from CFR that were 32 by 32 by three. If I were to convolve with 100 of these filters, I would have 100 of these matrices. Can I stack them together? And that becomes my new 3D tensor. That is by activations for the next layer. Alright, and then the next player might convolve with some other number of filters and that would give me a new 3D tensor. That's okay. The question is, can you think of this as a fully connected layer on the channel? The channel double, meaning that, right? Yes, because I think I understand the sentiment of your customers. You said you're always combining all of the datasets for the local patch state you're in. So you can think of it. Great. The question is, where is the plus one from when we did the parameter activation is coming from. So basically every single one of these filters, if this was say, a five by five by three filter. First one here. The number of wastes would be five times five times three, the volume of that tensor. But then we have a plus one. Because each of these filters also includes a bias term that's just added. So that's where that plus one comes from. Other questions. Okay. So that's the basic convolutional layer. We're going to do several case studies. They're going to become more familiar with this. As we look at these examples. I'm sure we talked about how all of the convolutions are so-called valid combinations, which means that we only do a convolution with a filter fully overlaps the input, right? So that would be at these four locations given by red, green, blue, and purple here. This means that if you were to compute the output size based off of input having a width w and height h, and the filter having a width WWF H, F. The output will always have a width w minus wi plus one, and a height h minus hf plus one. So as long as the filter is bigger than one, your output is gonna get smaller and smaller. So we then also introduced last lecture this idea of padding. So sometimes to keep the output the same size as the input, we'll go ahead and pad with zeros. And if you had with hat equals one, this means including one surrounding zeros and foot. I'll put it the complication that becomes these dimensions. Alright? And then we also talked about strive last factor, which means we do the drag of the convolutional filter. Stride equals one, which is the default that you're dragging this filter over by one all the time. But if you wanted to track it over by two, then you would set a stride equal to two. Now you have tried that will reduce the output size. All right. Any questions on padding or stride? Yes, that's correct. So the question is, when you tried by two, you're moving over by two. When you go to the right. But then when you go down, you also go down by two. So when you strike by two, you strike by two also in the vertical direction. Question is, is there ever a case where the stride size is bigger than the filter size? I've never seen that. Because then you wouldn't be missing entire sections of your input. Question is, how do you know whether the stride is valid? So basically, I realized I didn't have this slide here. Let me just from our last lecture. And this is the output width and height for a value of stride. And to make sure the stride is valid, this number should evaluate to an integer, not a decimal. Other questions. Alright? And then we talked about the pooling layer, which applies in operations such as if you have a max pooling layer, it'll look for the maximum value in your pulling size, in this case two-by-two. So something a two-by-two matrices. And it just extract some Epsilon. So here the maximum is fixed. Here, the maximum is eight, et cetera. So that's a point here. And this is the layer that is designed to try to introduce some positional invariant into your CNN. Any questions on the pooling layer? Yeah. Great. The question is, is there a reason the operation we do is max rather than me? So you, you can also do the mean. So that's usually called the average pooling layer. And we're going to see some of the neural numbers that we talked about today do an average pooling better than a Mac scoring? I believe that's pointing is usually chosen for empirical results, which that's a better performance. But you can do average pooling as well. Great. Yeah. The question is, to clarify, pulling is done channel wise. And that's correct. So when the input is a 3D tensor, so if you imagine that this input was a 3D tensor, you would do your pulling on every single matrix within that cancer. And then the outputs will be stacked. So you would do pulling on each individual matrix and then stack them to return. Also a 3D tensor. Right? Gentle thinks there's a spatial down sample and not the channel gram sample. That's correct. Their cousins here. Alright, let us spend debt into Linux. This will be the first. This is the convolutional neural network from me on LinkedIn and colleagues in 1998. It's the simplest one. And then after that we're going to go into. Alright, so this is the architecture of the input is going to be. So they use a dataset called or they used to. They wanted to classify digits. And instead of having these be RGB images, they were grayscale images. So these are just 32 by 32 matrices or not. Alright? So what they do is in the first layer, what they have is they have six, five-by-five convolutional filters. Okay? So let's go ahead and write this out. And these filters are applied at stride equals one and Patty equals zero. So the first question that we'll ask is, what is the size of the feature maps? That means the output of the convolution layer C1. C1 contains these 65 by foreign competition of filters. So what I'm asking you this essentially, when I apply six, five-by-five convolutions not alters. What's the size of the output? So for this, we're just going to apply the formulas that we talked about before. So when I do the convolution, the width and the height are going to change by w minus w plus one. So in this case, my initial image head was headed with it 32 and a height of 32. The thoughts or width is five, and then I add one. So the output of the convolutions are going to be 28 by 28 in width and height. And I can someone tell me what the doctors did. You just set it up. Perfect. Yeah. So I have six of these. So in total, the size of the feature maps are 28 by 28 by six. So that's what's drawn here. These are the six feature maps, each of them 28 by 28. How many parameters are there in one layer? So here we have six five-by-five convolutional filters. And so each five-by-five filter, It's going to have five times five equals 25 parameters, right? And then remember there's that plus one for the bias. So we're going to do it plus one for the bias. So this is the number of parameters in just one folder. And I have six of them. So in the first one layer, I have a total of 156 parameters. He does anyone want me to do that calculation again? Alright, great. We asked them signs up. Okay. Okay. Yeah, yes. The tongue way of saying if he asked for feature maps, the feature maps refer to just want to be. So let me just say size of, let me change these off the size of output, just to be clear, of C1. And then this will also be size of output. Size of output. Right? So the next after that, we're going to apply a pooling layer where the pool, the pools are two-by-two and they're applied at a stride of two. So we know that if you do this two-by-two pooling layer, where the filters are two-by-two hundred provided a stride of two. And I don't have the equation here, hasn't in the prior lecture. The output of the pooling layer will be w minus four, which is two, divided by the stride, which is two plus one. So we'll have a Tony eight minus two, which is 26/2, that's 13 plus one, that's 14. So the output of this pooling layer is going to be 14 by 14. And then just like Daniel is asking for the fight on each channel in the deaf separately. So if I had applied, I applied on this first matrix that gives me this. Then I play on the second matrix that gives me this one. The third matrix that gives me just wanted to say that this will be 14 by 14 bytes, six. Alright, and then how many parameters are there in the pulling? Someone who just have the answer? Yeah. So remember for the pooling layer, all we're doing is we're looking at some values and then taking their maximum or take their average or some other operation. There are no trainable parameters there. Alright? Saying for the point operation, is there a ceiling or floor? What if it's not an integer? Oh, you mean this equation? So again, just like for the convolutions, how the streets had to be valid. The stride will have to be valid for the full officer. So you need to choose a stride such that w minus w p over stride is. Before we do cited the output of s3, I want us to do a different question. And this one I'm going to ask you to think about. Number of parameters in S3 or third, comparable or display or C3, which is a convolutional layer. It contains 16 five-by-five convolutional filters. Okay? This is what width and the height of this bump there. Alright, take 15 s to think about what expression would give you the number of parameters in later S3. And then I'll ask someone to give me the answer. All right. Can someone raise their hand and tell me and you don't have to tell me that exact number to say like five times, five times, whatever. How many parameters are in this compositional layers. Okay, So the initial answer is five times, five plus one times 16. Who agrees with this answer? And if you don't agree, what would you change about it? Perfect desk. So I asked you guys to think about this one because I noticed that it's just something that's easy to forget. We will usually always write the filters in terms of their width and the height. And we're going to leave off the depth. The depth is always assumed to be matched to the prior later. So because the prior layer was the output of this pooling operation and there are six that adaptive six. Remember them two convolutional layers will be five by five by six. So the number of parameters in S3 is five times five times six. Because this is the depth input to this layer. The question is, isn't it five instead of six nodes? Six, because the pooling layer, I'll put his 14 by 14 bytes six, so it's matching. So that's the number of parameters and C3. If we want the size of the output of s3, then what we do is we do the width minus the filter with the plus one. So in this case we'll have, let me do this calculation here. The input is 14, the width is five. Now we do a plus one, so this is equal to ten. We have 16 of these doctors to be. The size of the output of s3 is ten by ten by 16. Any questions on any of this population? The question is, do we assume the input is base coat for this Linux, the inputs word grayscale. So the first one is 32 by 32. The question is, why wasn't there a gap? And the first one layer is because the input image was grayscale. So this is like, this is just a 32 by 32 matrix, ready to register 32 by 32 by one 3D tensor. And so the depth is equal to one. I'm sorry, what's the question? Does the output of which layer? I'll put up c3? Yes. So the output of s3 will have a depth of 16 because there are 16 of these structures. Sorry. The picture to have in mind again is when we do the complement cell layer, the filter Jeff has always matched to the input. So the result of the convolution is just a matrix. And then if I have 16 of these filters, I'm guessing, I'm going to concatenate these two matrices together. Are you referring to these five by five filters? The depth of these filters is six. So we could have written these are five-by-five by six convolutional filters. Because remember in a convolutional layer, the depth of the filter always matches the depth of the input. So that's kinda the checking using convention. We just write the width and the height, but you always have to remember that there's got to be smashed to the fire. Alright. Any other questions? No. So let me just explain one more time what's going on here since I, to make sure everyone's on board. So the layer C3 is a convolutional layer where the input is 14 by 14 by six. When we say that we have a five-by-five convolutional filter. The convolutional filter will always have a depth that's equal to the Delta B input. The convolutional filter always has six investigations to be inputted. Six, that's quite a number of parameters, will be five times five times six plus my bias. And so one of the filters convolved with this input is gonna get the a matrix that is ten by ten. But now I have 16 of these filters. And so that's why the output of the sphere is ten by ten by every day. Write down the dimension, becomes very clear that one of the great, yes, it's almost right down the dimension itself. We can put it everywhere because CNN tells you what the filter data should be, right? And we're gonna do this many more times today. Don't worry, use the CT scan. Are there questions? All right. So that is some examples of sizing on this convolutional neural networks. We're going to just show a few notations and then we're gonna get into studies out. Alright? So usually in a convolutional neural network, they will be comprised of several convolutional layers are stacked together with a ReLu in-between. And we're going to use this notation called Plate notation, where if I have a convolutional layer, an array blue, and then I drop box around it and I write a little n in the corner. I'm going to repeat that concrete root n times. So typical architecture for a convolutional neural network is that we're going to have the peaks of a Combray loop com, for a loop. And you usually, we're going to see this N is going to equal two or three, usually will staff, you know, two or three concrete. Concrete is concrete lose. And then after that is going to be followed by a max. Cool. Alright. And then we're going to have a bunch of ANDs, three called Rayleigh's, followed by a max pool stat that's displayed to n. After this, we're going to use k fully connected layers. And this is just something that we'll commonly see, although we'll see later on in architecture called Google that removes fees. And then finally, we're going to have our softmax output. Alright, so this is a typical, fairly conventional CNN architecture. All right. So with that, we're going to go ahead and give each state buddies, right? Yeah. So there's only one maximum. Yes. So what this is saying is that for every, let's say n is equal to three for every three called ReLu calibrated comp rabies. Sir. For a comprehensive cooperative new comp, remove it. Then we'll go through one max score. Alright? And then that architecture of comp ReLu, ReLu, cooperating back school is repeated n times rate. The question is, do we generally will, do we generally have batch norm? Yes, batch norm is something that you will watch to insert here. It's not going to be present in the neural networks that we discussed today because batch norm was made after these ImageNet competition. All right, okay, so we're gonna start off then with this case. Sorry. Okay. Now, before we aren't gonna do a bit more until next. So it's the one that we were talking about. There are just a few calculations I also wanted to do here. The first is how many connections are there? The first convolutional layer. So if you want to put Lynette paper, they're going to give they want to say something in that paper like that, there are 122,304 connections. And the first way I'm walking over that number comes from. So this also is going to give us some idea of nomenclature. So the first thing that you might wonder is what is meant by connection. So these 28 by 28 by six, I'll put one layer means that I have 28 by 28 by six artificial neurons. And I've drawn in red, one of those neurons is, alright. We know that this neuron is the output. The input convolved with a five-by-five filter, where this neuron is the value when that five-by-five filter is applied at the top-left. Alright? So to compute the value of this neuron, right? We know that we multiply the input with the filter. And we. Sum up all the values. And there are therefore 25 connections for the five-by-five filter from the input to this one neuron. There's also the bias. So there's gonna be the plus one for the bison, but we'll call that a connection, also positive connection. I want to know how many connections there are in the first layer. The first thing I'm going to say is how many connections this every single neuron in the output of the first layer had. So every neuron. Tomboy, isn't the bicep applied per filter? The answer is yes. So John McCain still causes the connection. So basically there's a bias b. Let me draw this in red, also a bias p for this one filter. And even though this bias b and this filter is five-by-five filter, or the same filter that I then used to calculate the value of this neuron, right? I shipped to filter over. Even though the bias b and this filter weights are exactly the same, the call each other connections. Alright? So every neuron has five times five plus one connections. Again, those corresponds to the five-by-five wasting my full term plus the one part that I want to know how many connections that are then all I have to do is then say how many neurons are there, and then multiply it by the number of connections per neuron. So let's write that out. The number of neurons I have. The output of my first layer is going to be 28 times, 28 times six, because that's the output of my first comp layer. And therefore, the number of connections is going to be the product of these two, the number of neurons, which is 28 times 28 times six, multiplied by the number of connections per neuron. And that's five times five plus one. And this gives you that number that you'll see in their paper, 122,304 connections in that first layer. Any questions there? Yeah. The question is can I explain why we're concerned? But the number of connections, it can give you a sense of the volume of calculations that you're going to be doing. But primarily in exactly like this. I think it's more for teaching purposes to them, you're understanding exactly responding on compositional errors. Like I said, this will help you off to get a sense of the hardware required because it tells you the number of operations that you need to do for this application. All right, so with that we'll go over the entire texture. So I'm gonna be using this notation. Where for every single layer, when I say layer here, I mean I just use a different version layer. Usually layer, layer is going to refer to either, either a conversation on transformation. So even though there are seven or eight steps here, this neural network only has four layers because that only counts the convolutions or fully connected networks or linear transformation. So C1 here is one layer, C3 is another layer, as tutors mark Kennedy layer because a point it's not canister layer. There's another pulling that happens here that doesn't cancel. Layer C5 is another convolution, that catalyst layer, that's the third layer and then F6 is my fully connected and that calf for here. So therefore total layers. But then every single computation, we'll give it a number. For each computation, I'm going to write the app, the size of the output. So the output of the first column player is gonna be 28 by 28 by six. And then I'll give a description of what's in that layer. So this will have six convolutional filters, each hip and five-by-five apply to strike one. So then after that we have the pool that we talked about, two-by-two pool with strike two. And then the lunette didn't use a max pool. They actually did a pool that how to train your book coefficient, but this is not used today, so we won't go into the details of this. And we've already calculated that the output of this pole is 14 by 14 by six. Then after that, we have another convolutional layer, 16 convolutional filters but with five and high-five. And then as per the questions that we just did on the prior slide, remember that these have a depth of six. Because this depth of six is always going to be matched to the depth of the input of the prior layer. So always remember that when computing, now for our parameters, there's layers. Okay? I said Tom was saying John McCain and others call something a layer if it attaches trainable parameters. And so these comps, layers that are trainable parameters. All right, So tomorrow is asking you about the details of this pooling layer. Told me I'm going to stay just take a look at the paper for that one. So it's not used today, so we won't spend more time on it. But he has a specific point there here that's not no longer be used. After this convolution. We go into a pooling layer with two-by-two filters applied at stride two, but a couple of width and the height by two. Alright? And then after that, there's one more convolutional layer. There are 125 by five filters. Five by five filters are going to have a depth of 16 because the input from the past point where has it up to 16? Because the width of the filter is equal to the width and height of the filter or equal to the width and height of the image, then the output is just a scalar. And because I have 120 of these convolutional filters, that just gives me 120 numbers. Right? After that I have a fully connected layer, that's my linear layer. And then after that, they do a comparison to them. They have a loss function which computes in a square error. So this is the architecture. You wrote it in terms of something like the plate notation. We would say that there's a column for column four, that's this times two, I'll apply a comp, sci and MT output. And that is just a concise representation of these $0.70. Okay. Any questions on this architecture? Great. Question is, can I explain why? For number five, the size is just one-twenty. So remember the output of the convolutional filter, sorry, a vicinal layer will be stacking the outputs of each filter. And for each filter, the output width and height will be w minus w plus one. So in this case the input width was five. The output with the thoughts are with this five. And then we have a plus one. So the output is just one-by-one. The way to picture this is that in this convolution, because the filter is the exact same size as the input. There's no drag that. You just put the filter on the input and you can drive it at all. It's only one vowel combination. And so the output is just going to be a scalar. And then I have 120 of those. So those 120 scalars gets stuck together and be after the fat layer. It's just 120 numbers. Justify understood. The question is just summarizing the architecture, which is saying it's architecture. We have cough, columns followed by pools. Essentially reducing the size of the features until we have a fully connected layer at the output. That goes to my ten classifications. That's correct. Answer your question or was it? The question is, what are Gaussian connections and why are we using MSE? I'm not gonna go into those details because they're not used today. Instead of the best practices if you have ten outputs, right, we would use a softmax classifier with ten, with ten classes. And that's what's going to be done for all the image guy, CNNs. But back in 1998, that was the prevailing way to classify these statistics. They actually had a template for each digit that they computed. The mean square error. For the shoes are not used to date. What if the reason they're not as well as performing territorial empirically, they're worse. Alright, so let's move on then to the ImageNet classifier. So now these ones that we're going to start talking about, beginning with outstanding 2012, carrying practices that are very commonly used today. So we're going to start off with looking at our tech, which is that first CNN that really reduced the top 5% error classification on the initiative dataset. Right? So before we talk about the architecture of AlexNet, I'm going to talk about some of the data augmentation and organizations have been used. And these are the things that should be familiar to all of them because we've talked about them in prior lectures. So first off, AlexNet is round and this next initiative we know is that dataset that has natural images, that volatile wasn't one of 1,000 classes. Alright? Images are usually relatively large. So what happens is because the Internet has variable sized images. Our first crops them to all be the same size. So what they do is they'll take an image and they will either resize it or if down-sample it so that the shorter side is 256 pixels. So the image may not be a square. Maybe it looks like this after cropping or down scaling. The shorter side is 256 pixels. The longer side to something larger than that. What they do is they crop out the central to the six by six pixels. So this operations says take the center square of this image. This will be a 256.2 56 width. And then the actual inputs of the CNN are then going to be this augmentation off of the image. So for this district, what they do is then they cropped out 224 by 200, 204-20-4204 crops of this image as ways to augment their datasets. So they might take this to 24 by 224 crop here. And then maybe on the next crop, it's going to end up being this to 24, 24 crop. Then those costs comprise different crops that are all now inputs to the AlexNet architecture. So in their paper, they say the inputs are 224 by 224 by three. We're going to see that if you apply their architecture to image at this size, it doesn't work out, which means that they have a bug somewhere. So we're gonna just assume that the inputs, perhaps it popped up to 27 by 227 by three. Alright, that's just a minor bugs somewhere in how they wrote the paper. Then we're gonna subtract the mean image over the training set, just like we do for our kind of nonlinearity. Alexnet is the architect enough to popularize the paper. What they did is they trained various of these convolutional neural networks where they use the ReLu after the convolutions and compared it to applying tan h units after the convolution and presented as over here. So what they do on the x-axis is they show the number of training epochs needed to get some training error. Lower the better. And the dotted line is tan h and the solid line is rabid. So they saw that change approximately six times faster than tan h. And therefore they use ReLu for their architectures. And so when we write out the architecture, remember that every fully-connected layer and convolutional layers followed by a rabid. Also train on multiple GPUs. So if you look at this image from their paper, this is the actual image. I didn't crop it to remove the top part of it. What happened is that back in 2012, they had GPUs, but just regular bytes of memory. And that wasn't enough memory to hold all the images that they needed to train on. So they literally split their architecture into two paths. So their first layer would have 96 convolutional filters. 48 of this would be put on one GPU and 48 on the other. Alright? So this was hardware constraints. But then say to do this step has a few other details. They use something called local response normalization. We're not going to talk about this because the next two architectures from now, VGG net, they determined that local response normalization doesn't help. If you're interested in learning more about this. Feel free to drop by my office hours is basically an intuition that if a given layer, one neuron is really loud, it's just silence for the other ones. That's something from biology. But again, made her paper so that this doesn't help. And then they're pooling layers are done with some overlapping. So usually pooling layers are done with the stride equal to the width of the pooling layer. But in this case for Alex that they have some overlapping pair dataset augmentation. So in addition to the cloud, So they did they also, after extracting out the 22224 pixels, they took their horizontal reflections. And therefore they had in total ten images from one, because it took five patches over to 245224 cluster reflections. With these ten images and put into AlexNet you again at 10:00 batch distributions. What they would do is they would output the 10th softmax probabilities together. And you can think of this as a sampling across and put examples. And this reduces the error rate by 1.5 per cent. Also did a few types of color augmentations to the images. And this also had the effect of reducing the top one error rate. And then finally, they didn't drop out with p equals 0.5. And they found that this substantially reduce overfitting, but lead to ten times over twice as long. Questions on any of these augmentations has accepted. The 1% is an absolute error rate. So I believe that the error rate, the report is either 16.4, 0.2, 0.4% error rate. So this 1% means like if they were to do this, pull their occupation. If they were not to do it, it would have gone up to 17.4%. Yes. So let me just repeat what Tony was saying, which is clarifying the crop. So basically when you get a test image, when after the crop down to the residency, but it's expected that they would take five to 24 by 224 prompts, as well as their horizontal reflections. So this gives ten subset images. Those images would go to AlexNet reading ten softmax probabilities are ten softmax distribution, then it would average those together. Any other questions? Question is, is the augmentation done at train time or just for testing? It's also going to turn the question is, is there a reason they chose 24 to 24 by 224 rather than say like 250-"
"All right, everyone we're going to get started for today. So the only announcement that we have right now is that homework number six was released last week, again, and last week. And it's going to be due nine days from today on June 2, 2021, uploaded to a grade of scale, all right? Any questions on any course logistics? All right, so we'll go ahead and continue where we left off last time, which was we were now going to derive the comment filter. So last lecture, we spent time discussing the intuition of the comment filter and how essentially it's using information from two sources of information, a state update process, which is this xk equals axk minus one equation, as well as the neural data and its relationship to kinematics, which is this yk equals c times xk equation, right? And so we talked about the intuition of that and we presented the comment filter solution and saw that it made intuitive sense on a few simple scalar examples, all right? So now we're getting into the comment filter derivation and that's where we're going to resume today. So a few things just to remind you of that we had these a few preliminaries. We had that if y and x are random vectors and they're linearly related like this, that the expected value of y is a times the expected value of x plus b. And the covariance of y is a times the covariance of x times a transpose, all right? And then we also had talked about Gaussian random vectors and these were these random vectors where they're jointly Gaussian and they can therefore be written in this form where the random variables, which are elements of this vector, they all have some means and then some covariance matrix. And the facts that we talked about last time were if I were to marginalize over this random vector, so say I have this random vector with x1, x2, x3, but I just wanted the distribution of x2. So I marginalize away x1 and x3 via my law of total probability, right? If I do that, then x2 is a Gaussian random variable with mean u2 and variance 4, all right? And if I want to marginalize, so I'm just left with x1 and x2, they would have mean 1, 2 and then this covariance gets in by this block here for the 1 and 2 indices. So marginalizing over this Gaussian random vector results still in a Gaussian random vector. If we have linear functions of Gaussian random vectors, they remain Gaussian. And then this one, which again, we'll talk about in more detail if we get to it today, which is if we condition with a Gaussian random vector. So if I have a Gaussian random vector where I'll split up this elements into two components, x1 and x2, then x1 given x2 and x2 given x1 are also Gaussian."
"All right, everyone will go ahead and start office hours. Let's use the raise hand system to take questions in order. Tyler. Professor, I, well, I guess, let's, I want to start off with the very start problem, one, a on the homework. Can you go over the math behind showing how the properties change. Okay. So question one a says, when f of t is periodic f of t is real. We have seen some properties of symmetry for the four a series coefficients. How do these properties change when f of t is purely imaginary. All right. Can someone who maybe to this question. Let us know your approach here."
"All right, everyone, happy to take any questions. I had a quick question about number three for the homework due tonight. Yes. Sorry. All right, question three. Yeah, for part B, I wasn't sure if my approach to this was correct, and I heard that one of my other classmates had used a different approach so I just wanted to verify my method. Sure. What I was thinking was that I would take the area from 0 to 1 millisecond and divide that by the area from 0 to 20 milliseconds. What was the reason for dividing by the area from 0 to 20 milliseconds?"
"All right, everyone, we can go ahead and start office hours. Please raise your hand if you have any questions and then I'll call on you to unmute yourself. Alexandra. I was wondering if we could go over number two. Should we go over number two from the top? I think I get part A. I just want to make sure I'm interpreting the diagram correctly and then from there on. I'm not sure how to deal with the complex exponential in terms of graphing. Yeah, okay, sure. For the complex exponential in terms of graphing, I think that we may have given, we gave that hint to approximate as one plus j omega t and then for the j, you can just approximate it. You can draw it on the graph as it dotted line, but we'll get into that. People write in chat if you want me to go, actually, you know what, we'll just do number two from the top. All right, thanks for asking me the question."
"All right, everyone, we're going to begin now. So first, the TSO uploaded homework number seven last Friday and homework number seven is going to be due this Friday on the last day of classes at 11.59 p.m. uploaded to grade scope. The first three questions on this homework are on sampling and because we haven't fully covered the Laplace transform yet for this quarter, we gave you three questions, four, five, six on homework number seven, which we are going to grade based off of correctness, which is based off of effort. And so as long as you make any attempt to do the question, you're going to get full credit. And so in that sense, we want you to think of these more practice questions for the final exam. The final exam will be commuative and it'll cover material up to and including the inversion of the Laplace transform, which we'll start today and then we'll finish in the first half of Wednesday."
"All right, everyone, we're going to get started for today. A few in essence before we begin. The first is that homework number one, a reminder is do this Friday, uploaded to grade scope by 11.59 PM. We're also going to release homework number two this Friday. And if you have already submitted homework number one, there are several of you. We ask that you please log in again and assign your pages to questions because when we initially made the assignment, we didn't have the outline. So the outline is there now. And so again, if you've submitted homework number one, please log in again and assign your pages to the question outlines. And then for everyone else who's submitting, please be sure when you log in to assign your pages to questions so that the graders know where to look for your work for each question. I also wanted to answer a question that I saw on Piazza, which is in the homework, there's going to be a signal that you'll see with infinite energy and zero power."
"All right, everyone, we're going to get started for today. First, a reminder that midterm will be during the next class time. So it's going to be a Monday, May 3rd in class. Oh, we'll go in class already. In class on Monday, May 3rd, 2021. We, again, on Monday, posted details about the exam as an announcement on CCLE. Be sure to take a look over those. And if you cannot take the exam during the in class time, because you're in a different time zone, or I previously approved it, please send me an email following the format of the announcement on CCLE. Then a reminder also that past years as midterms are all uploaded to CCLE and that the 2017 midterm was too long. So keep that in mind if you study by doing that exam. It was also a bit on the hard side too. You would expect that the exam should look more like it did in recent years. All right, any questions on the midterm? All right, so then another reminder homework number three is Tuesday, May 4th, 2021, the day after the midterm. And it has a good amount of Python coding."
"All right, everyone, we're going to get started for today. I hope everyone had a happy Thanksgiving and a good break. So announcements for today are first that homework number six, we sent an announcement on Piazza and CCLE last week that we have delayed the homework to be due this Friday, uploaded to gradescope by 1159pm. All right, and then start the Laplace transform. And then the rest of the class will be about the Laplace transform how to invert the Laplace transform and therefore has a high overlap in properties with the Fourier transform. So since we have gone extensively to the Fourier transform, hopefully going through the Laplace transform would be quicker."
"All right, everyone, we're going to get started for today. Our announcements are that homework number two is due tonight. And yesterday, the TAs released homework number three, which is a coding homework. And as per Tanmoy's announcement, we recommend that you get an early start. It's all on Poisson processes, but it's a lot of coding. So be sure to get an early start on that because the coding can be time-consuming, especially if there's debugging. Then our midterm is going to be in class a week from today."
"All right, everyone, we're going to get started for today. So a few in essence, just one announcement actually before we begin, which is a reminder that homework number one is do this one day up the fatigue rate scope. Any questions on any course logistics? Question from Jonathan. Yeah, I don't have a question on administrative stuff, but I have a curiosity on some of the biology. This is an okay time to ask that. Sure. Okay, so one of the things that we've been talking about is myelination of axons and nodes around VA. And I'm curious to know what is the scale of the difference in size between a myelinated section of axon and a node of around VA because a node of around VA can't be like a single point in space. It hasn't have a size."
"All right, everyone, we're going to get started for today. So just a few announcements before we begin. The first is a reminder that homework number four is do this Friday. And we're also going to be uploading homework number five. At the end of this lecture, we'll take some time to discuss the midterm. And I just want to put this in announcements so you all know we open up re-graves for the midterm exam for one week after we return the grades. And so if you want to get a re-grave on any midterm question, those are due by May 17th. 21. All right. Any questions or any questions on any any course logistics? All right. So we are going to continue with the screen classification. We're going to finish that in the first half of lecture today. So a reminder where we are. We had this model where we have neural data that's given by the X and the Y axis that comes from different classes."
"All right, everyone, we're going to get started for today. So just one announcement before we begin, which is that as per Tanwei's announcement on CCLE, we extended the homework number two due date to now be due a week from today. So it'll be due Monday, April 26, uploaded to Gradescope by 1159 PM. All right. Excuse me, any questions on any course logistics. Alright, so I want to remind you of where we left off last week which is that last week we started to look at plus on processes as a way to mathematically model. The spikes that neurons fire. Right. And so we're going to define the process more formally today. But what we considered was that there's a timeline on the timeline. They're going to characterize the so called inter spike interval, the time in between spikes and so little t one is the time from the process start to the first spike little t two is the time in between the first and the second spike little t three the time between the second spike in the third spider, etc."
"All right, everyone, we're going to get started for today's lecture. A few announcements before we begin. The first is we will release homework number one by this Friday. It'll be uploaded to CCLE. And then it's going to be due the Friday after that on October 16th. All homeworks will be due uploaded to Gradescope by 1159 PM. And so please be sure to leave enough time to make sure you can upload all the documents to Gradescope and that it doesn't push you past the 1159 PM deadline. We sent out an announcement with instructions on how to sign up for Piazza and Gradescope. And so if you haven't done that already, please go ahead and follow those instructions. And then lastly, we're going to send out an announcement shortly later today on the consolidation of discussion section so thanks for submitting the Google form that we sent out and we were able I believe just got an update from Tom, we were able to find three discussion sections that everyone will be able to make who wants to attend live discussions."
"All right, everyone, we're going to start lecture for today. So two announcements. First, we anticipate returning your midterm scores to you on Monday, May 10th, and we'll return those after class. And then the second is that today we uploaded homework number four to CCLE, and it's due in nine days on May 14th. The homework is it involves a lot of coding and also a bunch of math. So we really encourage you to start this early and you should be able to do most of the homework, which is today in class we're going to discuss how we use maximum likelihood to find the optimal parameters for our discrete decoding model. a likelihood under a constraint that some set of probabilities add up to one. And so the way that you do this is something called Lagrange multiplier that you might remember from a calculus class here, where instead of maximizing log l by itself when you have a constraint like this, what you can do is you could put that constraint into the objective function as well, and that constraint multiplies a lambda."
"All right, everyone, We're gonna get started for today. So a few announcements before or the day. If you are an MS Online students on good learned, we just such an announcement maybe an hour ago about the exam time. So it's going to be Saturday, February 25th, 1-250 PM. So if you're gonna sound like to me that you didn't see this massive. Please be sure to check through him as a reminder that homework number one is I'll put it in there and it's going to be due this Monday. I'll birth to grade scope. In homework number one, there is the coding components. And so to submit the code and components, please be sure to print out your Jupyter Notebook so that all of your code is because the auto-grader and the grader will look at that. Alright. Also had their solutions and Clarksville better. Readers need to see your code to give you credit for your car. So also in future assignments, you'll be editing d2y files and for any files. And yet it felt to my file to also have to print to PDF that codes of the Greater can see any questions there. Alright, so on Wi-Fi and how it affects the recordings. So we saw on Piazza that some of the Zoom recordings may not be totally reliable and that's probably because of the Wi-Fi issues that we couldn't happen. Today. We're having our MSM one TA can join the room, both be reliable Wi-Fi connection, hopefully the top of the quality. But please also remember that crew and cast upwards lectures. They're under UCLA to get reserved some Boomer. And those are also, we ran discussions for the first time last Friday. And for discussion once c, which is one of collides discussions, there is no reliable Wi-Fi connection in that room. And so for discussion once C, we will not have a simultaneous Zoom meeting for that. Any questions on this or discussions? Okay. And then the last comment is that we'll go to discussion video regarding the coding part of homework number one and that will be posted this Friday. Alright, any questions generally about logistics for this class? Alright, we're gonna get back into material. So last lecture we were talking about how to assess whether a model is overfitting or underfitting. So this is recap. We talked about this procedure called k-fold cross-validation. Where what we'll do is we'll start off with our original data, right? The first thing that we do is we split the original data into a testable than extreme fold. The test bulb is set aside pristine dataset that you clarify once at the end to score your model. Alright, but then for the remaining training data, for 24 fold cross validation. And what we've reduced, we would split it into four segments, all of equal number of trials. And then we would designate three of them for training and one of them for validation. Validation is to optimize our hyperparameters. Remember that these are the choices in the discipline, the algorithm that you as a designer get to choose beforehand. And what you would do is you would train your model on the green training folds and then you would evaluate its performance on the yellow withheld validation fold. It gives you a validation accuracy to score your model. And then you can keep doing that for different sources of hyperparameters. And then you can also switch which folders, the validation fold and which folds of the training. Just to make this as concrete as possible. Here's an explicit example that will happen in your homework. So resting for ten data set, which we're going to talk about today, is a dataset that contains 60,000 images. There are ten classes, so depth ten different categories of images and there are 6,000 images for each category. What we're going to do is with these 60,000 images at the onset, we're going to set aside 10,000 images, and that comprises the test set. And we will never touch this test set until one time at the end, when we overall score how good our model is. Right? After setting aside these 10,000 test set examples, we still have 50,000 images. What we use for training and validation. So let's say that instead of four fold cross validation study into 4.4 holes without you fight for cross-validation. Five-fold cross-validation is put the 50,000 images into fibrin. So each fold contains 10,000 images in five-fold cross-validation, one called this validation. So one fold is my validation dataset with 10,000 images. The remaining four folds are what I used to learn the parameters of my model containing the 40,000 images. Then I can repeat this training process five times, each time swapping out, which is the validation. And then we can average those five validation accuracy to determine the best hyperparameters. After you do that, then you can train one more time across all of your images for training and S-Corp once on your test set model question. The question is, is it just a coincidence that the test set is the same size of the polls that we used for training and validation. In this case, we chose them to be the same. Oftentimes, the test set should be relatively larger. So it makes sense to make it similar to an old size, but it doesn't ask the question. The question is, when we do five-fold cross-validation for each of the hyperparameters are going to have five validation accuracies. So then how do we choose the best hyperparameters? It's not a question, right? So if you are doing, actually I had this example on the next slide. So this is code that will optimize the polynomial order for our linear regression example. And you can see us a polynomial order gets higher and higher. The training set error will decrease like we talked about last time. But when you average those five validation accuracies for each polynomial order from order one to order five, you can see the accuracy, sort of validation or the validation set error increases, which is bad. So in this case, you would choose that you would use a first-order polynomial. It's the best performance of the other questions here. Wonderful. So time-wise, making the point that once you split the images into folds, those folds, you shouldn't shuffle them in between repeated validation. Otherwise you're going to be, you're going to have data-set week where across different validations you're gonna be using the same data for training or for validation. Alright, so that's cross-validation. You're free to take a look at this code, which is what you'll also incremental hallmark number one, that we'll do a cross-validation for our polynomial example. Like we were just saying. If you do to compute the best hyperparameters using the validation set with health data. You'll find that for the data that we were showing you earlier for the linear regression example, is best to use a linear form. Or one thing you need to talk about. Each of the validation sets just not profitable. Yeah, you're welcome to shuffle. What does he would say things, but then you can shuffle your examples. But you shouldn't change the examples in these folds across each cross-validation. Question is, can I explain what the difference is between the left and the right click or yes, the left finger. Sorry, I went through this quickly, is training set error. So this we should expect to get better and better because you'd expect lower error as you increase the polynomial order. Because you can use the expressive, the higher capacities at the data points better. But then this one here is validation set error. So this is how it generalizes to new data that was not used to learn the parameters. Just to keep the colors consistent. I think the training set we were doing in green and validation set we were doing in yellow or orange. Right? Any last questions on this simple example? Yes, this student is saying, just to clarify this secret clarification, we tested 25 models here, and that is correct. So every single validation fold, and we didn't five-fold cross-validation here. So there are five folds. Folds. So we're going to have five validation accuracies. Per hyperparameter, setting of hyperparameter. And then here we have five study for the hyperparameter, which is the polynomial order that goes 1-5. And so this would be five times five, and that will be a total of 25 mortals. Alright? So hopefully again, these past few lectures, these, the last lecture was a review for most of you. If you haven't had a machine learning background, please be sure to become very familiar with these. Will still talk about some things that you probably learned in your prior machine learning classes. But now there'll be relevant for neural networks. So in this example, we talked about minimizing the mean square error. It turns out for neural networks, we're going to need to use different loss function. One that is based on something called maximum likelihood. We're going to talk about this later today when we talk about and derive the softmax classifier. The softmax classifier we're going to learn is the most commonly used output of a neural network. Alright, so we're gonna be going over the softmax classifier today. So this is the topic of supervised classification. The deep learning book, it will correspond to these chapters are reading. So today we're gonna talk about supervised classification. We're talking about two classifiers. They're gonna be k nearest neighbors, which will just use as a motivational example really. Then after that, we're going to talk about the softmax classifier. And again, like I was just saying, this is the output of most modern neural networks today that do classification. And the softmax classifier we're going to train using a loss function derived from maximum likelihood. And then the way that will optimize this loss function is with gradient descent. Alright? And so these are the next steps that again may be review for some of those who had machine money back down. But this will be basically the first component of our neural network, which is the output. Right? So the motivation for this is, we're going to talk about image classification. That's because computer vision is really the field that gave rise to this past decade revolution in neural networks. And like I've mentioned before in our homeworks, are going to be looking at the image classification, which is T4 tag. And so I want to give some background on why this is a challenging problem. So we look at this image and we can clearly see it's a cat. But what does the computer C. So if you were to lose this captains your computer, you all probably know that every single pixel of an image contains three numbers. One that tells you the amount of red, green, and blue in it. And these values for red, green, and blue are all numbers I can go 0-255. If you have sympathy for red, green, and blue, that means you have a lot of red, green and blue together. And together they mixed to become white. If you have zero for red, green, and blue, then you have no read it didn't prove the absence of color is black. So if we were to zoom in on this part of the cat right here and look at how it's stored on the computer. Because it's what you would see numbers that are really close to 255 across the swath of the cat. So the computer is essentially just storing numbers there at 02:55 to represent this cat. Zero to 255. Again, just for the RGB of which there are three for every pixel. And then there's of course, a width and a height of the image. So the images are stored ultimately as a 3D array or a 3D tensor. And each of those values in that 3D texture range 0-2. Alright. So this is gonna be the input data or your computer vision algorithm for your neural network. And there's clearly a semantic gap here because we looked at this and we think a computer looks at this and sees all of these numbers. And we have to go from these numbers to determine what makes a cat a cat. All right, Any questions there? Alright, so the image is sorted array of numbers. And so you think about this a bit. So notice that there are a lot of challenges. One of the challenges is viewpoint variation. So these are different at the same object, which is a statue of David. And even though these are all the same object, if you look at what their numbers aren't any period, they're very different. So we just take the bottom left corner of this viewpoint. All of these values are close to zero because the image is comprised of black. Whereas if we were to take a look at the bottom left for these images, and the numbers will be closer to 100, 1,600% because it's not black. Alright? So just even a different angle can dramatically change the values stored for that condition. Even though they're all the same object, you point variation become awesome effect in the form of CO2. So just like the last thing WE showed, same image just zoomed in. It's still a cat. But again, the values are very different. If you're trying to decode. And cats in general, they can come in different illuminations. And here you can see again, the pixel values will be as different as they can get. Because here we're going to have a bunch of 200s or the captain bright illumination. But for the cat and dark elimination is going to be cluster two zeros. Any questions there? Alright, so then at this point you might potentially, okay. Obviously we can't use just the absolute value of the numbers making classification. We can start to maybe in trying to think of features that define what makes it. So you might say, a cat has ears, too pointy ears, two eyes, and four legs. But then there are going to be images where there's deformation. And so you see an image like here. We see three of the lens of attack, but then they're all inter, twined, entangled. Even if you were to say, Okay, it can have two eyes and image can have occlusion. And so you might have a cat hiding behind. You're here and you might only see one I get for all of these, we understand that these are the care. They can be cluttered by background. I chose this image because this cat bears or as almost as if camouflaged and the trees, and the trees have very similar values to let the capitalists like. Then lastly, we want to know how does attack. But then in fact Clement all different shapes and sizes. And we would still want to be able to classify that all of these are cats, right? So these are some of the challenges that come with having a classifier that takes an image and is able to identify it as a cat or a dog or something else. Any questions here? All right, So when trying to classify the image, there are several approaches that one could be located. One of them is this one that I just mentioned, where perhaps we can define the features of what makes the cardiac cath. And I look for those features in the image. But then this is going to require a lot of human expertise, right? Or a human has to first determine what Mason Katic cat, and then design an algorithm to look for those features with an attack. And that overall is time-consuming and not the most efficient use of resources. So the other way is to take the data-driven approach where you kinda throw up our hands and we say, Hey, I'm not exactly sure algorithmically, what makes a cat a cat. But I wanted to train a neural network, a machine learning algorithm to do it for me. And basically in the process of training based off of what the data looks like, the neural network, the machine-learning algorithm is going to learn what are the features that allow you to classify data. So in this setting, just like we've talked before, we're going to be to train an algorithm. We train an algorithm in this approach, we're given data. That data will comprise images of cats, dogs, airplanes, automobiles. And what we will do is we will take this data and train a model that takes the image and reports the class. And so this will give us in the training phase model parameters that we learn from data. Alright? So for deep neural networks, we're going to find out that these parameters result in learning. The features. Let me remove, digest learning features that are optimized to classify an image. In other words, to say that this image of a cat is in fact the cat. We might not know what the speech or talk to the algorithm will learn. After that, we'll have then our test phase. So in our test phase, we're going to deploy this model. So we start off with our model that was learned from training. So this model comes here. And now from this model, what we can do is we can put in a new image into this model. And it's going to tell me what class of students that's going to tell me is this image of a cat, a dog? It's alright. Any questions here? Alright, this is all set up. So like I mentioned earlier, we are going to be using this department dataset. The dataset is going to comprise 60,000 images, each of them or 32 by 32 pixels. And therefore, each image is going to be. I'm a 3D tensor that is 32, 32 by three. And at least to start for this lecture, we're going to go ahead and we're going to just restrict this into a vector. So our image, which was 32 by 32 by three, we're going to shape into a single vector, which is 3,070, two-dimensional. Thousand 72 is 32 times 32 times three. There are ten classes. These are the tank classes with 6,000 images per class. And then we're going to withhold 10.10 thousand of them for testing. 50,000 of them will be used for training. And I think I was looking at the homeworks. Sometimes we change these numbers and this sometimes it's gonna be like a fauvism for testing and if thousand for validation, but in the home or school. And so again, the goal of our machine learning or deep learning, will be to take in the input image and then predicts the class. I'll call this y. And remember what that we've been using the superscript I to denote an example. So why I think sample is going to correspond to the label of that image and it's going to be one of these ten classes from airplanes. Got Dr. Chuck. This is a code that will provide for you on our comforts two to five. And this will go to S4. And like I mentioned before, we're going to be taking these 32 by 32 by three images. So with this mode, seek part-time scripts. It'll give you x train, which are the images. X train is going to be 50,000 images. You took them 32 by 32 by three. Y train will then be the labels of those 50,000 images. So y train will be a number 1-10 for every single example. And class one would correspond to their plane. The y was equal to two, of course, automobile, bird, et cetera. And then we'll have our test data and our test labels, and they will be in the same dimension as the suspect there attend. These will be input data to our training. The question is, what we refit that 32 by 32 by three tensor into a 3,072 dimensional vector, we lose the positional information. Are you mean like, like e.g. like facial correlation, immunity. But when we get to later neural networks, just to motivate the first algorithm, the softmax classifier load mean of vector input. Other questions. That's a great question because there's a lot of information that spatial, spatial relationship. Alright, so we are going to talk first about a simple algorithm that doesn't require machine learning called K-nearest neighbors. Who here has seen k-nearest neighbors before? Most of you. So this should be hopefully straightforward. And dental. A different approach, which will be our softmax classifier. Alright, so consider a setup wherever using the maximum, oh, sorry. We're going to discuss maximum likelihood classifier in this lecture. We're given input vectors. These are my seat bar ten images, each of them in our 30, 72. And then for spawning classes, so each of these y's correspond to a label of that. And that'll be some number 1-10 or maybe it's Sarah tonight. Let me do one to ten just for the sake of simplicity in the center. Now, with this, we can train a classifier. And our goal is to now be given a new data point x you. And again, sorry, ignore this part. The probabilistic model, we decide to remove this from the structure based on past chairs. Feedback. We want to do it in a way that doesn't, that is kind of intuitive. And so that'll be something like k nearest neighbors. And intuitively what K-Nearest Neighbor says is, I can look at my new data point and look at the labels of the images that are closest to me. And then I'll think of both. And that'll be what I guess my images. Let me draw this out. Let's say that we had 2D data. So our examples at xy are all two-dimensional. And then this will be the first dimension of x. This will be the second dimension of x. And then let's say that I just had two classes, so I had cats and dogs. So let's say that orange circles are caps. Then we'll say that green X's are dogs. Right? So maybe my examples of each of these is a different example in my dataset. Looks like this, and then my dogs look like. So now if I were to give you a new data point that we have never seen before. And I put it, are the x values of this data point right here, this red square. And they asked you, what is the class of this new data point? What boat I could take it to be, you would say is probably going to be a dog because the things that are closest to it are all dogs. So K nearest neighbors formalizes this intuition. What we do in k-nearest neighbors is we set the hyper-parameter k. K is the number of nearest neighbors we're going to just consider. So let's say that k equals five. What this algorithm says is take my new data point and look at what my five nearest neighbors are based off of distance. And then whatever the majority or the plurality of those neighbors are, is what my class is going to be. So e.g. if my x nu was over here, alright, I will look at my five nearest neighbors. And it would be three cats and two dogs. And because of cats have three and salts have to then either classified that this point here is a any questions there or even any tenures neighbors. If you have a tie, then you can just choose randomly one of the classes that was time for the most votes. So more formally, and K-nearest neighbors, what we do is we choose a distance metric. Because ultimately we need to find our k-nearest neighbors. So we need a distance metric to assess what the distance is. All of my other data points, most commonly, we're going to use the 2-norm between two vectors, XI and XJ. Although in homework number two, we'll ask you to try also the one norm and infinity norm. Yeah, the question is on the slide, are the values X1 and X2. X1 and X2 will correspond to my example x psi. What are the two values there? So e.g. we're going to have some number of data points where each data point, data point is going to be X-i, Y-i. But then for this constructed data and I've just defined excited be some 2D vector. And maybe the values are 1.0, 0.5, which means that for this example, I would recite it one on x coordinate 0.5 and x2. Yes. We choose the values of K so that we never have to write good time, e.g. but the thing is if we have more than two classes, it can be like, let's say, added cats, dogs, and horses to this my nearest neighbor. But if you'd like to cats, to dogs and then one course and then in that case you just choose cat or dog. The question, yeah. The question is, if you make K larger, will that lead to a better classifier? I'm weren't accurate classifier. There isn't. Making cave bigger in general will help to ameliorate overfitting. But the optimal value of k will not be the biggest value of k. And that's something that you will find k-fold cross-validation. I will ask you to do that on homework number two. The question is, would you say is common to treat the distance metric as a hyperparameter as well? Yes. So in homework two, you will get back. Actually, I have a slide later on that was asking that question. Oh, yeah. Soon as asking. It could be that you have an example where let me draw this out. Maybe there's an x over here. And maybe we're doing like, let me make this. I'll do some different color. We'll call this threefold and purple. And maybe you're new example is, is over here. So in this example is much closer to the dog, but then the CAT score when by boat. And so in three, not threefold, sorry, I'm three nearest neighbors. K equals three. If you're doing a three nearest neighbors, that purple square will be classified as a cat. You can make modifications on this algorithm to, instead of doing the neighborhood kids like compute the distances, e.g. we're going to talk a bit industries size. Why not do it? Right? Perfect, Yes, a tomboy is raising the point that in this case, you actually probably want to guess that this is a cat, because in this green x here is an outlier because the screen x is very far away from the other dogs. Alright, last question. Yeah. Yeah. Wonderful. Yes, I start to write that L2 distance. In this case, this would equal the square root. And so x is a vector in RN. This distance would be the sum from k equals one to n. And then we're comparing example XI and XJ. We're comparing all n dimensions between them. So this would be x k minus x j k. This whole quantity squared. Sorry, I, right, if it's small because I'm at the edge. Standard Euclidean distance. But again, in homework number two, you'll try also the L1 norm and the infinity norms. We will also choose our nearest neighbors k. Again, notice that for the distance metric and the nearest neighbors, I said cheese, right? And so these are hyperparameters. You get to optimize over. We take our new data point, which was that red square. And we calculate the distance between that red square and every single data point that I have. And then we choose the k nearest neighbors, which have the case smallest distances. And we choose the class to be the thing that went for plurality vote. So the class that occurs most frequently amongst its nearest neighbors will be the class that we got to guess. And if there's a tie than any of the classes can be selected. That's the formal description of what we just described an image, the last slide. Any questions? Yeah. Right. Yeah. The question is, in the case of a tie, is fair randomization to choose. The time. That tells me what I just said. There were four pockets and I withdraw uniform random variable amongst four. Okay. That's great. Yes, it's one-way says that it's sometimes the case of ties. Instead of doing randomization, you can also use the actual distances to break the time escalate idea. Alright, so here's example dated at this time we'll have three classes, the green triangles, the red Xs, and the purple circles. And we're going to build this classifier. Alright, so the first question I have for you all now is how do we train the classifier? I'll give you all 15 s to think about it and then I'll ask someone for an answer of how to find the classifier. Hoping to talk to neighbors to the lungs. All right, It's disgusting. Someone told me how to train this classifier. Wonderful. That's all come back. So I just asked, how do we train a classifier? And Jacob answered, all you gotta do is save the data, and that's correct. So you were to make a k-nearest neighbor class? I need to classify by just comparing. You did appoint all of my existing data points. And then I also have to know the labels wise so that I can do the book for K Harris papers. Alright, so this is all the training purposes which is to memorize your data. Alright. What are the pros of this was 200. So it's the book. And it's also fast, right? Presumably already in memory. So you just have to make a pointer to that memory. Someone told me what a con of this training approaches as slow as well. In what sense? The testing us though, yes, and we'll come back to that. Wonderful. Yeah, so it's memory, and especially because I'm gonna write memory intensive, as your dataset becomes bigger, you have to store more and more examples. And in general, if we have large datasets like ImageNet, right, that pelvic gigabytes and that will not be a wise store. So it's memory intensive because we need to store all the input data. This will be in contrast to machine learning algorithms, where we have a predefined set of parameters. And all we have to do is store those values of Theta. So keep that in mind when we talk about the softmax classifier. How do we test the new data point? So let's say that we are working in R2 just to have a concrete example. So our data points that are two-dimensional, well we would do, is we would get a new data point by x tests. And then when I calculate this distance here, what I'm doing is I'm Kathy calculating the Euclidean distance between X test the new data points, and self-talk x train. Remember x train was my, was just my entire dataset. So if you haven't used Python before, you've likely not heard of broadcasting. Even if you use Python, maybe you haven't heard of broadcasting. This is going to be really important to make your code more efficient. So let's say that we had an examples in my training set. Then train might be, wouldn't be then a two by n array. Then my new data point is just one-sample. Z-test would be a 2D array. When I calculate the two norm, I have to subtract from every single example in the training set. Alright? One way that you could do is do this is to write a for loop. Alright? But writing a for loop is going to make your code into walk longer. And the procedure of subtracting this 2D array from every single 2D array of which there and at them in X train should be something that you can do just like a carrier. If you come from that background and you know that the function you would use DSX fun. In Python, this will be done automatically through something called broadcasting. In broadcasting, what'll happen is if I kick self dot x train dot transpose, that makes this term here. And n by two array because it transpose is the array, so it's n by two. Then X test here is a 2D array. And if you do an operation like minus, and the trailing dimension of the first variable matches the leading dimension of the second variable. Then Python will know to perform this operation minus subtracting this 2D variable from each of the N examples in which each one is two-dimensional and self-talk extreme dot transpose. Alright, so this is a quick way where as long as the inner dimensions match for your operation, Python, we'll apply this operation minus for every single example in extra. And that saves you from having to write a for loop. And it's also a lot faster time. You'll use this in the homework. Any questions there? Okay, the question is, why is x trained to buy in to begin with as explicit and bite to? It might be more common. This was just a preference for me. When I wrote this code, I defined x train to have the Fincher is being the rows and the number of examples you can call it, but it could accompanied by two, in which case you wouldn't have had to have this dark transpose here. The question is, why is X test two comma empty as opposed to two comma one or one comma two. So in Python, it's better when things aren't bacteria to just define them as two comma empty. Because then Python will know to treat it as a column vector or a row vector depending on the context in which he sees. So it's more flexible to define it as two comma as opposed to two comma 12 comma one, then a has to be a column vector. The question is, when do you define it? You will define it to be at, to come after me. You could write it that way or you could just write an array. And bettering will be like an array with two numbers, then you don't need to cite to comment. That's right. Right. Tas? Far less than I used to when I was in graduate school. So sometimes, sometimes I had to check with the TAs for syntax. All right. Other questions. Yes. The question is, isn't this square root unnecessary? Yes. That's correct. Yeah. You could have used the two norm squared because you just care about the relative distances. That would give you the exact same answer. Alright, so this is testing the testing function for our k-nearest neighbors class. The pros, again is a simple. Let's go through these lines, but these lines will just sort the distances and then choose the k nearest ones and then do the plurality. What's the chronosystem? Our students already said it. It's really slow because whenever you want access to new data point, you have to compare its distance to every single point in the dataset. And that is going to be slow. And not only is it going to be supposed, going to scale with the amount of training data and not of training data. This is really bad if e.g. we're looking at real-time applications. If you're building a neural network that isn't a self-driving car, right? It h"
"All right, everyone. For today, we have a few announcements. First, a reminder that homework number four is due this Friday, uploaded to Gradescope. And on Monday after class, we sent out an announcement on CCLE with details about the midterm exam, which will be on Monday during class time. Alright, so please be sure to read over those midterm details, which talk about the exam timing. So the exam will happen between 2 and 3.50 PM during class time. We're going to upload the exam to CCLE at 1.55 PM so that you can download it and print it out if you desire."
"All right, everyone. For today, we have just one announcement, which is that homework number five, which is pen and paper homework on graphical models with upload to grade scope last week and it's going to be, sorry, uploaded to CCLE last week. And there will be due this Friday, uploaded to grade scope by 1159 PM. Any questions on any course logistics? Oh, and a reminder that today is the last day to get in your midterm read grade requests. And so if you haven't taken a look at your midterm yet, please be sure to do that today. All right. All right. So last lecture, we started to get into BMI continuous decoding where the goal is to decode the signals from the motor cortex, the spike signals. And instead of using that to classify, which you all did on homework number four, to classify one of eight targets, now we want to decode algorithm that translates the neural spikes into continuous movements like a robotic arm or in our case, a computer cursor on a screen."
"All right, everyone. For today, we have two announcements. The first is a reminder that homework number five is due this Friday, May 21st. I'll upload to Gradescope by 1159 PM. The second announcement is that last lecture, someone asked a question about homework number seven. And I would look at the syllabus for this year, and on the syllabus for this year, we only listed up to homework number six. And given the pace of the class this year, this will be the last homework for this class. So usually we do have a, when this is taught in person and the pace is a bit quicker. We do have a homework number seven, which is on the mentionality reduction. And this year we are not going to have that homework, but we will likely release, release solutions for our difficult carriers. All right. Any questions on any course logistics?"
"All right, everyone. For today, we have two announcements. The first is that homework number one is due today uploaded to Grayscope by 11.59 pm. And this morning, Tonwoye already uploaded homework number two onto CCLE. And homework number two will include a Jupyter notebook, and so it includes Python coding. Again, this is your first time doing Python coding. And you're, for example, transitioning from that laptop to Python, please just budget in some time for that. We have just a few things for the homework. Question one C was about patch clamp, and I had removed some slides on that in the interest of time. So patch clamp is a technique to measure the current through a single antenna. So this is the answer for that question one C. And then in question two G, you'll need to use a technique called least squares, which hopefully you have from a prerequisite class."
"All right, everyone. I had mentioned, I think, I don't see style here right now. He had a question at the end of 102. When comes an off against question. Yeah, so, I hope everyone had a good Thanksgiving break. And hope most of you were able to take a look at, at least read over the project. So, I know we went over a lot last lecture in terms of just going through the MATLAB data. And so, I want to start off this lecture by just taking any questions you may have on the project setup, the project code, etc. All right, I'm, I, I plan for most of the lecture to the project questions. If there are no questions right now, what I could do is I could at least finish off."
"All right, everyone. I'm happy to commend soft as ours and take questions in the order that hands are raised. Eric, I wasn't sure about 3C. It's the one where you can involve a signal and the HFT that they drew and you want to find a period where it's a constant. I did the question by like, I considered it conceptually if I have some periodic signal and then if I like to look and drag it, then intuitively if I had the same period, then no matter how you drag it, you'll get the same convolution. And then if you decrease it, decrease the period by like, integer values, it'll also work. But I'm not sure how to actually mathematically work that out. Great. That's the correct intuition, which is you'll notice that in the sink there are zeros at particular locations. And we know that a periodic signal corresponds to a sample signal, the periodic signal and time corresponds to a sample signal in the frequency domain. And so if that sampled signal has the samples at the zero points of the sink function, then they'll all be zeroed out and then any integer multiples of that period will also be that way. So we can do that mathematically. I want to ask if anyone had any questions on 3ARB before we did that. So as to be helpful to anyone, if not, we'll just start with 3C."
"All right, everyone. So it's been two weeks since our last 189 class. I was going to pick off where we left off, but before that, I want to stop to ask if there are any questions about overall class logistics, anything about this class. All right. So what the rest of this class will look like is today, we're going to finish deriving the tools and equations that you need to build a brain machine interface for cursor control. And then next week, we're going to release the project. So next week, I'm going to release the project assignment on CCLE. And what we're going to do is in class, we're going to go through the data together, be sharing my screen. So the project has three tasks. And what I found in the past is that it really helps the students a lot if we do want to pass together. So the project has three tasks and we'll do tasks one together for the project."
"All right, everyone. So today we're going to finish going through the sides of this discrete brain machine interface that we started talking about last Monday. And then after that, we'll start doing the tool derivation, which will be used for the project. All right. Before diving in, does anyone have any questions from last week? All right. And then I received a few emails asking about the participation grade for this class, especially some are some are not in the same time zone and therefore can't attend these lectures. And so what this means is for this quarter during the time of COVID, I'm just going to give full participation points with understanding honor system that you'll watch all the seminars, which is typically how I would give the participation points in the past. All right. And so really for this class, you'll be graded on the project, but you'll have received the 80% participation points automatically since this class is now being taught during an extraordinary time. All right. So I want to remind you where we were last Monday, which is we were talking about this delayed reach task where we have a monkey. The monkey touches and holds a center target. And then after that, a target appears somewhere on the screen. In this example, it appears below the center target. And the monkey during this time period develops a plan to reach towards this target. And so from the delay period to the go queue, the monkey is planning to reach to downwards target. And then after if he gets the go queue, then the monkey is free to actually reach to the downward target. And so during this phase, the monkey actually reaches to the downward target. And the reason that we're distinguishing these two is because we want to build a brain-mishing interface where the monkey just plans to reach somewhere and we can decode that immediately and then show that and say, okay, he's thinking about going down."
"All right, everyone. We'll get started for today. So we have several announcements. Didn't hear me, I think he goes. Did you have several announcements? The first is that homework number one is due tonight, upload it to Gradescope by 11 59. And for the code, this is a friendly reminder that you need to submit the code that you wrote. So if the grazers evaluated, and what that means is that we would want to print your notebooks to PDF with their solutions and clocks built in. Homework number two is going to be uploaded today by the TAs, and it's gonna be due in a week on Monday, January 30th. Upload it to grades together. A heads up for homework number two and also assignments moving forward is that the assignments are going to have a good amount of Python coding in them. And so we really encourage you to get started early in this homework number two, you are going to be commenting k-nearest neighbors as well as the softmax classifier, which we will finish discussing it and derive it in details up today. Any questions on these first three announcements? Yeah. The question is, can I clarify what it means to submit dot PY files as PDFs? So in homework number one, there are no dot PY files, but later on in homework number two, moving forward, there'll be classes that are in d2y path and just wanting to be. So ignore that for homework number one, but burrito numbers. Are there questions on the first three-and-a-half sense? Alright. So there's a detail on TA office hours notes, but I don't believe either pronounced so because we have so many CA office hours. The way we tried to organize the uploading of thumb is that the TAs are going to upload basically a consolidated office hours notes on Friday along with the discussion video for that week. So we will upload notes from office hours for the TAs, but they will all go off together on Friday. Any questions that alright. And then lastly, I'm just due to some administrative reasons, we haven't yet hired the graders for this class. And so, apologies on my end. Homework number one is going to, there's going to be the array and returning your grades to you since we haven't get fired, our graders. However, we will, of course, I put the homework number one solutions on Wednesday so that you can check your work and understand it. On Thursday. Great. Yeah, we will upload the homework number one, which is on Thursday because they'd go off after the late deadline, which is Wednesday, right? Any questions about any logistics for the course? Alright, we're gonna get back into material back. So last lecture we discussed this simple classifier, k nearest neighbors. And then we talked about some of its weaknesses. And then we mentioned that we would then look at a classifier based on linear classification. This classic buyer ends up being at the output of most classifier neural networks today. Also, it comprises something called a linear layer or linear building block, which is a key component of neural network architectures, which we should also get to likely by the end of lecture today. So we mentioned that a linear layer will implement this function if x is your input and y is your output, they'll do wx plus b. And then what we said is that this vector y, if we're doing classification, can be interpreted as a texture of spores. So y will be a C dimensional vector, C being the number of classes. So in C4, ten, that's ten classes, so it'll be attendee vector. And then the first element is a score opinion classic one. The second element is the score of being in class two, etc. We talked about how this will compute a score for every single class. And we also talked about how this wx plus b is implementing linear classifier and ends up finding solutions if you just have wx plus b. When w, because we're going computing like a w1 transpose x, right? The score will be highest when the weight wi looks most similar to x. So if you've looked at the weights, e.g. if the car cost, you can see that they kind of present all cars. Were there any questions from the last lecture? All right, We'll move on then. So after we get the scores, what we need to do is we need a way to translate them into a loss function L. Because we don't wait to say for some setting of the weights w and the bias vector b, how good is my Softmax classifier this money earlier at predicting the correct class. So I need a loss function L to evaluate how good it is. And then after that, I need to know how to make w and b minimize that loss, right? So we're going to fill in those gaps today. And so like we talked about how we're going to do this. There's something called maximum likelihood. What maximum likelihood does is it says for some observed data, what we're going to do is we're going to find a way to evaluate the probability of having observed this data given the parameters of my models. We've talked about this coin flipping example where the parameter is the probability that it comes out to parents. And then we're going to apply this today to our softmax classifier. So when we change the softmax scores, which in general can be positive negative numbers, into a setting where we're going to use maximum likelihood is going to be critical that we are able to interpret the scores as probabilities. Alright, so to use maximum likelihood, I need to be able to calculate a possibility to begin with. So that's going to be done by the so-called softmax. So we're going to do is we're going to define score for class II. I'm going to call that a as a function of x. X is my image. And then Ai of x is gonna be the score that the image belongs to classify. So this is a variable that we want or just use the I element of that vector y. And we know that the score is given by W transpose X plus some scalar bias beyond. So I want to know the score of the image X being classified. All I have to do is calculate wi transpose x plus b. Again, the score could be any number as negative if you want or as positive as you want depending on the values. Is that wi. When we define the softmax function, the goal will be to change the scores for each of these classes into probabilities. And for probabilities, we know that there are two constraints. Probability has to be 0-1. The sum of all the possible probabilities for each class at the sum up to one. The way that we do that is with the softmax function. But we're going to do is the softmax, the image X being in class. I, would later find out that this is gonna be interpreted as the probability of class I definitely image x is gonna be e to the score of class I divided by the sum of e to the scores of every single class. Alright, so in a very simple example where we just have two classes, the score for class one is going to be a one, x is going to equal w1 transpose x plus b one. I'm going to talk through these just for the sake of writing. A score for class two will be A2 of x equals W2 transpose x plus, I'm going to drop these. Alright? So those are my scores for class one, class two. The softmax function says softmax for class one of x is going to be e to the w transpose x divided by the sum of the exponentiated sports broadcaster. So it's gonna be e to the w transpose x plus e to the W2 transpose x. And then softmax for class E of X is going to be key to the W2 transpose x divided by e to the w transpose x plus e to the w transpose x, right? So you can see for every single softmax, the denominator is always the same. It's going to be the sum of all the scores. The numerator changes is going to be the exponentiated score for that class. You can see that if I sum up softmax one and Softmax two, they add up to one. All right? You can also see that these are all going to be values 0-1, right? Because I'm dividing a score by, by some larger non-negative score. And so therefore, because we know that each softmax for class II is going to be 0-1. And we know that across all classes, they sum up to one. And what the softmax function does is it turns my vector of spores into a valid probability distribution over each class. Said a lot there, I want to pause and ask if there are any questions. I said Jake's question is, what is the advantage of using the exponential instead of doing one score divided by the sum of all scores. That's a really great question. So the first thing to note is that Any function that makes the scores positive will work as a normalizing that's already normalized probabilities. If they were negative, then we might not satisfied all of the constraints. The probability distribution you could do like absolute value of Ai divided by absolute value. But then if you asked me that it has a negative score, large positive score, right? But you can choose other functions that are monotonic. So some people say like, Oh, you can do to, to the AIX instead of e to the x. That's totally valid. Also, you could do E to the two to the AIX that's also valid. All of these will create a valid probability distribution. When you use ie. This is very common, but also has some relationship to information theoretic quantities. And so that's one reason why it's preferred. Excuse for perfect gas. It's always just another thing, which is when we derived by people, like we talked about last lecture, you see we take the log likelihoods to turn all those products into sums. If you have logs of ease, we know that they canceled after just give you an exponent and it helps to make some of the math simpler. Right? Yeah, so remain as mentioning that also has a strong nonlinearity. We're gonna see how that leads to his name softmax, that this function is almost by taking the maximum of the neck, is almost something that returns the maximum score. Although it's gonna be differentiable. The question is, would I repeat the first advantage of that information theory? I'm going to leave that for beyond the scope of this class, but nobody talked about it with me in office hours. Basically, if you choose the softmax function, you'll see that several information theoretic terms fallout. Another name for the loss function, both the ride today, the maximum I can put boss watching is called entropy. Entropy is a concept from information theory. All right, we'll continue on that. So that's the softmax function. And you're going to do is we're going to define the probability of an example. So this is going to be e.g. J. We're going to say the probability that example j has a label that is I. So I will be from one of the tank classes. Given that I know the image, which is x, j, and I know what my data is. Data are my weights and biases like gave me the scores. We're going to define this probability to be the softmax applied to the image for that class. Alright, so let me just write this out in words because whenever we see probabilities, we have a good intuition over them and what they universe. So this is going to be the probability that x j, j, this is just with Jacob, my training set. This is probability that image x j belongs to class. All right? Any questions there? Yeah. Great. The question is in the data matrix, x is each column. So if we're just talking about C4, C4 can we may have any images. And then remember for these first examples could restate them to the 30 72 vectors. So each X is gonna be a 30 72 vectors. So you can imagine we might have some big matrix X of all of our data. And it's going to be 50%, sorry, big N, big N images by, by 30, 72. So then every single row here would then just be one example. This would be x superscript one. The second row would be x superscript two. So x superscript j is just the state of new tricks. Great question. Other questions here. Alright. So let me just say it one more time. This is, we're going to assign, we're going to define that the probability of your image belonging to class I is going to be equal to the softmax function applied to your image for that class already. Remember this is going to, this is going to be based on the scores for Phi-Psi AIX divided by the normalized, divided by the exponentiated sum scores for the classes. Alright? Any questions there? Alright, then we're going to go ahead and derive what the maximum by, because it was. So what we wanna do is being able to take our data. In this case, we have datasets where this is our first example, right? Again, remember X1 will be some image. Then y one will be a stable. So let's say that the first image has got a dog. So X1 is the image of the dog. And then Y one is the label that tells me it's a dog. And then epsilon would be my image and maybe white and as a cat. So what I wanna do to do maximum likelihood ready? As I wanna do, I want to compute the probability of having observed all of my data. Which means I want the probability of having observed image one. And image one was a dog having observed image and that image and was account. Alright, and this is going to be conditioned on my model theta. So remember that Theta then is going to comprise my matrix W and my biases. And those are the weights and biases that control the scores for each class. And those scores in turn affects the overall probability of my bottles. Want to pause here and just ask if there any questions on this. Just writing out the most naive. Just running up the phone. I'm sorry. Great. The question is, what does it mean by the likelihood of having seen the data? So what we're going to do is we're going to have a model, w and b, right? And that controls the score for each class, right? So let's say that image one looks like a dog and it's payables and dog. So for image one, our model would have high probability if the w's and b's assign it a highest score for dog. If the w's and b's told me that image one is a car, then I'm going to have a lower probability. Because the probability of my model saying that image one is the dog is going to be very low. So this is a way for me to write out what my model to say. Image one is a dog and a cat, et cetera. Analogously, I always like to go back to the simple example when I'm thinking about this, the data is our observations. But we're giving like the sequence heads, tails, heads, heads, tails, tails for that point where we want to make sure that our model does a good job at predicting that we see a sequence of four heads or tails. Similarly, in this case, our model parameters, but tell us that each one has a high probability of being a dog. That's a high-quality bean, et cetera. Good question. Any other questions? Great. Yeah, so Rockford is going to hit that magic, something which will bring us for our first step. So in general, when you see a probability with many terms, that is going to be something that's very, it could be something that's difficult to compute. We have to start making some simplifying assumptions to be able to actually write an expression for this profitability. Assumptions might not be true and the board, they are untrue, likely the term that we compute will be more off, but many of the assumptions that we make are not totally unreasonable. So the first assumption that we're going to make is that image is conditionally independent of dimension j given my parameters. What that is saying is that what that is saying is that I treat each data point as my prediction for data points to is going to be conditionally independent of my prediction for Data Platform image to image one. Okay. Any questions there? Yeah, from each one? Based on that. The question is, isn't that basically an approximation? Because images can have some correlated features of sorts that my model might pick up on. And being able to do an associate that might make this assumption not totally valid and not That's correct. Yeah. Given data, they are independent. Given my model. Alright, so at this point, we're not going to make our next simplifying step, which is ever going to apply the chain rule for probability. So I'm going to break this in to the probability times the probability of y given x psi. Alright? Remember that in words, is probability of y given x psi is saying, this is the probability that my image x belongs to the class. Why? Alright, and this thing is going to be our softmax probability. I didn't even pass rate e.g. the thousand samples. Samples or samples from the bulk glass. And Muslims. Tomboy asking you, what is the dependence? Hear me. So the Independents here is the identical fire sorry. The identical distribution is referring to the samples. X i, y are. When you're seeing the samples from the same class type of distribution, are you talking about p of x given y 0? I'm talking about, let's say. But ultimately, if you wanted to raining or snowing identically distributed according to this training samples. Even less, all the samples we go get the same distribution that's provided. So when we see identically distributed here, what we're saying is that p of x i comma y given Theta will all have the same distribution. P of y given x, I will always be the softmax. And I think if I understand what you're saying, tomboy, you're saying that if you have the probability of an image given its class, why I equals k or something, that this will also be a different distribution in general for each class. And that is also true. So general the distribution of exercise, which are the images given the classes will also be different, but the identical distribution refers to this term here. Actually this is a, this is a great question to make sure that we're all following. So when we apply the chain rule for this term, we do the probability of the image and then the probability of YI, the label given the image. We know that in layman's terms, this is the probability that X belongs to the class. Why? I could have also rewritten this in the following way. I could have written product from I equals one to n of p of y i given beta times this term. Next time I mentioned p of x given y i. And fair, right? I could have used the chain rule in the other direction. Why don't I do what's right? Great. So the student thinks like from a practical point of view, the goal of a classifier should go from x to y. And because we're going from x, y to y, I didn't say this, but this is kinda where the actors going. We're going to have to calculate a term that takes us from x-i to y-i is gonna be our classifier for which we have a term is soft match probability already computed. So in essence, we have this term to find because that's what the problem that we're doing. And that is correct. That's one reason we want to go with this expression. How about another reason we prefer this one over this one in terms of complexity. Wonderful. Yeah, so this student is saying, for a given class, there are many different images that belong to that class, which is true. And that's kind of getting at this answer, which is, if I were to write out this probability P of x given y, right? What does that mean in words? It's saying, let's say that the class is dogs. So let's say that the Y is the label for dogs, is saying, what is the probability of how images look like, right, of seeing this image of a dog, which is a 3,072 dimensional vector. When, what is the distribution over images that come from the class dog, right? And in general, this will be a high dimensional complex distribution. There'll be a 3,017 and I shall distribution. Whereas the softmax vector saying over my ten classes, right, which is a simple distribution with ten possible outcomes. This is much more easier to estimate or approximating this thing, which can in general be intractable. Any questions that you recently had a good following. Okay, great. Majority of class question. Intractable. When I use that phrase, music, We can't write it there. So it would be saying, well, what kind of distribution can we write down? That's 3,072 dimensional, which would model the distribution of what the dog images, stuff like. Tom Waits question was just pointing out which distributions are identical. And Tom, I was making the point that this distribution, which is if k is equal to the dogs, the distribution of but dogs look like there's going to be different than the distribution of what Casper quite et cetera. We have nothing that those aren't ethical. We've assumed that these distributions are identical. So we haven't assumed that p of x given y is identical, right? Yes. Thank you. So the student says it's a big idea, but the probability of all the pictures being classified correctly is the product of each individual image being classified correctly? And then multiplying it all come together. The answer is yes. All right. Last question. I'm sorry. You said we didn't tractable, Why would it? Oh, yeah. The answer is, are we using Naive Bayes? So yeah, if you know that I Bayes classifier, that will work. But the more troubling thing is that you will need to assume a distribution over the natural images. And probably the approximation you make there would be very poor distribution. Yes, it's almost making one last point here. If you were to actually learn this distribution, that would be very cool, right? Because then you can say, I'm going to set y to be the dogs. And if I can draw from this probability distribution that I can generate images of dogs. We'll talk about some of these generating distributions at the end, the classroom effects of games. And okay, let's move on for now. So what we're gonna do is I'm going to take this expression over here. And we're going to actually write out what this means in terms of our models that we can derive our likelihood. So I'm going to take this expression just copy and pasted from the last slide. And when we do optimization, our goal is to make w and b, which on my parameters theta, to optimize them to make this likelihood as big as possible. The first thing that I'm going to do and going from this step to this step is I am going to just erase this term. Alright? So from going from here to here, I just erased the first term. And someone raised their hand and tell me why I'm allowed us to do. So. Great. Yeah, so let me write out what, let me write out what this probability P of XI given theta is. First off, let me start off actually with a simpler question which is, can a student told me in words what this probability distribution means? You will go with, you got that. Wonderful. Accept this. The probability that you see this image or this image occurs given are setting up my w and b matrix and vector. I'm going to say that this is equal to p of x, sorry, and discuss the student answer. Because the probability that you're going to see an image of a dog, right? Or the probability of an image of a dog in a hurry. It's going to be the same irrespective of my bias is one or two, right? The been totally unrelated things like the parameters of my model to whatever I want them to be. That's not going to affect anything about how the images, images are data that are given to us in this setting of W and B have no impact whatsoever on what the images look like. And therefore, P of XI given theta equals p of x sine. This is true. Then P of XI given theta has no dependence on theta. And because I am going to be maximizing this expression over theta and this term, does it impact data? You can imagine that it comes up, that I can just remove it because it won't change what my final setting up data is. Alright, any questions there? All right, and just in general, always helpful to make sure that when we write gotten distributions, they aren't mysterious to us. For the distributions that we write down, we should always be able to say intuitively what it means in words. And that'll help people with simplifications like this. All right. Alright, we're going to continue our work here. So this is going to be equal to ArcMap data. And recall that we define this probability distribution here to be softmax side and now voting, overloading variables. So this probability here is going to be assigned softmax. I'm just gonna be the softmax score for class. Why are they? So I'm going to give us superscripts right here. So I'm going to write this in terms of my bug, in terms of my softmax is one other thing I want to do is I'm going to take the log of this so that the product turns into summation. So I'm going to turn this product over examples n into summation. So it's gonna be argmax I equals one to n. And then have log of these probabilities. And these probabilities are gonna be those softmax course. There's going to be var log softmax. That example, I belongs to class label y i. All right, we're going to then go ahead and expand this. So I'm gonna write that this is equal to arg max over theta sum from I equals one to m. And then I'm just going to plug in the definition for softmax. So this is gonna be bogged. And then the numerator is gonna be the score of the correct class. So the score of the correct class will be e to the w transpose x plus BA II Plus BYUI. Then it's gonna be divided by this for the rest of the class, it's just gonna be divided by the sum. I'm going to use, I'm Jay to index the rest of the classes. So j equals one to see. And that's going to be E to the W j transpose x plus b j. Now sometimes people get confused by this notation, y being in the subscript. So remember y is one of the numbers 1-10. Let's say that y equals 4.4 is the class label for dogs. And so this is saying, compute the score for the dog exponentiated and then divide it by the score for the rest of the classes. So example I tells me what the correct score is, dog and cat, etc. This is saying compute the score for that correct class. Any questions there? All right, so we're gonna go ahead and simplify this even further. At this point, I'm just going to go back to using that notation that we had, where this w j transpose x plus b is just going to be the score. Ha, so I'm just gonna do that. So I have plus routers today. So this is gonna be argmax theta. We're going to have a sum from I equals one to m. Now I'm just going to simplify log experts. So we're going to have log E to the score of class YUI. That's just going to equal a YI x phi. Alright? And so this AY XI says this exponent over here. Then it's going to be minus log of the sum of scores across all. It's gonna be minus log. Sum from j equals one to c, e to the w j transpose X. Score notation e to the x. Then usually one other thing that we do is we'll make this an average across example. So I'm gonna put a one over m term here. This hasn't changed the actual answer because one over m is just a constant and that won't change the state that maximizes this term. Any questions here? The question is, why don't we take involved with Chicken balls to turn all of these products into Sunday? And so in homework number two, you're going to have to take the derivative of softmax with respect to w and v easy. And That'll be easier when these are songs and our products. Raised the question is, why did we use, if I understand correctly, y in the numerator and the denominator, this is so close, has this question. Okay. Usually, you see this is the most frequently asked question. Yes, sir. Um, remember that what we wanna do is we want to compute the probability of the correct class. Let's see, let me just write it out explicitly. Let's just say e.g. all right. The correct class is at y equals four. If we look at our students are ten, we can see that the fourth example is cat. So we're saying that example, I as a cat, y equals four. And this means I belongs to class four, which is a cat. And the probability image xy being in class for, right, we'll just write as softmax. Softmax for objects. Alright? So this would be the probability that xy belongs to class board. I wanted to write this more generally then for all of my examples from I equals one to m. So instead of putting this explicit subject for, I'm just going to say whatever YI is for that example. That will be the softmax publicly. The question is, what is this probability? What is this probability? This is the probability that given my model, you observe that the image xy. We observed the image XI and its label such, such as like y equals four mini that this is an image of a cat and this is cat, Right? Yeah, so y is going to be the label that I asked. And I want my softmax to set the parameters of Theta such that y1y was equal to four. Softmax for an xy would be a large number, something closer to what? Yeah, softmax for class three would ideally be zero or something close to zero. The question is, is the probability of the incorrect class? Can you go in this expression? It is not, however, it is implicitly in there because this is a distribution that sums up to one. So the probability of cat is high. The probability of dog and everything else has to be less. Okay? So we're gonna go from this expression. This is again just plugging in the softmax. And we're going to do one last thing, which is this is maximum likelihood, but we know that in general, the convention is that we have a loss function that we want them in MRI. So to make this an RNN problem, we're going to use that property where if I have some function of Theta, now want to match it over theta, this is equal to minimizing negative f of theta over theta. So this thing here equals argument over theta, one over n. Sum from I equals one to n. Remember this over my examples and then I'm just going to flip these two terms together, my negative sign. So this is going to be log sum over all my classes. E to the h minus y. Alright? This function here is going to be my likelihood that I will desire to minimize. Honor. Yes. The question is confusion over this notation here. Oh, yeah, it's confusing over the notation W subscript y. So let me write this also more explicitly. So if I were to write, this is important because you all need to implement this on homework number two, a phi of x phi, which is this term here. This is defined to be W for the correct class Y i transpose x i plus B, y, and z. For ten, we have ten weight vectors, W1, W2, all the way up to w ten. And then we also have ten biases, v1, v2, all the way up to be taxed, right? That's an art have been here model. When I say a YUI, what I'm saying is that when you get Example I is gonna be an image. You're going to get a data example with XI YI image. I'm just going to come with a label and that label y will be labeled image. And I will be some number 1-10. So if e.g. I, they face a cat, right? And he saw before a cat corresponds to y equals four. Then a y i x i will be a four. So this will be W4 transpose x plus b for e.g. are e.g. I. Plus one. It can be that y equals ten. Then this numerator would be e to the w transpose x plus b times et cetera, et cetera. Right? Right. The student is asking, so let's say that y was equal to four, so it's a cat. What is this expression actually equivalent to in terms of thinking of like five linear classifiers. So what it would correspond to is what you can do is you could take your road W4 transpose here, dotted with x and then add your fourth price here. Or you could just take the fourth element of this Y matrix, a wide vector. Both of those would give the equivalent thing, just the score of the image being in class for. Yeah. The question is, why did I add the one over n? You don't have to add the one over n. Remember the one over m is just a scaling term. We added so that this loss is basically the average loss per example. So it's just a normalizing term that says, okay, this is going to be like physically my loss, e.g. but it isn't necessary. Alright. Go ahead and take our five-minute break. Given the number of questions, I think I didn't do the best job of explaining this, so please take some time to look over it with the five-minute break and then when we come back, I will be happy to take other questions too, hopefully. Two questions. The first one is directly given theta equals P x, y term. You say that the px py doesn't matter because even though we get, we are dealing with different Thetas. Dxi is still the same. But I wanted to ask like, we, you have a whole data set like say 60 or I could call a hundreds of patients dataset. Split. This patient's ear training set and testing set. Doesn't matter. It doesn't, because remember, this is the true distribution over images or whatever data that you have. That distribution is given to you and you get examples. Distribution. You're not going to get, you're not going to just won't look different based off of your weights of one or two to the images aren't just something true before you do any. Assuming this is the true distribution of the population we are analyzing, we are splitting Reimer with, right? Yeah, so that's t"
"All right, everyone. We're going to get started for today. A few announcements before we begin. The first is at homework number three with uploaded to CCLE last Friday. It's going to be due this Friday, uploaded to grade scope by 11.59pm. On grade scope, you also have the option of submitting homework regrages. And typically, we're going to open them for one week after we release the homework grades. And so when you submit regrages, that question will go back to the greater, the reader who created that question and the reader will be able to assess for comments and determine whether to give you back points. And so that's how regrages should be handled for this class. All right, we also saw that on Piazza, there was a post about how long the homeworks are taking you. And there's a poll that showed over 30% of students are taking 15 to 20 hours or over 15 hours to do the homework. And so I want to start off by saying the homeworks that we are giving are of similar length and difficulty to the ones that we gave in prior years. However, we know that this year is not normal in particular."
"All right, everyone. We're going to get started for today. A few announcements. First, homework number four is, do this Friday uploaded to GreatScope. And you'll notice that on the homework, we've designated several questions to be optional. And so those are our questions that can be additional practice for you if you so desire. We are going to be setting up an announcement tonight with several midterm logistics, including the timing of the midterm and Zoom link for where the TA's and I will be in case you have questions about the midterm. And so all of that information is going to go out tonight and an announcement. There are a few things I want to state here in class, which is, I guess, people in different times zone will see this in the video. But if you're taking the midterm at a different time, I know that last week I wrote to send me an email. It turns out I should have provided more detail because it's become a bit haphazard with the emails."
"All right, everyone. We're going to get started for today. A few instances before we begin. The first is that homework number two was uploaded to CCLE on Friday and it's going to be due this Friday, October 23rd, uploaded to Gravescope by 1159 PM. In general, homework solutions are going to be posted after the late deadline has passed and so the TAs will, they have already uploaded homework number one solutions to CCLE but if not, they will certainly be by the end of today. And then lastly, if you're going to be using late days for an assignment, you don't have to notify us. We keep track of that via the Gravescope portal and we count those late days at the end. We don't have to notify us if you're using a late day. All right, any questions before we begin? All right, so at the end of last lecture, we had to start to talk about systems and so we had defined a system as something that transforms some input signal x of t into an output signal y of t. This should say signal map system. And last lecture, we just went over a few example systems and so I'll just recap one of them. One of them was AM radio where we may have some message xt that conveys information that we want to convey and the AM radio system what it does is it takes your message x of t and will multiply it by a signal cosine 2 pi fct and then y of t then would be the output of our system where again the message the input x of t has been transformed through multiplication by this cosine. All right, so that's just an example system and we'll talk about we'll talk about today several properties of systems. And so on homework number two and throughout the rest of this class will sometimes be asked to show some proper that some systems exhibit various properties and so today we're going to start talking about some of those properties and so a first property is called stability. Okay, and so a system is called bounded input bounded output stable or bibo stable if every bounded input leads to a bounded output. Right, so that's the definition of stability. Let's unpack this and see what it means. So the first thing we want to define is what is a bounded input or a bounded output. All right, so a bounded input is a following. Some input x of t is a bounded input. If there exists a constant n subscript x such that the following is true. The absolute value of x of t is less than or equal to this constant m subscript x which itself is less than infinity and this is true for all time. So let me draw a picture then of what a bounded input, sorry, what a bounded input looks like. So let's say that we had some signal, draw more straight line. Let's say that we have some signal x of t here. x of t is going to be called a bounded input if I can define some constant. And so this constant would take on the value mx on the y axis. This value here is mx. If x of t its magnitude is always less than mx. So if x of t stays below this line then it's going to be a bounded input. And that makes intuitive sense. If bounded means that I can find a constant such that x of t goes no higher than that. And similarly a bounded output is the following y of t is a bounded output. If there exists a constant m y such that similarly y of t is absolute value is less than or equal to m subscript y which is a finite constant for all time. Any questions on the definition of bounded input or output? Alright so then the system is called bivostable when a bounded input meaning that the absolute value of x of t is less than a constant which is less than infinity implies that the absolute value of y t is less than a constant which is less than infinity. So that's what bivostable means. Give me an input that I can bound and it's going to be bivostable if I can also bound the output. Alright so let's go ahead and do some examples. So the first one that we'll do is a and radio where again a and radio means we take some input x of t and the system is I'm going to take x of t and multiply it by cosine of omega ct and together that's going to give me my output y of t."
"All right, everyone. We're going to get started for today. So a few announcements before we begin. I sent out a CCLE announcements with instructions on how to sign up for Piazza and grade scope. And so those are just recapituated here as well as a link to a Python tutorial. If you have only used MATLAB in the past, this tutorial will probably be helpful for you. And then we have also uploaded to CCLE material reading for this class, the lecture notes, and then we also uploaded all the midterms and final exams dating back to 2017. If you're someone who isn't sure if the pre, if you satisfied the prerequisite material, I encourage you to take a look at the midterms and final exams as well as the common filter dot pdf which are probably the most technically demanding for this class. Any questions on any announcement related matters? Any course logistics?"
"All right, everyone. We're going to get started for today. So a few announcements before we begin. I sent out a CCLE announcements with instructions on how to sign up for Piazza and grade scope. And so those are just recapituated here as well as a link to a Python tutorial. If you have only used MATLAB in the past, this tutorial will probably be helpful for you. And then we have also uploaded to CCLE material reading for this class, the lecture notes, and then we also uploaded all the midterms and final exams dating back to 2017. If you're someone who isn't sure if the pre, if you satisfied the prerequisite material, I encourage you to take a look at the midterms and final exams as well as the common filter dot pdf which are probably the most technically demanding for this class. Any questions on any announcement related matters? Any course logistics?"
"All right, everyone. We're going to get started for today. So a few announcements before we begin. I sent out a CCLE announcements with instructions on how to sign up for Piazza and grade scope. And so those are just recapituated here as well as a link to a Python tutorial. If you have only used MATLAB in the past, this tutorial will probably be helpful for you. And then we have also uploaded to CCLE material reading for this class, the lecture notes, and then we also uploaded all the midterms and final exams dating back to 2017. If you're someone who isn't sure if the pre, if you satisfied the prerequisite material, I encourage you to take a look at the midterms and final exams as well as the common filter dot pdf which are probably the most technically demanding for this class. Any questions on any announcement related matters? Any course logistics?"
"All right, everyone. We're going to get started for today. So a few announcements. First is a reminder that homework number six is due tonight. I uploaded to grade scope by 1159 p.m. Last night we sent out an announcement on CCLE regarding the final exam and particular details about it. So the first detail is that Tomoy will be holding a final exam review session from 3 to 6 p.m. on Sunday June 6, 2021 at this Zoom link. And Tomoy will be uploading a problem set to CCLE and the solution will be also uploaded after the review session. He will also record the review session for people who can't make it at the time until posted on CCLE. And we highly encourage you to attend the review session. Right? Any questions about the review session? All right, beyond that, we have details about the final exam. So the final exam is in six days on June 8th and is from 3 to 6 p.m. We sent out an email regarding the details, which will be similar to the midterm."
"All right, everyone. We're going to get started for today. So a few announcements. First is a reminder that homework number six is due tonight. I uploaded to grade scope by 1159 p.m. Last night we sent out an announcement on CCLE regarding the final exam and particular details about it. So the first detail is that Tomoy will be holding a final exam review session from 3 to 6 p.m. on Sunday June 6, 2021 at this Zoom link. And Tomoy will be uploading a problem set to CCLE and the solution will be also uploaded after the review session. He will also record the review session for people who can't make it at the time until posted on CCLE. And we highly encourage you to attend the review session. Right? Any questions about the review session? All right, beyond that, we have details about the final exam. So the final exam is in six days on June 8th and is from 3 to 6 p.m. We sent out an email regarding the details, which will be similar to the midterm."
"All right, everyone. We're going to get started for today. So a few announcements. The first is at homework number five, which we uploaded to CCLE last week is going to be due this Friday. I'll put it to grade scope by 1159 PM. And then later this evening, we're going to release a midterm grades, the statistics as well as the solutions. Are any course logistics or admin questions? All right, so it's been over a week since our last lecture. So I want to recap a bit about what we were doing last time. We had derived the Fourier transform, which is the generalization of the Fourier series to arbitrary signals that may not be periodic. And so it's like the Fourier series and that it tells us the frequency content of a signal. All right, and so the Fourier transform capital F of J omega is this formula and computer Fourier transform of our time domain signal F of T. And there was some discussion last lecture about why we have a J here. So in other settings, you may just see the Fourier transform written as F of omega. In the end, the Fourier transform is a function of frequency."
"All right, everyone. We're going to get started for today. So just one announcement. One second. All right, great. Just one announcement before we begin, which is that homework number two is do this Friday uploaded to grade scope by 11.59 PM. Any questions on class logistics? All right, so we're going to continue talking about the impulse response today. And so last lecture, we had started off with the motivation of why an impulse response. And we said, well, oftentimes in life when we are interacting with the system, we may not have the luxury of knowing exactly what the system is. What is the function that it implements? Or maybe we only know it imperfectly. Further, even if we did know what as well as it could take on a complicated form. And so if we don't know what S is, how are we going to calculate what a system does to an input to transform it to an output. And so the impulse response gives us an answer to this question. It says, as long as I know the impulse response, I don't have to know exactly what S is. But if I know it's impulse response, then I'm going to be able to calculate the output, which is my Y of X. For any input given which type Y of T, Y of X. I can calculate the output Y of T for any input X of T. And so though I don't know S, I can get from X of T to Y of T as long as I know what the impulse response is. Right. So this is a very powerful tool for in particular linear time and varying systems. So the last lecture we said, well, what is the impulse response? What it is is if I take some system here, capital H, and I apply an impulse at the input."
"All right, everyone. We're going to get started for today. So just to announce for today, the first is a reminder that homework number six is due a week from today. And the other is that today we're going to be doing the derivation of the common filter recursion. And this is probably the most challenging derivation in the class. And it's going to piece together a lot of things we've talked about. The derivation is challenging, so you're not going to be tested on reproducing the derivation. But when you look over the derivation, you should be able to understand every step that occurs in the derivation in terms of us knowing the facts for each step. And so again, this derivation will be challenging and you won't be tested on reproducing it. But we really do encourage you to look at the derivation and make sure that you understand every step along the way. So today we should be finishing common filter. And then if we have some time, we're going to start the last topic for the course, which will be dimensionality reduction. And we'll be discussing a technique called PCA, which is principal component analysis, and one of the most commonly used dimensionality reduction techniques. All right. Any questions before we get into the common filter derivation? All right. Question from Andrew."
"All right, everyone. We're going to get started for today. So just to announce for today, the first is a reminder that homework number six is due a week from today. And the other is that today we're going to be doing the derivation of the common filter recursion. And this is probably the most challenging derivation in the class. And it's going to piece together a lot of things we've talked about. The derivation is challenging, so you're not going to be tested on reproducing the derivation. But when you look over the derivation, you should be able to understand every step that occurs in the derivation in terms of us knowing the facts for each step. And so again, this derivation will be challenging and you won't be tested on reproducing it. But we really do encourage you to look at the derivation and make sure that you understand every step along the way. So today we should be finishing common filter. And then if we have some time, we're going to start the last topic for the course, which will be dimensionality reduction. And we'll be discussing a technique called PCA, which is principal component analysis, and one of the most commonly used dimensionality reduction techniques. All right. Any questions before we get into the common filter derivation? All right. Question from Andrew."
"All right, everyone. We're going to get started for today's lecture. So one of the nastiest before we begin, which is that R&OB, R&OB and RTAs are uploaded homework number one to CCLE in the week one materials on Friday of last week. This homework is due this Friday, I'll upload to grade scope by 11.59 p.m. Right? Any questions on any course logistics? All right, so we're going to get right into material then. So last lecture, we had for us to talk about signal operations and properties. We talked about time scaling, reversal and shifting, as well as even in odd signals. And we had finished class talking about periodicity. So today, I want to finish periodicity and then also want to get through material on energy and power signals as well as complex numbers. And hopefully we'll also be able to start some signal models today. So last lecture, we had talked about what a periodic signal is, which is a continuous time signal is periodic. It's an only if there exists a big T0 and this big T0 is called the period of the signal. And if the signal repeats every big T0, then the signal is called periodic. Right? And then we also showed last lecture that if it repeats every big T0 and is periodic, then it'll also repeat every two T0, three T0, four T0, five T0, etc. All right? So I wanted to start off this lecture by just talking about the simplest periodic signal, or one of the simplest periodic signals. And then we'll have a brief poll question after that. So one of the most basic signals in this class that we'll cover is the sign or the cosine wave."
"All right, everyone. We're going to get started now for ECE189. I want to ask if there are any questions on the seminar or even one or two related things before we begin. All right. So we're going to continue where we left off last time, which is last time we start to talk about neurons and how they signal. And so last lecture, we had talked about how neurons essentially have three parts that are important. They have the inputs, which are the dendrites, and they're these arborous life structures. And then there's an integration center, which is called the axon collect. And what this integration center does is it receives voltages from all of these input dendrites and it adds them together. And if that sumed voltage is above a threshold, it's going to send an output called an action potential. And that goes along is axon, which then connects to the dendrites of other neurons. And so the axon, we can think of as the output of the neuron. All right. And we talked about how even though neurons have many different morphologies and shapes and sizes, they are essentially the same in function that they are always going to have inputs coming in on their dendrites. They're going to sum them together. And if it's a big enough value, they're going to fire off an action potential to them downstream neurons. And the complexity then in signaling arises from how we connect neurons, much like how in computer circuits or analog circuits, all transistors operate based off of fundamental principle. And all transistors have per CMOS, the source of draining the gates, but then connect them in special ways. And you have a common source amplifier, or you have a common gate follower, or you have an x-or gate, or an or gate, or an AND gate, etc."
"All right, everyone. We're going to get started. So for today, announcements are a reminder that homework number three is to this Friday. And last lecture and on CCLE, we announced that 20% of the homework three questions are going to, you're going to automatically receive full credit for them so you don't have to do them. So be sure to check out the CCLE announcement to make sure you know what those questions are. A heads up, the midterm is not this Monday, but the Monday after November 9, 2020, and it's going to be held during class time. If you're in another time zone and cannot take the midterm during this time, please be sure to have emailed me already. As we said in the beginning of class, all the past exams from prior quarters that I've taught this class are on CCLE. I want you to note that we usually hold the midterm in the week of the Veterans Day holiday and in former years that holiday occurred later on in the quarter."
"All right, everyone. We're going to get started. So this is ECE C143A, C243A, Neurostic No Processing. I am your instructor, Jonathan Cowell, and on behalf of RTA's Tonmoie and Shashank and myself, we're excited to be your teaching staff for this quarter as we delve into topics related to neural signal processing. And today we're going to unpack a bit about what that means. We're just going to start off lecture by going over a few quick Zoom guidelines. So if you're taking a class with me before, you know that I'm very happy to answer questions and try to clarify things and that I encourage you to ask questions. If you have a question, very likely another student has a question and it gives me an opportunity to re-explain something or clarify that. And so please ask questions by raising your hand. You won't be able to unmute yourself on your own, but if you raise your hand, I will see that and then be able to unmute you so that you can ask your question. In addition to this, you can ask questions in the chat. So I will not be monitoring the chat, but RTA's Tonmoie and Shashank, they are going to be monitoring the chat continuously. And so if you write questions there, then they'll be able to answer questions in the chat. And then lastly, everything that we do for lecture in terms of everything we present here, as well as the annotated notes, we are going to be uploading those to CSELE. And so if you're not able to view a lecture live, which we do encourage, the lectures you can watch post talk on CSELE as well."
"All right, everyone. We're going to have homework number six not do this Friday but do Monday of next week. All right. And then this homework will cover material up to and including this lecture. And so we had a typo on the homework that said it covers material up to lecture 14 should just be up to include, including today's lecture. Alright, any questions. Before we begin. All right, so we're going to continue with frequency response today. You'll recall that after Fourier transform and knowing the convol- Melissa. Are you going to be holding office hours, adjusted office hours this week? Yes, I, so I won't be holding office hours on Thanksgiving. I did see a Piazza post requesting me to change my office hours. I'm looking into that right now because I, I'm not sure I'm going to have availability prior to the day the homework is due. But if I if I'm able to find a spot potentially on the weekend, then I will do it."
"All right, everyone. Welcome to each one of two for the fall quarter of the academic year. I'm your professor Jonathan cow. And we also are we also have three TAs here are we not. Go Sh Conway Montsler and Guangwen Zhao right and so I wanted to just start off with a few words about online lecture before we begin. So the first is that as you all know, the lectures, discussions and office hours for this class are going to be carried out on zoom. So lectures and discussions are also going to be recorded and uploaded to CCLA. We encourage you to attend lectures live, which will be Monday Wednesdays, 2 to 350 PM Pacific time, as will aim to have these lectures be interactive and also have an opportunity to ask questions. If you don't want to appear in lecture video, it's fine to opt out, but because we're recording these for the benefit of all students, we asked that you opt out by not attending. This will be the link for lecture for the entire quarter and also be the link that I use for my office hours. Alright, so in addition to that, so by default, everyone in this lecture is muted during by lectures. However, I do want these lectures to be interactive. And so we allow there to be questions and we'll do it through the following mechanism. The first is through the raise hand button. And so if you raise your hand, which I'm sure all of you know how to do, but if not there are instructions here, then when there's a natural breaking point during lecture, then I'll take questions. The other way is through chat functionality. And so our TAs are not on boy and Guangwen who introduced at the end of class. They'll be monitoring the chat. And so if you type in a question into the chat, that will be seen by the TAs and the TAs will be able to answer that question. And we just asked that you also record your question."
"All right, happy to start off anywhere. Yeah, Link's here. Okay, so I actually have a question about a lecture. I can really confuse about the Q and W matrix. I assume they are similar. So I'm just confused about first, why is this symmetric? It's one of my problems. And basically, I know that in lecture, we discussed about how we can just ignore the W matrix because somehow we average them out. Essentially, and then we have xk equals to a xk minus 1. I just would like to know, like, what's the reason behind it? Yeah. I think I'm just like wanting to do like idea about it. So that's basically, I'm just really confused about how the noise was set up basically. Okay, great. So first, let me wait for this to catch up."
"All right, homework to the gray. Let me pull it up. Yeah. Which notebook is it on or is it the? It's just the problem three. Oh, yeah, the problem three notebook. Okay. Problem three. The in homogenous problem prophecies. Yeah. Okay. Yeah. So my question is, so I did problem two, which is the homogenous one and I understand it. And like everything is striking out properly. When I'm trying to generate the histogram, so part B."
"All right, Sal? Hi. I just wanted to ask a quick question, and I'll stick around too. For this homework that we ��� homework five, I've been working at it. I was just curious, how much of the ��� with what we've already gone through, how much of this homework do you expect us to be able to complete with the material that we already know. Yeah, so Fourier series number one should be able, you should be able to do. Yeah, I did one. The number two is symmetry properties of the Fourier transform. So we did go over this in class briefly since they were the same as the Fourier series. So we have to go. So are we going to go over it more or is this going to be like kind of that's going to be the exposure to it and then that will be the exposure to it. So we won't cover that anymore. And so, yeah, for that one, you'll need to spend some time, likely with with the symmetry properties and just thinking about what they imply. And so, And then the figure one will take some time to."
"All right, so excuse me. We're going to go ahead and finish off the syllabus. So last time we talked about the grade breakdown for this class and that it was graded on an absolute scale. And I know I went through it a bit quickly at the end, so I just want to put up this slide to ask if there are any questions here on the grade breakdown for the grading scale. All right. So then we had also put up this information about pass no pass or satisfactory on satisfactory grading if you choose to take the class in that manner. We also talked about how for exams during remote learning, the exam are going to be open note open book and you can access your notes and CCLE on your computer, but it's going to be closed internet. And that the TAs and I we are going to perform some analysis on the answers given. And if we suspect anyone collaborating, which are not allowed to do for the exams, we request reserve the right to give a superseding oral exam. Right. And then also if you are in a different time zone, we can make accommodations for you to take the exam at a different time."
"All right, so excuse me. We're going to go ahead and finish off the syllabus. So last time we talked about the grade breakdown for this class and that it was graded on an absolute scale. And I know I went through it a bit quickly at the end, so I just want to put up this slide to ask if there are any questions here on the grade breakdown for the grading scale. All right. So then we had also put up this information about pass no pass or satisfactory on satisfactory grading if you choose to take the class in that manner. We also talked about how for exams during remote learning, the exam are going to be open note open book and you can access your notes and CCLE on your computer, but it's going to be closed internet. And that the TAs and I we are going to perform some analysis on the answers given. And if we suspect anyone collaborating, which are not allowed to do for the exams, we request reserve the right to give a superseding oral exam. Right. And then also if you are in a different time zone, we can make accommodations for you to take the exam at a different time."
"All right, so excuse me. We're going to go ahead and finish off the syllabus. So last time we talked about the grade breakdown for this class and that it was graded on an absolute scale. And I know I went through it a bit quickly at the end, so I just want to put up this slide to ask if there are any questions here on the grade breakdown for the grading scale. All right. So then we had also put up this information about pass no pass or satisfactory on satisfactory grading if you choose to take the class in that manner. We also talked about how for exams during remote learning, the exam are going to be open note open book and you can access your notes and CCLE on your computer, but it's going to be closed internet. And that the TAs and I we are going to perform some analysis on the answers given. And if we suspect anyone collaborating, which are not allowed to do for the exams, we request reserve the right to give a superseding oral exam. Right. And then also if you are in a different time zone, we can make accommodations for you to take the exam at a different time."
"All right, so I want to pause here and ask if there are any questions on final exam logistics or homework or any other class logistics. Alright, great. Sounds like there are no questions so we're going to get back into material then. And then we will then address Laplace transform applications. All right. I see a question in chat saying how many questions will there be on the exam? We can't disclose that information, but the length of the exam will be will be comparable to prior years and the prior years as final exams are all on CCLE. I should say the exam length will be comparable to or shorter to prior year since we also recognize that this year. There are extraordinary circumstances and online teaching. Okay, so So in the lecture we had talked about how we had talked about the Laplace transform and several properties."
"All right, so office hours are being recorded now and we'll do the raise. I'll take questions in the order that hands are raised. Collecky. All right, I was kind of used on number 4D on the homework. Sorry, could you say 4D or 4D? 4D. Have the harmonics one? What was your confusion on the question? Just like how to go about the question. I, yeah. All right, yeah. So did other people have questions on 4B? So should we do 4B1 or 4B? Actually, that's 4B1 and then what do you? Yes. So I think the question is, inventions on harmonics? I don't know if there's some property or something like that that we're supposed to know and use. Yeah. So do the five harmonics?"
"All right, so that's the next relationship that we have, which is C star k is equal to minus C of minus k. And then we have two other relationships for the amplitude and the phase. So can someone tell us how you approach these questions? For the amplitude, it should stay the same because the negative gets squared again. Perfect. Yeah. So, Melissa says that Ck, its magnitude, equals C minus k, its magnitude. When we calculate the magnitude, it's the square root of the real part squared plus the imaginary part squared so the negative sign will go away just like Melissa said. Okay."
"All right, so those are the details from the midterm announcement on CCLE. HKN is going to be holding a review session for the midterm for this class. It's going to be Friday, November 6th from 4 to 6 p.m. Pacific Standard Time at this Zoom link. Now I didn't request, or I haven't coordinated this review session with HKN. These are other students who have taken ECE 102 before and wanted to do this service to help students review the material. And so again, we haven't coordinated with them. But likely, topics that they cover are going to be related to what we'll do, what we've covered in class. All right, and then Tom boy is going to hold a midterm review session."
"All right, so through this node C, A gives me information about B. And then the second question we asked is is A independent of B given C. And the intuition here is that the answer should be yes. And the intuition is this, when I know C, I observe its value for A. And so then A being independent of B given C tells me in intuition terms that is asking the question, does A give me any more information about B than I already knew from knowing C. The way that A and B were dependent is that A gave me information about C that then gave me information about B. But now if I know C, A doesn't give me any more information about C. I know C perfectly. And so I don't gain any information about B from knowing A because the information I have about B, I already know from observing C. All right, so those are the two intuitive answers. I know we covered this intuition a bit quickly at the end of last lecture. So I want to ask if there are any questions here. All right, so then we're going to go ahead and do the more rigorous derivation."
"All right, so we'll get into material then. So, oh and a reminder, the midterm covers up until the end of Fourier series. So we're going to talk about Fourier transforms today, and this material will not be on the midterm exam. So we've talked about how for Fourier series we can decompose any signal as a sum of complex exponentials and this is for periodic signals. But in reality or in real life in several applications we'll work in, the signals that we work with are not periodic. And so we need a generalization of Fourier series to signals that are not periodic and And this is called the Fourier transform."
"All right, then we are going to then go ahead and what we'll do today is we'll talk about. I'll say some videos of the callman filter performance since we spent a lot of time last night during the derivation, but what's come up at level now and see how the callman filter actually does. And then after that, I wanted to introduce this idea of dimensionality reduction and present the mathematical derivation of principal components analysis, which is one of the most simple techniques for dimensionality reduction. But incredibly powerful and like the callman filter, powerful beyond applications and neural data, just generally in data science, PCA can be very helpful. All right, so for the callman filter, we call that as in homework number six, right for the comment filter, we have a linear dynamical system where now we have instead of just one equation relating neural data and kinematics. We have a second equation, which tells us how kinematics evolve through time in principle ways. And this a matrix that dynamics matrix reflected that. And so in the homework, we had you all implement this a matrix where the positions obeyed physics because they are integrative velocities. And then we had you use maximum likelihood estimation to infer the parameters of this little two by two matrix here, which tells us how the velocities at time k, update to the velocities at time k plus one. And this model is inertia, so in the homework, you should have gotten values in this matrix that are below one, but are, but that are not close to zero. They're the closer to one. And that means that the velocity at time k plus one is going to be close to the velocity at time k. And so that means that if you're moving to the right at time k, then this equation is saying at time k plus when you're probably still moving to the right. Okay, and so that's how these velocity terms incorporate a smoothness or inertia. Right. Any questions here. So then after that, we went through the common filter recursion, which we derived last week, and you should have been able to implement the wild loop that runs through these equations and then calculated these m1 and m2 matrices."
"All right, then we are going to then go ahead and what we'll do today is we'll talk about. I'll say some videos of the callman filter performance since we spent a lot of time last night during the derivation, but what's come up at level now and see how the callman filter actually does. And then after that, I wanted to introduce this idea of dimensionality reduction and present the mathematical derivation of principal components analysis, which is one of the most simple techniques for dimensionality reduction. But incredibly powerful and like the callman filter, powerful beyond applications and neural data, just generally in data science, PCA can be very helpful. All right, so for the callman filter, we call that as in homework number six, right for the comment filter, we have a linear dynamical system where now we have instead of just one equation relating neural data and kinematics. We have a second equation, which tells us how kinematics evolve through time in principle ways. And this a matrix that dynamics matrix reflected that. And so in the homework, we had you all implement this a matrix where the positions obeyed physics because they are integrative velocities. And then we had you use maximum likelihood estimation to infer the parameters of this little two by two matrix here, which tells us how the velocities at time k, update to the velocities at time k plus one. And this model is inertia, so in the homework, you should have gotten values in this matrix that are below one, but are, but that are not close to zero. They're the closer to one. And that means that the velocity at time k plus one is going to be close to the velocity at time k. And so that means that if you're moving to the right at time k, then this equation is saying at time k plus when you're probably still moving to the right. Okay, and so that's how these velocity terms incorporate a smoothness or inertia. Right. Any questions here. So then after that, we went through the common filter recursion, which we derived last week, and you should have been able to implement the wild loop that runs through these equations and then calculated these m1 and m2 matrices."
"All right, we're going to get started for today. So a few announcements. The first is a reminder that homework number two is do on Monday in five days on April 26th, uploaded to Gradescope by 1159 PM. And then last lecture, someone had asked about the phantom factor of the Pricot G. It's a thousand cerebellum. And from this paper by Brits and Sour Bray and colleagues in their on 2015, they reported that during locomotion, the Pricot G cells actually have a very variable firing rate. And so their phantom factor on the y-axis is variance and on the x-axis is mean. And so the phantom factor was one, then the data points would all lie along the line, meaning that the mean and the variance are always equal. You can see the dots are generally above the line. Each of these dots is one neuron. And so neurons generally have, for Pricot G, neurons, higher variance and higher spike count mean."
"All right, we're going to get started for today. So just two announcements. The first is that homework number six is due this Friday, and then, including today we have three lectures left so we're going to finish sampling today and then the rest of our Any other admin related questions. All right. Cool. So, we can't store the value of the signal at every single point in time because there's infinitely many of them. And so we have to store samples. But then, now comes the question. How quickly do I sample my signal because if I sample very quickly like up here, then I'm definitely going to be able to know what my original signal was if I sample too quickly, then I'll also end up using a lot of memory storage to store the signal. And so what we're going to finish deriving today is the sampling theorem, which tells us that, tells us the minimum, sorry, tells us the maximum time in between samples, or the minimum rate at which we need to sample a signal, so that we can perfectly reconstruct it."
"All right, we're starting our office hours now. Hi, Professor. Hi, hi, Chris. Hey, um, I got a little bit confused in the last graph that we talked about in lecture just now. Okay. The one with the introduction of the plus. All right. Ion channel, I just wanted to clarify a few things. Yep. First of all, is there a reason why we use that negative 65 millivolts as our resting potential, or sorry, not our resting potential, our resting voltage because like in an actual neuron you have both K plus and NA plus?"
"All right. All right everyone, We're gonna get started for today. Can people hear me? Alright? Alright, it was a rainy treasures walk away over. One more. I am Jonathan cow. I am your instructor for this course and on behalf of our team of TAs tomboy, can shift to lie, shouldn't be, and remain who will introduce themselves later. We are really excited to be your instructors for this course, which is C1 47, 47 neural networks and deep learning. All right. Today we are going to be basically doing an intro into deep learning. We'll go over the syllabus and you have time. We'll begin lecture material. Alright? So high level of things before we begin. The first is that this is actually one of my favorite classes to teach. Feels really rewarding because as a result of this process sees so many students who come back to me and say that they found deep learning does for further research are useful as they went on to industry. So deep learning is a very relevant topic for many different applications areas today. And we hope that this class will be useful for most, if not all of you. This class will hopefully be fun in terms of, I know that for many of you this is required classes and elected at the same time as we'll talk in the syllabus, there will be quite a bit of work for this class. We'll discuss that with the syllabus, but please take that into account as you determine whether this process for it or not. And then if you've ever taken a class with me, you know that I like to keep things interactive. Lecture hall this bid. So please feel free to ask questions. I will be very happy to entertain your questions. And usually if you have a question, you're not the only one with that question and you ask them that will give me an opportunity to explain something once again. Alright. Any questions before we got one quick thing? So I think I already saw there are many students who are joining via Zoom. So if you think about the lecture, first off, this class will be reported by doing tasks. And then brewing costs will upload those bacteria to grow and learn. Use it the under a tab called UCLA media reserves. We'll send out an announcement with all of this information. Because this class has also taught for the MS Online students. We're going to have a live Zoom Room, which we'll share the contents of my iPad screen and also record the audio. One of our TAs can is going to be monitoring the chat. And so while we won't take any verbal questions from the Zoom room, will be verbal questions for those who are in-person in the classroom. If you have a question on lecture material and just put it in the chat and camera or one of the other TAs will get through any questions there. Alright, well get started. So if you've been following the news lately, you'll know that there's this new chat bot activity who here has interacted with just one half the class. Shattered beauty is created by OpenAI and it's a very powerful language model that can do some pretty remarkable things. And so last night I asked chat TBT. I would be best for me to motivate this class. So I said motivated course to find the corresponding enforce sentences. And this is what she did. She says, deep learning is a revolutionary field of AI that is driving many of the recent advances in machine learning. It has the potential to transform a wide range of industries and has already begun to do so in areas such as images, speech recognition, natural language processing, and machine translation. Uh, deep learning course will provide you with the skills and knowledge you need to understand and apply these powerful techniques to the real-world problems. Whether you are a beginner or experienced practitioner, a deep-learning course can help you take your careers conducts bubble. The rapidly growing field of AI. That's a pretty good introduction, might even be better than this and that I could write on this. And it actually comes pretty close to the intro I usually give for this class. But just to motivate the applications of deep learning and get you excited about how it can be applicable to the things that you're interested in. There's one thing here is that whether you're a beginner or an experienced practitioner. So we just decided to be an introduction to deep learning course. If you're an experienced practitioner student in this class, this class will be easy for you, so this should be fairly for beginners. Alright. I usually motivate this class is going to talk about its impact. And one way to talk about the impact of deep learning and neural networks is to point out that you have publicly today interacted with at least one, might be several neural networks. So starting off with, if you've done a Google search, we know that you're going to use it the page rank algorithm to show you the search results. However, Google also augments the page rank algorithm with other deep learning-based techniques, including rank brain as well as neural matching. Neural matching, e.g. isn't NLP based technique. And one example of how it is this sport is it tries to infer the meaning of what you're saying. A commonly given the sample is if you are searching for a boot on Google and you are, the less you're likely looking for footwear. But if you're in the UK, you're probably referring to the trunk of the farm. All right, So Google uses deep learning to augment their surface. And if you searched on Google today, if it's rapid with a neural network, you might also help me know that I do go, there are reports that there is a code red over there in response to chat TTT as to whether and how that threatens Google search engine. The search engine. You start to knock people if you do not get to today. And you've been sharing videos, suggested videos, those recommendations come from deep-learning based algorithms. In part. If you've been on Facebook today, facebook uses a variety of deep neural networks to cases that comes up. One is something called deep text. So here is a video of Facebook Messenger. Now, I don't use Facebook, so I'm just trusting that this is actually a Facebook Messenger. But if you say something like, I need a ride, then they'll give you a link that says request a ride. Right? But it's not just looking for the word ride and showing you that week whenever you save, right? Because there are many sentences. Include the word ride, but don't need you to request a ride like this one here that says I don't either I or this one below that says, I like to write it down. Alright, so within this speech processing and NLP road, neural networks are really useful for trying to understand sentiment or even the meaning of what they're trying to compare. In addition to this, better uses something called deep base. And so here are two pictures of Emilia Clarke, but one, she's dressed up as dinars target area. And even though the therapists are very different, face, which is metaphase algorithm is able to verify that these are facts, the same person and even more from the images alone, they can deduce many things, including the emotion conveyed and can even guess the age share. So share is predicted to be 31. And I think they say that they're using accurate plus or minus five years. In fact, Image Recognition, Computer Vision was one of the key fields that really helped to spur a revolution in deep learning that we've been experiencing for the past decade. So this is a video of a professor at Stanford talking about the impact of Computer Vision and massive data from ImageNet and the modern CPUs and GPUs to train such a humongous model. The convolution massive data from ImageNet and the modern CPUs and GPUs in such a humongous model. Let me just see if I can find it probably is. I'll just try to hold the mike really first. Raise your hand. And the faculty had trouble getting massive data from ImageNet and the modern CPUs and GPUs to train such a humongous model, convolutional neural network blossomed in a way that no one expected. It became the winning architectures to generate exciting new results in object recognition. This is a computer telling us this picture contains a cat eye where the cat is. Of course you are more things that cats. So here's a computer algorithm telling us the picture contains a boy and a teddy bear, a dog, a person in a small cavity in the background. Or picture of very busy things like a man, the skateboard, railings and lampposts and so on. Sometimes when the computer is not so confident about what is C's. We have taught it to be smart enough to give us a safe answer instead of committing too much, just like we would do. But other times, our computer algorithm is remarkable at telling us what exactly the objects are like the make model year of the car. All right, any questions on this example of the make model year, the car is pretty remarkable because we'll notice a few things. First, all these cards are different sizes, right? So let me backtrack. If I asked you, how would you identify that a car? You might say things like car is relatively large compared to other things in the scene. Or you might say, a car has four wheels, right? But then in this image, the cars can be at different depths and so you don't want them the same scene. There are different sizes. And then this for this vehicle, only two are visible. And actually this image is cut off. And so this top vehicle, only one is visible. You and I all know that that's a car. But if you're just trying to hand craft an explanation for the features that tell us what a car is. Might be a bit harder to put that into words because these images are all cars. Have four questions here. Alright. So beyond those examples, if you've used recently are today, to try to determine what's the page. You'll know that York has a section called popular dishes where they show you things that are popular to order at the restaurant. That includes deep learning, which identifies the images. And what I didn't say, we're going to write that the image is coming from the different reviewers are of the same object. Will also use deep learning to parse the reviews of the users who uploaded those images to determine their sentiments and evaluate, was this a positive review or was this a negative review? And use that to figure out what the popular dishes. If you've interacted with Alexa or Siri or Google, you have interacted with the neural network because the speech processing algorithms on these all rely on neural networks. If you use Twitter, than the Twitter spine line would have determined to show you is based on deep learning. Lastly, if you have lost something, a lot of these recommendation systems today also use neural networks. And there are algorithms that you might be most likely, very likely you have interacted with the neural network today. And that's just the ubiquity of deep learning. Just in our everyday lives. Beyond that, deep learning is able to achieve things that are remarkable to us. So this is based off of something called GPT-3. It's also published by OpenAI and shares many similarities. Similarities are overlaps with that chapter. In this video, what we're going to see is that if you use a neural network to get a description of something degenerate, and then the neural network would generate the code to create that. He said here, some auto-generate a button that looks like a watermelon. And here is the code that generates that for them. Somebody who you might find this even more impressive injections, since it actually generates combined with Voltaire. There's syntax error. Then after correcting it generates exactly what the fastball. Right? Beyond this, you can also generate things that aren't as naturalistic. So here, this is a tool called Adobe, who here has interacted with Dolby on it. I say that somebody is like 30% of the class. You can give a text prompt and it'll generate images according to that prompt. And so this is an illustration of a baby daikon radish in a tutu walking a dog. I'll give you a few seconds to think about what that might look like. Alright, and this is what the AI generated image makes. An armchair and the shape of an avocado Looks like this. And then a storefront that has the word open, hey, I've, it's done. It looks like this. Yesterday went on dali also. And here is a UCLA professor teaching deep-learning in a large lecture hall. The large vector. Hello, What's more grandiose because this, but just a bit. Here's Joe grew and doing cartwheels at the Rose Bowl. Alright. So I also mentioned at the very start that deep learning has this really diverse applications. And many students find it useful in their research areas or even in big industries that they go into. So here's a video of Jack gene. This is from when Google was releasing TensorFlow. And he talks about a very interesting application of neural networks. It system. We built this called TensorFlow and we use it for everything that we do for this area. And so the system we've built this called TensorFlow, and we use it internally for everything that we do for these emphasis area. And last year we decided we would open source it because we wanted people to have the ability to take this software, download it for free, and use it for their learning problems. It's been really great to see different things that people have used it form. So here's one example. There was a Japanese cucumber farmer. And it turns out when you harvest your cucumbers, you have to sort them into all kinds of different categories for sale. Small ones, medium ones, large ones, particularly one's not particularly ones straight one's curved on. This is pretty complicated and pretty time-consuming and harvest time. So the farmer was able to take a camera and using a computer vision model that he trained with TensorFlow, actually have the division model determine what category of cucumber was looking at and then rigged it up to some conveyor belts and some little switches that would push the cucumber into the right box. And so this eliminated many days of labor that the farmer and his wife would have to do it at harvest time. Just one tiny example of something you can do now that would be hard to four. Alright, so I hope you hear there's just been talking about a Japanese cucumber farmers used TensorFlow and in turn disability to straightforwardly implement neural networks. To be able to easily classified cucumbers into different categories. And that's saved a lot of their time at work. When I first heard this, I just thought Japanese cucumber farmers are super hardcore. I teach this stuff and I would've never thought if I was to tell the farmer to do that and also their cucumber farmers of Kennedy go deep learning. So I looked into this a bit further. And they kick up and farmers there sudden as a software engineer. And he was able to build a system that uses a neural network to get a visual classification of the cucumber. And here's that system network. Questions there. All right. Continuing on, its applications are diverse. Learning has a lot of important applications in a variety of different fields. I've been on several PhD committees after having taught this course for students are using deep learning, e.g. to try to classify the fMRI scans and PET scans correspond to certain diagnoses. Here's an example. Here's a recent example from DeepMind, which is one of the challenges in biochemistry and biology, is to understand what the structure of a protein will be based on the sequence of amino acids. And so DeepMind created an AI based off of deep learning that is able to take the sequence of amino acids that comprise a protein and very accurately predict its structure. And this has been a challenge for many decades that DeepMind has a big lead towards solving as a result of deep learning. And this one is called alpha. Beyond that, it's used in other areas and other example I like to give is a cancer moonshot. There is this thing called candle, where it tries to integrate disparate sources of information about the chemistry until drug, how a patient might respond to it, and the types of treatments that the patients are going to try to get the success of that cancer drug in helping the patient. Another area where it's helpful is in developing new technologies. And so this is a video that talks about how there are many challenges and self-driving vehicles. Same time, neural networks are at the foundation of self-driving vehicles in terms of being able to take what's been seen by the camera, parcel of what's going on in the road and then make decisions based off the bat. This is a challenging problem. So let me play this video. And then once you have that problem-solve, the vehicle has to be able to deal with construction. So here's the cones on the left are forcing it to drive to the right. So not just construction in isolation, of course, it has to deal with other people moving through that construction zones as well. And of course, if anyone's breaking the rules, so the police are there and the car has to understand that that flashing light on the top of the car means that it's not just a car, it's actually a police officer. Similarly to the orange box on the side here. It's a school bus. And we have to treat that differently as well. When we're on the road, other people have expectations. So when a cyclist puts up their arm, it means they're expecting the car, yield to them and make room for them to make a lane change. When a police officer student the roads are vehicles should understand that this means stop when they signal to go and we should continue. Alright, so examples are just things that you wouldn't understand all the red, right? But there are so many of these unique roles and edge cases that self-driving vehicles have to be able to take into account. And people are solving this with neural networks. The questions. What did you do on? An example that I really like to use is the recent progress made in games. I like this because when I was a senior and undergrad over a decade ago, machine-learning luminaries at the time, dr. the game of Go wouldn't be solved in the near future. Of course, we know that there was deep bluish, I say I, that Garry Kasparov in 1997 but goes and much more complicated game. And this is demonstrates harvest, the CEO of mind explaining why that is games or kind of microcosm of the outside world. That's why games were invented, That's why humans find it fun to play. There's a rich history of compete attacking board games. Started with games like backgammon, drafts. And then finally there was deep blue 97, the beak Caspar for chats, watershed moment for game. Since then, the really big remaining Holy Grail, if you like, has been done. Chess number of possible moves to about 20 for the app. And go it's about 200. Another way of doing the complexity of Go is the number of possible configurations of the boss is more than the number of atoms in the universe. But if you ask a great Go player why they fade a particular move? Sometimes they'll just tell you it felt like. So you can, one way to think of it is that Go is a much more intuitive game, whereas chest is much more logic-based, right? Yeah, So because there are so many possible configurations. And further that when you asked a really good Go player why they played a move, it comes down to, it becomes really challenging to think how do I train an AI that can beat the best human Go player in the world? Because what this intuition even mean, how do I create an AI? Just have this intuition. Even with these challenges in 2016. And this is actually right when I was interviewing for my job here at UCLA. During those interviews, there was this game going on where AlphaGo and AI traded by DeepMind was playing all who is widely regarded as one of the best skilled players in the world. Before the day reset all before the game, AlphaGo had already exceeded the European champion. But Lisa Dole was at a level of play much higher than the European champion and he was very confident that he could win. The first game was a massive surprise. When AlphaGo won. Alphago won came to an end. Alphago won game three, at least for me, there was this feeling of excitement when AlphaGo won. But then it slowly changed a bit into a tiny bit of despair, as in humans ever going to be the AI. But recent Dole was able to teach game for. Then ultimately AlphaGo won the fight in sets or 21. And this was something again where she learned luminaries had said earlier that the statement of the salt for a long time and get deep learning was able to solve it. One way that these AIs are trained to play Go was to look at the human expert data and in essence try to determine some of that intuition. So from these examples as to what's the best group of plaintiffs, third position. But here's a video from David Silver who led this DeFi projects at AlphaGo pocketed be talking about how they were even able to later on upbeat the algorithm so that they can make even better go fire that requires no human data at all. Alphago Zero is the strongest Go program in the world, outperformed all previous versions of AlphaGo, specifically defeated the version of AlphaGo, that one against the world champion at least at all. And it beat that version of AlphaGo by 100 games deserve to all previous versions of AlphaGo started by training from human data. And they were told, Well in this position, the human expert play this particular movement in this other position, the human expert played here AlphaGo Zero doesn't use any human data whatsoever. Instead, what it has to do is learn for itself completely from self play. So the reason that playing again itself enables it to do so much better than using strong human data, is that first of all, AlphaGo always has an opponent have just the right level. It starts off extremely naive. It starts off with completely random plane. Sorry, that was my bad. I actually don't be attached the video, but the video wasn't much longer. It starts off with continuing, I can phrase, and then they do something where they trained the eyes and unmatched at the same levels. And just by playing other versions of the AI, it's called self play. They are able to become gonna go. This new algorithm which they call AlphaGo. Alphago recently hold the world champion. 100 days is around. The AI has improved a lot even just by, by algorithms. Any questions bear in mind is working on several other names instead, here is the video of their AI office car to play the game of Starcraft. Here's another application that is hopefully showing how diverse application to my own area of research. So I worked on something called brain machine interfaces. And our goal is to help people who are paralyzed. So this is a picture of Christopher Reeve, who many of you know was the Superman and the actor who played Superman and the original trilogy. And Christopher Reed suffer horses. Horseback riding accident, paralyzed from the neck down. For the rest of this month. For the rest of his life, he required a ventilator support to help with breathing. And we use deep learning to build brain machine interfaces. This is to help people who are paralyzed due to spinal cord injury or disease like ALS communicate with the world once again and move once again. So the basic idea of how this works is for someone who's paralyzed, the natural pathways that take information from a region of their brain called the motor cortex. This part of the brain and goes down the spinal cord to the alarm, these natural pathways are broken. But even though these pathways are broken, intense but thought of I want to move my arm still persist in the motor cortex. What we can do is we can read out that activity by going to the brain directly. In the first example, I'll show you what we did is we did neurosurgery to record from neurons in the motor cortex. And the neurons in the motor cortex communicate via signals called action potentials. And so I'm showing you 96 electrodes here, where every single electrode is getting really close to a neuron and bust into its voltage. And these spikes here are the voltage signals neurons in MIT. And that's the fundamental currency of information. And the brand is how your neurons talk to other neurons and how neurons talk with the rest of your body. So what we do is we read out these electrical neural activities from these electrodes, and then we pass it through what we call a decoder algorithm. Basically, we translate that electrical activity into the intent of what the person actually wanted to do. And what you use that to control across thesis by the robotic arm or a computer. Alright? And this decoder, the best ones out there today are based off of deep learning. Deep learning we'll learn the patterns between the neural activity in the brain and how to best decode that, the movements of some device. So here's an example of how this works. In this video, you'll see a 52-year-old woman ALS. She's paralyzed and she's controlling a keyboard that's already completed computer cursor just by thinking about it. To answer this question, how did you encourage her sons to practice music? I think you've got it. Target to target. And then when she gets over the letter that she wants to select. That select the letters if I might be part of a football at Cambridge University. Another video of the participants where instead of using this custom keyboard that we created, she's now controlling the computer cursor on Android tablet. I see the control isn't perfect. Sometimes you struggled to get to the exact location. And there is definitely more improvement to be made based off the algorithm. So many advances we tilt the performance of these incidents. He's able to use this to surf the web. I want you to write emails. The question is, how do you collect the same data progress? Yeah, that's a really good question because these are trying to be a supervised learning, which means that we need to know the movements she intends to break and collecting their own data corresponds back. So actually what we usually do is we will move a computer cursor on the screen for her. And it will say, imagine moving your arm to follow this computer cursor. So when those perfect goes to the right. So imagine wearing her arm to the right. That's how we get the target signal to decode. That's a great question. Yeah, The question is, Venice seems like you're paralyzed and you want to use a system like this, you have to go through some pretty intense training. So actually for these algorithms, they were able to be trained with just 10 min of data and probably could be more fully. Deep learning has done better algorithms. But if you put in once the firefight, but those will be data that have been collected through normal abuse over the course of months. There's also the other thing that is pretty glaring about the system which is either requires neurosurgery to get these signals. And so, excuse me, try next video which is at UCLA, what my lab is working on is trying to make a non-invasive. And so this is actually just from a month ago showing a system where instead of doing neurosurgery and report neural signals non-invasively through this one. But this is one of our graduate students wearing a cap. And now he's able to control this. And this cursor is entirely driven by burning. There are nine neural networks working together in the air. Have you include supervised learning and involving buses, but because of where it equal to that, some performance is not as good as one way doesn't require neurosurgery. And this student is able to just practice on both of those into a high level. But I haven't talked for my office hours. In neurosurgery. What happens is that the participant is first given fMRI MRI. The MRI that asked to imagine you live in. There are, or there can be areas that we did where we want neural signals and then MRI when they're asked to imagine moving your arm. Areas of the brain that become activated will become known to the nurses and the neurosystem. Although this board then will spread through the non-invasive system. The question is, do they use, are we using the same or lupus as what? The invasive subject? Because easy. The answer is. So actually, in the video that I showed you that 52 year old woman with ALS, I said that she's imagining moving her arm, but she actually isn't. She is to move the cursor up and down. She's imagining her thumb going up and down. The cursor left, right? She's imagining her pointer finger going left and right. In this video, I'm, the graduate student is doing right-hand group is for right that kind of movements for both hands and feet for gas. And so part of the reason why that is is because when we record non-invasive signals, they are, they are all such, such for spatial temporal resolution that they can't resolve various movements in the fingers. And so we have to use whole body movements. The question is, if you train this on one subject, is it generally enough to apply it to another subject? In the intercritical case? No. And that's because it depends highly on which neurons we end up recording. Even for the same subject. If you do two surgeries, the signals from the first surgery will be very different. Music comes on the second side, so they have to be retrained. Whereas for these non-invasive methods, because the signals are so poor spatiotemporal be resolved. They are correlated across subjects, and so these do generalize across subjects to a better extent. Take one more question and then we'll continue on. The question is for action potentials, are they in binary or is it important to maintain the analog signal? But also talk about this when we talk about neural network architectures. But the output of biological neurons is binary. Either a spike, what happened? There actually isn't any information can be in the voltage, can be analog signal. All that matters, but that's just my calendar, not the brain actually uses digital communication. That's one of the reasons why we can have pretty good control of our fish, even though the brain has to send electrical signals all the way down our body, that's a pretty long transmission line. And there's a lot of them aren't all the way. But the brain has developed these digital robust mechanisms to convey that information. Because there are so many questions about this. Let me just plug that. We aren't going to cover those in these cost is the cost of the machine learning class. But I do teach about this next quarter in a class 14038 for undergrads at 02:43 for graduate students where we talked about how the brain generate signals and how we can use that for brain machine interfaces. All right, so moving on, whole bunch shown you that deep learning is really becoming a part of everyday life. And you've interacted with the neural network today that has resulted in key breakthroughs in many areas and in different fields. Many people are looking to deep learning to try to boost performance. One thing I learned earlier this year is that if you've seen those little cocoa robots that deliver food out on the street. The founders of the company where students in this class, six years ago in this class, was something that inspires them to do. So deep learning may be useful to research in your area or future. Any questions here on any of the overview that we've given. Alright, so I'm gonna give a very brief background on bidding that even though my expertise is in the brain and then brainwashing interfaces. In this class, you're primarily going to be studying neural networks in the context of computer vision. And that's because like we saw in the earlier video with Professor, That's really where neural networks, neural network research dates back to 1943. Mcculloch and Pitts in 1958 with Rosenblatt, who here has heard of the perceptron before. You, I couldn't have to try and learn your first neural networks class. That's really the first artificial neuron where there is a node that will sum all of its inputs. But so does the affine transformation. Then after doctrine applies, a thresholding operation so that the output of this artificial neuron is going to be zero or one. That was inspired from biology. But the answer from the prior question, distinctive aspects, biological neurons communicate in an all or nothing binary way. So really in the 1940s and 1950s, researchers to start to think, okay, can we design computing elements that have similar properties to biological neurons? What happens when we looked them up? Alright, so Rosenblatt perceptron, if you just have one neuron, is something that's linear. And to make "
"All right. And last lecture, we had gone over several videos showing the history of these continuous motor BMI's. And now we're going to get into the algorithms. And so there are going to be three algorithms that we cover in class, which are the optimal linear estimator. Here we call it linear vector, the weener filter, and then the common filter, which will be for the next few lectures. And we go over the Matthew hindies in this review paper that we wrote in 2014. All right. Were there any questions from last lecture about anything related to the videos or to any historical BMI performance? All right. So then let's set up the decoding problem and let's get to algorithms. So in the data set that we are going to give you for homework number six, what we'll have is a monkey, monkey J, and he reaches to eight different targets without a planned period this time."
All right. And so if sigma is greater than a minus a then the Laplace transform exists because this term again there goes to zero. And therefore we say that the region of convergence for this Laplace transform occurs when sigma is bigger than minus a. And sigma again is the real part of s. And so in our picture of a 2d plane right if the x axis is the real part of s sigma and the y axis is the imaginary part omega. And a is some positive number. Right. Then minus a is going to be a negative number. And as long as the real part of s is bigger than minus a by e to the entire green region then we're going to be able to compute the Laplace transform. And what you're going to notice here is that this region of convergence contains the j omega axis which is where the Fourier transform is evaluated upon.
"All right. And so we notice then, well, we presented the equation for the Laplace transform, that's this equation here. And we notice that it bears a striking resemblance to the Fourier transform. The only difference is that the Laplace transform instead of an e to the minus j omega t, we have an e to the minus s t, where s here is sigma plus j omega. All right. So now this exponential here in the Laplace transform integral includes not just the j omega term, but also the sigma term. All right. And we said it would be good to conceptualize the sigma and the j omega as in a complex claim where the real part of s is sigma, that's the x axis. And the imaginary part of s is omega and that's the y axis. And what we note is that the Fourier and Laplace transform look very similar except for the Fourier transform."
"All right. Any questions from last lecture in the setup? All right. So broadly, when we look at what spikes mean, they fall into two categories, which are neural encoding and decoding. So neurons, we know that they transmit information by the firing of a sequence of spikes. And it's the pattern at which they fire that encodes information. Remember, it's not the spike shape, but the actual pattern. And these patterns are used to represent all types of information. And so neurons, the visual cortex, they will be involved in coding or encoding. Stimuline the natural world like a light coming into your retina. All right. It can also be audio waves coming into your being processed by your cook. All right. So neural encoding is how we map these external stimuli from the world into a neural response."
"All right. Any questions on course logistics or any administrative related things? All right, so we'll go ahead and get started. So, I just wanted to remind you. Last one last Monday actually before a week ago before Thanksgiving. We had started this lecture on frequency response, where we had talked about low pass high pass and band pass filters which extract out frequencies in particular ranges, and we went on to derive their impulse responses, and some characteristics about them. We talked about this am radio example, and how filters could play a role in helping us to recover the signal signal after demodulation. some practical limitations of these filters. And one thing we mentioned was that if we were to take an ideal low-pass filter whose impulse response is a sinc, well, the sinc is not causal because we know that a causal impulse response must be zero any time when t is less than zero. And so we said, okay, well, to make it causal, we can do something. We can shift the signal in time, so that now the signal is zero prior to zero but then after zero we have a shifted impulse response."
"All right. Any questions on part 1? All right. So if you have questions for number 2, just feel free to unmute yourself and ask them. All right. Let's go on to, let me write that this is 2a. All right. So then 2b says find the Fourier transform, big f of j omega, right? And so this one is fairly straightforward if we've gotten 2a correct. So can someone unmute and tell us how you did, or what was your thought process in doing 2b? I guess let me do the first part, which is that we know that the Fourier transform of delta 2 of t is going to be, well, we know that in general delta big tt is going to be omega not delta omega not omega. And so for delta 2t, omega not is going to be 2pi over 2, which is just pi."
"All right. Any questions on the task setup? All right. So then last lecture then we talked about the system about how what would happen is the monkey touches and holds a target. We show a peripheral target. And if it's a BMI trial or if it's a neural prosthesis or brain computer interface trial, the monkey would plan to this target. And that would correspond to this activity over here, the monkey's plan activity towards this target. We would decode this activity and then draw a circle where we think the target was. And if the circle overlaps the square, then it's correct. And then we will show the second trial, which would be the target a new location. The monkey would plan that's the neural activity over here. And then we would make a decode. And if it's correct, we would draw the circle in over the square. And then we would present another trial. And this is a plan activity again. And then if we decode correctly, then we would draw a circle over the correct target location. And so that's where we ended last time."
"All right. Any questions so far? All right. So this was a figure from a study that essentially motivated the common filter. I kind of mentioned this study in passing last lecture where in this case what these people from the brain gate clinical trial did what these researchers did is they had the subject perform a participant perform a center out in back tasks to four targets and on a single trial if they decoded with a wiener filter these are what the trajectories look like and they look pretty noisy. If you take all the trajectories that go downward and you average them together you get these this black line here for the right word trials would be this blue line and you can see that even averaging across all the trials to a particular target the trajectories even in the memes look pretty noisy. So this is a wiener filter and this paper established that we should work with common filters although again for this study like I mentioned last time their delta t times p the number of things they look back in history was 1,000 milliseconds which actually poses problems for the common filter because it means neural data from one second to go is forming your movement at the current time point and you can imagine if motor commands from a second ago are influencing your reach right now that could actually be difficult to control and that's why in the homework we asked you to set p delta t on the order of 100 to 200 milliseconds which we find these to better performance."
"All right. Can everyone hear me now? All right. I'm sorry about that. I'm not sure what happened with Zoom. If you if anyone again has trouble entering, please just write something over chat. All right. So I'm going to just restart. We lost 10 minutes, but given that we maybe had half of the class not here, it'll be, I did, I think, to restart. So very quickly, homework is going to be released this Friday and then do in a week on Friday, upload to Grayscope by 11.59 p.m. Homworks in general will be due 11.59 p.m. the day that we say they're due. And so when you're uploading to Grayscope, make sure that you have some leeway to submit by 11.59 p.m. The portal will close after two late days following 11.59 p.m."
"All right. Cool. So. This is lecture two for EC 189. I was planning to continue where we left off, which is we were going to start to talk about devices that exist that interface with the brain. Or with the central nervous system in general. Before diving back in to where we were, I wanted to just ask if there were any questions from last lecture that people want to follow up on. All right. So we were last talking about the retinal implant. The idea would be that you would wear glasses. Sorry. Before that in the retinal, in retinal blindness, what happens is that normally there are cells called rods and cones that process light that naturally comes into your eye. And those rods and cones activate a circuit in the back of the eye that ultimately sends signals from so called retinal ganglion cells to the optic nerve. And then the optic nerve carries those electrical signals that were transduced from the white in the circuit and sends that off to the brain. Where then we have our perception of what we what our eye saw. And so in retinal blindness, these circuits at the back of the eye that send signal to your optic nerve no longer work. And so the idea is, although they no longer work, it actually turns out that we have really good knowledge about how this circuit."
"All right. Okay, and Sal. Thank you. I have a question following up on that. So would it also be released on Friday or there should be expected to be released on Friday the discussion questions in the recording? Good question. At T.A. do you have an answer for Sal? So we will try to release it by this Wednesday night so that students have a lot of time over the weekend to look at it."
"All right. So for example two, this is our graph and we have A being the parent of C, which is the parent of B. And so the factorized distribution was P of A because it has no parents, so there's no condition. There's no condition on any variables. Times P of C given A times P of B given C. And then last factor we had run the poll to ask the intuitive answers for these questions which will not show rigorously. But we asked two questions. First is A intuitively independent of B, which means if they're independent knowing A does not give you any more information about B than you already knew. And so when no variables are observed, in particular C is not observed, A and B are not independent. And that's because if I know A, I know something about C. And if I know something about C, then I know something about B."
"All right. So for this question. Regarding the setup, it should start off similarly to the other questions. And so for example, the derivation of pi k should be exactly the same. You just look at that pi k is the proportion of trials. The difference will come when we now. So just going up from the start to this line, this is writing the total data likelihood. So we're going over all of our classes with OK. And then for each class, where iterating over the trials, J in that class. And then we had the log probability of the neural data and the class. And so I feel free to anyone stop me if any of this is unclear. So this is our data log likelihood data log likelihood. And then going from here to here, I think all of you have done this already. I just use Bayes rule to write its p of ck plus p of y given ck. And this is log of pi k again, the same derivation has in the prior parts."
"All right. So I have a question about part F and part G of 12 and 2. Okay. I have a question in general about some simplifications that I'm not sure I can do. Okay. So in part F where we're trying to see if B is independent of C, I'm able to isolate out C. So like from the probability of B and C, I say, I do the joint distribution, summing over all of A and D. And then once I go through all of that, I can take C out. And A given C just factors out as probability of, as one because it's law of total probability. But my question is I get like a summation over A, a summation over D of the probability of B given A, D. And so if we're summing over everything that's given, can that become probability of just B? So in general now, although we have extra terms in there, but a P of B given A, a, D summed over A, D in general only equals P of B when it's P of B, A, D. And so, in general P of B given A, D summed over A, D is not equal to P of B. But then, sorry, let me get my idea. I should have done this before, once I get it. So let me just connect my idea so we do have, and then make sure that everything is consistent."
"All right. So if I were to open up some Na plus channels, the resting potential, which is normally negative 65 millivolts, if I open up these ion channels because sodium flows into the cell, now that voltage is going to increase slightly. And so that voltage is going to go up a bit. And we talked about how, and again, we're going to talk about this more in today's lecture, these ion channels are also special in that if the voltage increases, they open. And these are particularly these Na plus channels. And so if Na plus flows in, it increases the voltage, Vm, and that causes other channels, which are going to open based off of the voltage increasing to also open, if these channels open, more Na plus is going to flow into the cell and the voltage is going to go higher opening up more of these channels. And this initiates this positive feedback loop that causes the voltage to skyrocket because all of this Na plus is flowing into the cell. And that is the positive rise of the action potential."
"All right. So just a recap last lecture, we talked about passive properties of neurons and how they have membrane resistance. Those are the ion channels that go from that take ions inside to outside the cell or vice versa. They have a membrane capacitance because the cell wall has positive charges and negative charges separated across two plates, which reminds us of a capacitor. And we talked about how these sets the dynamics over which the membrane voltage can change. And then we also had talked about how propagation happens or the types of parameters were interested in terms of neuron propagation, how there's this internal axial resistance and that along with CM, first the axial resistance and RM dictate how far a charge can propagate and then RAMCM detect RAMCM may affect action potential speed. And so we had also discussed the slide here on action potential velocity and how the action potential velocity is a function of parameters of the cell, including the cells by ammeter as well as the thickness of the cell wall or the cell wall plus myelination."
"All right. So last, we were in last lecture. We ended talking about the syllabus, which we didn't complete, but we'll complete right now. So I had mentioned that homework is worth 50% of the grade. And it is a very high weight for this class. That reflects how important we believe the homeworks are. As we'll discuss later on, we designed the homeworks to be fairly difficult, but the exams to be no more difficult than the homework. We also made the homeworks work a lot to decrease the percentage that the midterm and final exam assessments are worth to try to lower the stakes on those exams. And to discourage any academic dishonesty. And so I wanted to then talk about academic integrity, which is very important to me and to the teaching staff of this class. And so this slide reminds you all of the true brewing UCLA academic integrity principles. I take academic integrity very seriously because it's important that we are all on a fair and level playing field. When you cheat, it's not only the service to yourself, because it's not a reflection of the material you've learned, but it's also unfair to your fellow peers who may have worked very hard to study for the exam. Another thing that we'll do in the class and I'll discuss shortly is that this class is rated on an absolute scale not on a curve. So you aren't compared to your fellow peers. In any case, for this slide, my point is showing this is to tell you that I take academic integrity very seriously."
"All right. So neural encoding. So in neural encoding, what we want to do is you want to go from an external stimulus to a neural response. So you see, it can be a neural response. And the first thing that we're going to talk about is some of the difficulties in modeling this, which leads to some techniques we see later on. So again, when we do neural encoding, what we want to do is we want to record from a neuron and understand how it represents some external stimulus. And so let's say that the stimulus is audio. And so let's say that you're listening to an orchestra. And so the stimulus would be, for example, let's say you're at the very beginning of the concert and you hear the obo playing obo or violin playing the concert A, which is the 140 Hertz audio wave. All right."
"All right. So now we're recording. So I was just responding to Jerry's question, which said what before we start the project. I don't recall the exact week off the top of my head, but it's around Thanksgiving and it's listed on the syllabus. So the syllabus date will be the date that we release the project. And it'll be equivalent to about two ECU 102 homeworks. All right. Okay. So are there any questions that people had about this example brain machine interface system from last lecture since I know we ended while we were talking about it? All right. So we'll go then to some other examples building off of this idea. And so if you can control a computer cursor on the screen, you can bet you can also control a robotic arm. So I'm going to show you a video here. What's happening is just like for a computer cursor, what you're doing is you're controlling the 2D position of a cursor on the screen."
"All right. So then last lecture, I was showing you what raw data looks like coming off from one of these electrodes. We see this slow, unjuating voltage. But then on top of these voltages, we see these big spikes. And these spikes correspond to the neurons that correspond to spikes coming from neurons that we're recording from. And we were unpacking this a little bit. We saw that in this way they form. There are actually spikes of two amplitude. So there are these red spikes here, which have a really high amplitude. And there are these medium amplitude spikes here, showing green. If we just plot them on top of each other, we'll see that they do have different amplitudes. And last lecture, I asked, what does this mean? And you all correctly responded that it means that we're listening to two neurons. So we might have a green neuron and then a red neuron. And what happened is our electrode landed over here. And so even though it's close enough to fear from both neurons, because it's closer to the red neuron, it's action potential, it's spiking pit to pit voltage is going to be higher than for the green neuron. And in this way, one electrode can actually give you measurements from multiple neurons at the same time. I so want to pause here and ask if there are any questions about this. So the way that we usually, what we call this is spikes sorting. And so what we'll usually do is we'll record from these electrodes. And if we see spikes at different amplitudes, we'll talk them about each other and then isolate them into clusters. So here, we'll have a red and a green cluster."
"All right. So whenever we see a result like this, it's also good to make sure things make sense. So the first thing that we could check are the units, right? So remember lambda was a rate with units of spikes per second, right? And so we would expect the expected time in between spikes have units of seconds, right? And so one over lambda will have units of seconds per spike, but it'll be the massive time in between spikes. All right, so the units make sense."
"All right. So with that other course information throughout this class, we're going to cover a wide range of topics, including some intrusion neuroscience, which we'll do for these first two and a half weeks. And then after that, we're going to cover topics in modeling spikes. That's going to be drawn from this theoretical neuroscience textbook, as well as topics from machine learning and statistical signal processing, which we take from this Chris Bishop textbook. And because we didn't want to have students need to to purchase all three books, what we did was we took the excerpts of the chapters that we used for these books and we put those on CCLE. Right. So that material should all be on CCLE. Other notes for this class. So the last two notes that we use in class, I just asked that they not be publicly posted due to matters related to copyright. And so we'll be happy to distribute them in the annotated notes on CCLE. But we just asked that they remain within the class population. Like we said, last time a piazza should be used for almost all major class discussions. And of course, these office hours to get any other questions answered."
"All right. So with that other course information throughout this class, we're going to cover a wide range of topics, including some intrusion neuroscience, which we'll do for these first two and a half weeks. And then after that, we're going to cover topics in modeling spikes. That's going to be drawn from this theoretical neuroscience textbook, as well as topics from machine learning and statistical signal processing, which we take from this Chris Bishop textbook. And because we didn't want to have students need to to purchase all three books, what we did was we took the excerpts of the chapters that we used for these books and we put those on CCLE. Right. So that material should all be on CCLE. Other notes for this class. So the last two notes that we use in class, I just asked that they not be publicly posted due to matters related to copyright. And so we'll be happy to distribute them in the annotated notes on CCLE. But we just asked that they remain within the class population. Like we said, last time a piazza should be used for almost all major class discussions. And of course, these office hours to get any other questions answered."
"All right. So with that other course information throughout this class, we're going to cover a wide range of topics, including some intrusion neuroscience, which we'll do for these first two and a half weeks. And then after that, we're going to cover topics in modeling spikes. That's going to be drawn from this theoretical neuroscience textbook, as well as topics from machine learning and statistical signal processing, which we take from this Chris Bishop textbook. And because we didn't want to have students need to to purchase all three books, what we did was we took the excerpts of the chapters that we used for these books and we put those on CCLE. Right. So that material should all be on CCLE. Other notes for this class. So the last two notes that we use in class, I just asked that they not be publicly posted due to matters related to copyright. And so we'll be happy to distribute them in the annotated notes on CCLE. But we just asked that they remain within the class population. Like we said, last time a piazza should be used for almost all major class discussions. And of course, these office hours to get any other questions answered."
"All right. So, to show that we have a Poisson process starting at time S, and we have a Poisson process starting at time S, and looking forward, right, all we have to show is that all of the events that happen after time S are IID exponentially distributed, interspinently, happen with IID exponentially distributed interspinally intervals. And we're mostly good because between T4 and T5, we know that this is exponential lambda, between T5 and T6, we know that it's going to have an exponential inter-arrival time. Actually what we don't know is if this first interval over here is this exponential, is this ISI between time S when I restarted my Poisson process, and the next spike event is this ISI exponential with parameter lambda. Because if it is, then every single ISI looking forward is exponential parameter lambda, and that defines a Poisson process. Right? So at the end of last lecture, we gave the intuition for why this is true, which has to do with the memoryless property of the exponential distribution."
"All right. We can do the race hand system. And I see first the question in the chat. So Caleb asks on the Wiener filter slide. So sorry, let me connect my iPad again. So I can get the Wiener filter slide up. All right. So Caleb's question on the slide. How about getting from the Wiener filter to the correct form? I think that blows probably this slide. Can you confirm that the top yellow example is correct? It seems like the last term should be L5 times Y0. If you follow the pattern, L3 times Y2 L4 times Y1, you are correct, Caleb. So let's see."
"All right. We have quite a few people here so I'm happy to do a race and system where if you raise your hand then I can call on each of you and Brandon you can go first, and you should be able to meet yourselves. Yeah, so, Professor cow I was just wondering, um, what's the monkeys name in the video. That monkey was named George. Yeah. And then we'll show you later on when I show you my experimental setup, which was the rig that I had built actually in that same area, but it's a totally modernized for 2D cursor control. You'll see a video later of a monkey called Jenkins. And so usually in the papers, we just write monkey G or monkey J. But yeah, each of the monkeys have had their own unique personalities and whatnot."
All right. We now we know that when we move our movements are smooth and our arm has inertia. So we aren't making a jittery movements like we saw for the optimal linear estimator. All right. So the idea behind the common filter is can we incorporate additional prior knowledge we have like the fact that our movements are not jittery in real life or typically not jittery. Can we incorporate that into the decoder? All right. And so the premise of the common filter is to ask what if we have additional information in this case that our movements are smooth. Right. So here in lecture we're going to we're going to primarily talk about a velocity common filter which incorporates the smoothness of movements for a computer cursor on the screen. But maybe you're controlling a robotic arm and you know something about the dynamics of how that robotic arm moves.
"All right. Well, let's say that I did want to shift the cosine. So let's say I wanted to shift this cosine. So it looked like the following. All right. And so this cosine now has its peak occurring at time t equals 1. All right. So I'm going to ask if I want to take this blue signal here and shift it by an amount of theta. So now it's time shifted delayed so that it's peak instead of happening at zero, happens at time one. For what value of theta ought that be? And so take 10 to 15 seconds to think about that. And I'll ask someone to write the answer over chat. All right. So I see several answers in the chat. I see pi. I see pi over 2. I see 1. All right. So the correct answer is pi. And let me explain why that's the case. So the first answer that you might have thought of is that the answer is 1. Because what I'm going to do is I'm going to shift this blue signal over by time one. So if I call this blue signal here x of t and I call this green signal y of t, then your first attempt may be to say, okay, y of t is going to be equal to x of t, which is 4 times cosine of 2 times pi times the frequency, which is 1 half t. And then because it's shifted over by 1, then I should write a minus 1 here. All right. We put a question mark there. Let me simplify this a bit. This is equal to 4 cosine of pi times t minus 1. All right. So you can do a few sanity checks to see if this is correct or not."
"Alright, let's get started. You only have one announcement before we begin. And that is that homework number three is due a week from today. And please be sure to print out your Jupyter notebooks as well as your code for that assignment. All right, any questions on NH4 statistics? Okay. I went through this tensor derivative last factor and it was a bit fast at the end and we didn't get a chance to ask too many questions. So I wanted to come back and recap this and I'm happy to take any questions on this tensor derivatives. So we were in this study where we have y equals wx, That's right here. Here, y and x are both vectors and w as a matrix. So if we have d y d w, d y d w is a derivative of a vector with respect to a matrix. And we talked about how you can think of this as a 3D block. So what we can do is for each element of y, we can take the derivative of that scalar element with respect to a matrix, which we know is a matrix of the same shape as W. And that's this red matrix here will, would therefore be the derivative of the first stellar element of y with respect to the matrix. Now we can do this for every single scalar element of y and that all these matrices together we get a 3D block. And that is what the gradients dw will look like. It's gonna be a 3D tensor that looks like this block right here. Alright, Were there any questions about that? Russia. Music, pop music. Oh, I see. Okay. I see what you're saying about it. Yeah, so we talked about how in denominator we add the dimensions are m by the thing in the denominator. The W are the leading dimensions of the resulting tensor. So it's going to be n by n, the dimension of w, by m, The dimensions of y. And so if we're thinking about this block is having the height in the first dimension, the second dimension, and the depth d The third, then you could rotate this 90 degrees if that helps you to visualize what that cancer is. Thank you. Other questions? Okay. So then after that, we did this problem where we have z equals y minus WX. And we ultimately wanted to first take the, sorry, I wanted to first take the gradient of z with respect to w. And so we did the operations for that. Following the exact same logic over here. What we did is we broke down into its scalar components, ZK, and we wrote what each of those dk are. And then we differentiated those ZK with respect to w. And that gives me one matrix. Alright? And then I stack all of these matrices for k equals one to the dimension of Z, which is N. And that will give me a 3D tensor corresponding to the gradient of z with respect to w. So this was the derived matrix, which is the gradient of a scalar element of Z, Z k with respect to w. And we saw that it was a matrix that's everywhere zeroes except for the kth row by x minus x. And what we then saw is that by just changing Z1, Z2, Z3, all the way up to Z, right? If I just stop all of those and I have the 3D gradients, but 3D tensor gradient d z, d w. Were there any questions from this slide? Alright, so here's the last part of it. In our question that you're trying to solve for, we had a scalar epsilon. Epsilon, I believe was equal to something like one-half Z transpose Z. And I want to compute d Epsilon dW, where then I would use my chain rule, which we derived last lecture goes from right to left. So I would take d Epsilon DC and then multiplied by d z d w. This is going to be that 3D tensor that we just derived. Times a vector. And we saw that the operation of this 3D tensor times the vector will simplify to a matrix. And if we went ahead and wrote out this operation, we said, we see that this thing here, d z d w times d epsilon dc, mathematically simplifies to d Epsilon DZ. That's this term right here. Then the effect of multiplying on the left by this 3D tensor d z d w. Simplified to take the d epsilon d z and actually write multiplying it by this one detector. So the Chain Rule runs right to left. It's a, formally to compute this, we have a 3D tensor times a 1D vector, but when we simplify all the math, it turns into a 1D vector times another 1D vector transpose. So this is the result that we get from the Chain Rule by following these steps precariously. Alright? Are there any questions over here? The question is, is the same as well? Yes. So in this example, if you look at the slide from last lecture, the loss of epsilon was just one term of the loss for one example. But the loss is the sum of all of these epsilons. Other questions. Okay? So I also talked about how the last lecture, the 3D tensor that we went through is really to show you all that everything works rigorously and nothing is magical. But really there is a much simpler way to arrive at this answer. Kind of just using intuitions about what gradients to reflect and also naturally dimensions. So I'm going to redo this because this is quite important and it'll be the way that you usually end up doing these ingredients for backprop your assignments. So we had derived this result earlier last lecture, where if I take w x, which is a vector, the product of these is a vector. I differentiate with respect to x, I get this matrix W transpose. Alright? So even though I know that this gradient of a vector with respect to a matrix is a 3D tensor. If I kinda follow the rules of how when you differentiate with respect to a variable, you get the other variable, right? So if I differentiate w x with respect to w, actually get something that looks like x transpose. We can say that the gradient of w with respect to w looked like x transpose. Because it's not rigorous because this gradient is a 3D tensor. X transpose is a row vector, but it should look like that because of this trend that we see in the gradients. And so what people usually do is they say, Okay, if I know d Epsilon dw is an m by n matrix, I know that the Epsilon DW, right from chain rule is going to look like a swan. Dz times a DZ detail for you. What I can say is I know that this dw is going to end up being the epsilon d z times the gradient dv, dw look like. So this d epsilon d is the I copy down here and that's an n by one matrix. Sorry, I meant by one vector. And then I say this gradient of w x with respect to W. So let me change all of these years to i w x's bit more clear. Wx, wx, this gradient dw, dw soup look like x transpose. And so if I fiddle around with dimensions, X transpose is one by n. If I take this m by one and multiply by one by n, I get an n by n. Alright? And by doing that dimensional matching, I can come up with this as an argument for what this gradient should be. It's not rigorous, but you can use this in the future. We have already shown it is rigorously true by doing this 3D tensor. But now moving on to the future, you can just use this results and just try to match the dimensions to get your gradients. Alright? Again, this is not rigorous, but it's a really handy trick moving forward. Any questions here? Yes. We verify this by using the numerical gradient. Yes. So in homework number three, we will have to implement backpropagation and there'll be a numerical other questions. The question is, does this gradient here denominator layout? It is. And I think I know where the clustering comes from because in denominator layout, we need to multiply. We need to go from right to left. But here in the final answer looks like we're going from right, from left to right, right. So how is this denominator layout? It's because this expression is a simplification of having done correct denominator layout where DC dw is indeed on the left. But this is a 3D tensor. And when you do this, calculation out, simplifies to multiplying by X transpose on the right. So we did things rigorously denominator layout with the chain rule going right to left. It's just that when you happen to simplify it, it ends up as the same effect of multiplying X transpose on the right. That's really important to remark. Any other questions here? Great homework. Question is, does this simplification only hold for 3D sensors or does it go for 4D tensor or higher dimensional vectors? And it does hold for higher detectors. So given that, I'm going to go ahead and the next slide and just summarize everything that we need to do back propagation for all of your homework questions as well as for the coding. Alright? So the first thing is you will have a neural network layer. Neural network layer. We're going to multiply a matrix W and some input x. And that's going to give me some vector hidden activations. I'm just going to call this y for simplicity, since we've been using Y, X, and W and these examples. So we're going to have y equals w times x. And in backpropagation, I'm going to have some upstream gradient, DLD. Why? This is gonna be a vector derivative of a scalar loss with respect to the vector y. And I want to know how to backpropagate to both w and tax. So here are the rules. If I want to backpropagate to do to x first, I want to compute dy dx. Then we derived this is just gonna be del dx equals DLD y on the right. And then there's going to be a DY DX over here, which we derived already is w transpose. So backpropagating to D L dx will be w transpose y. The other more challenging one that requires this tensor derivative. But from here on out, you can just solve by saying the gradient should look like a transpose and then match dimensions. We would have that DL DW, He's going to be DLD Y times X transpose. All right? And again, you can kinda see the symmetry and b's when we backpropagate through this multiplication, right? We're multiplying by the thing on the other wire transpose. So backpropagating to DLD x, I multiply it by the thing on the other wire, which is w transpose. To get some DL DW, I multiply DLD, why my upstream gradient by what's on the other wire transfers, right? And this is the thing that we've rigorously derived. Capture any questions there. Okay, so then we're not going to show the following in class because it would involve doing 40 tensors. But this pattern that we see the generalizes ever want to use that dimensional matching tricks that I talked about calcium back there. The question is, why is X transpose on the right? It's because when you go ahead and you do the gradient using the chain rule, when a 3D tensor is on the left and you simplify through all the math, it ends up being multiplication by x transpose on the right. So we did do the chain rule correctly, where this 3D tensor is on the left. It's just that when you mathematically simplify everything, as it turns out to multiplying by a row vector on the right question. And that's something again where I encourage you to just tilting to you. Please just go back and watch the lecture or watch the segment on this 3D tensor derivative. And I'm also happy to take any questions here. The question is, why is this w t, w transpose on the left? This is chain rule. So the chain rule is ideal dx equals DLD y times DY DX and DY DX, we derived last factor is equal to w transpose. So this is just straightforward chain rule where this simplifies to w transpose. This is also the chain rule, but it's sympathizer, right? Multiplying by X transpose. With this, you can back propagate through a neural network layer. I was just next thing also, we're going to write down the backprop rules for if you have a maintenance funds, a matrix. So let's say that we're in the setting where we have a big matrix, Y equals W times a big matrix x. And here I'm going to say w is, sorry, I'm going to say x is n by p, w is n by m, and then y is therefore n by p. So if you tried to take the 4D tensor derivative of a matrix Y with respect to the matrix W. That gets carried very, but the same kind of intuition that I mentioned before, it happens in that case. And following these rules can be used here and that's all you'll be responsible for for this class. We're just going to write out what these gradients are. If you want to backpropagate to D L, dx, and you have some upstream gradient DLD. Why? Backpropagating through this will actually be the same one as this one. It's going to be multiplying by W transpose on the left. And then if you want to backpropagate DL to get DL DW, you would take your upstream gradient DLD y, sorry, I shouldn't have renewed far away. You're gonna take your upstream gradient d L, d y and multiply it on the right by X transpose. And so you can see these two look eerily similar to this, where instead of x and y vectors here, they had some big wire matrices. And dimensional way everything works out. Any questions there? Alright, so that's all you need for homework number three, these backprop rules should allow you to solve all the three nm paper cautions that we gave you for solving backprop. And then these are also the Bangkok rules for neural networks. So I'm also just going to take this opportunity to buy what backpropagation neural network there looks like. So let's say that this is our neural layer. The neural network layer we know comprises first a linear operation, w times the activations are the input, right? That layer, I'm going to call this thing H1. So it's clear that this could have been e.g. the output of a neural network layer. We add a bias to it. And then after that we pass them through a non-linear activation, Wally. And that gives us, we'll call this thing actually, I'm here NH2. So this is your next layer. And eventually this thing just gonna go to a softmax classifier. And that's going to give you some loss. Okay? So in homework number three, you're going to write the backpropagation for the neural network layer. It's going to start off by getting some gradients with respect to your softmax. And so this is exactly what you implemented in homework number two, a softmax, loss and gradient. So this is going to give you the derivative of the loss with respect to your Softmax parameters. And then we're going to assume that we backpropagated this. So we have some upstream, upstream gradient, DL, DHA. And now what I wanna do is I want to back propagate DLD A12 through all these operations so that I can get the gradients with respect to weights and oxygen bears and neck and depleted. So the first thing that we're going to do is we're going to backpropagate DLD CH2 over here to the LDA. So if I compute the LDA, that is going to equal my upstream gradient, DL dA2 times but local gradients of the ReLu operation. Remember the operation is a map of my input a when the zeros. And so know that the maximum function routes the gradient. So whichever item is bigger. And so what I can do is I can represent this routing via indicator that a is bigger than zero. So this is going to be here are the same size as a, where a is bigger than zero, that element, sorry, that one is bigger than zero, then the first element is one. If A2 is less than zero, the second element is zero. So the scar to be the same size vector. And if a is bigger than that element of a is bigger than zero, then the gradient should pass through. So I'm going to just multiply these element-wise. Alright, so I do that with this notation here. This is called the Hadamard product. So DLT H2 is an n dimensional vector. Then this indicator is also going to be an m-dimensional vector. This Hadamard product just means take the first entry of this vector and the first entry of this vector and just multiply them together. Alright? And that's going to give me the LDA. So that's backpropagating through the reboot. Any questions about that? Great, Yeah, because every element of this indicator vector will have, will be either one or zero. It's not gonna be that all of them are ones are oligomers. Yeah, So in general, it'll depend on the value of the a in that element. And as long as some are positive and some are negative, you'll have ones and zeros in this factor. And basically what this is saying is that the LTA is going to be equal to DLT H2 whenever a was bigger than zero, right? Right. The question is, where does the chain rule come in here? So in this case, we didn't have to use the chain rule. This is because let's say that let me actually just write this out. There'll be better. Let's say I had my vector a and I just have three artificial neuron. So if I say the values for like 52, negative one, and then three right here then goes through a ReLu. So after it goes through a ReLu, which is comparing these to zero, I would get 5203 because the ReLu is applied element-wise to each of these components, right? And this ReLu was doing max of zero. And whatever element in a row. When we backpropagate through this, we have from last lecture this gradient where if we're backpropagating through a maximum, this gradient d LDF is just going to go to whichever wire with. So in this case, for the first element of a 52 is bigger than zero. So the gradient DLD H2 in that first hour in the first dimension would drop back to the first element of the LDA. But the second element of a was negative. And so the gradient here would be zero for the second element because, because it was not bigger than Sir, our intuition is if I take negative one and I would go a little bit, not much change my output at all because my optimal voice stays values and you can write, Tom way is giving a better answer to your question, which is that the chain rule does apply our next sorry, I should have said that also worked fairly. The chain rule applies because that's how we ever backpropagate through anything. I was doing it element-wise, but tomboy was saying and other ways that you can view this is the chain rule applies. But TH2 da is a matrix that is diagonal and the diagonal elements are one or zero based off of that element, if a is bigger than zero. For all of these, if you're having trouble following this, don't worry about it, but there's something where again, just try it out on paper, looking at each dimension individually and hopefully everything will make sense. The question is in lecture and discussion, Yes. You did find this indicator function as returning all zeros or all ones. So I'm not sure what that okay. Yeah, The TSA, they didn't define it that way. Let me tell you how the indicators used here. So let's say that. Let's say that this is our vector a here. If I do indicator of a greater than zero, this equals checking each element and comparing it to the first element is greater than zero. Okay? So the TAs are saying that in discussion, the argument to the indicator with a scalar, in this case, our a is a factor. So when I say Indicator a greater than zero, what I'm saying is we're going to look at all the elements of a 52 -1.3, compared them to zero. And for each one we're going to say whether it's true or false. So 52 is greater than zero, that's one minus one, not zero. That's alright. Any other questions on this? Okay, let's continue on to do the backprop. So now I'm going to back up to my biases as well as the squire. So this is just backpropagating through a plus sign. So if I backpropagate through a plus sign, the gradient just passes through. So we can say that DHL DB gradient of my boss with respect to my biases also equals DL DC. That's the gradient at this wire for C corresponds to the value of this wire. And this is just equal to the LDA, where D LDA is this quantity. Alright? Back propagated to this plus sign. The last step is to backpropagate through this multiplication, which we now know the answer to, because we did both of them. So if I back propagate to the LDH one, then Dio di H1, we know is going to equal w transpose times d L d, c. Alright? And the LDC put the LDA, which is equal to this point. I can plug everything. And then similarly we will have to backpropagate to here. And so DL, DW, we know, will equal LDC, the upstream gradient. And then it becomes a, right multiplied by x transposed by H1 transpose. So that's backpropagation neural network. With this, you can compute the gradients of the loss with respect to all the weight matrices in your neural network. And if you have all those gradients and you can optimize via gradient descent. Okay? Any questions here? Customers, is there an easy way to remember what does all that work is on the right? So I'm gonna give an answer, which is, I don't remember. The reason is, oftentimes, when you look at code, it depends on how big matrix of examples. Exclusivity is N by the future, because this is what we do in the homework. But sometimes it's features by n. So in the end, all I remember is that if I go back to hidden activations, I'll have to multiply by something like w. If I'm going back to DL DW often multiply by the hidden activations. And that's where then I put a break point and my kid, I do x dot size or gradient dot size. Choose the transpose of the orders so that the dimensionalities correct. Alright, that's how, that's how it's being done. And of course for the gradient checking to make sure that that's why I mentioned in that strip is a very frequently that said on the exam, if they asked you to do a backpropagation question, you will expect you to get these dimensions correct. So make sure that for the exam room will give you a cheat sheet. So you want to write down how to backpropagate through these matrix vector or a matrix matrix multiplies. Or you can also take the dimensionalities of things. Okay? Other questions. Okay. Yeah, so sometimes they're giving some tips to remember things. I won't repeat that one because I guess like students may think of different tricks for how they remember things. So yes, this is the answer. And one way again to always check it is to match or dimensions. Other questions, Yeah. Alright. Can you raise your hand if you feel that you generally understand how to back off through the neural network. Alright, I see like maybe that was 75% of the class. I want to ask if there are any other questions on the back propagation steps. I'm happy to answer them. Yes. So the question is, what are the dimensions of DLD X1? So let's say that in this example because I didn't give dimensions, we'll see, we'll see H1, H2, m-dimensional. We'll call H1 n-dimensional. And then if H2 is m-dimensional, that means that w has to be N by N D 0 T H one will be m-dimensional. Tld top view will be n by n. The LDA will be, I'm sorry. Each one is n-dimensional. So this is the LDA will be m-dimensional. Db will also be n dimensional. You can go ahead and verify all of this and introspect yourself well, so, all right. Any other questions on this path problem? Alright, cool. This is something where, you know, I also had an electrode like this and it's for me solidifies when you actually put it up in the homework or do a trick question. So please come to office hours if you're still struggling with anything there. This is our last slide on that truck. So now that the gradients DL DW, that we can now compute the neural network. We can go ahead and apply our learning algorithm or gradient descent. And gradient descent will tell us how to update the weights W to make our loss, cross-entropy loss or smallest possible. On homework number three, you're going to find out that when we do this naive Lee, their performance is still not going to be great. That's because in addition to just the loss function, how to calculate the gradients, which is backprop. And then gradient descent, which is our naive learning algorithm. We're gonna find out that actually there are a lot of specific trips for neural networks to get them to train well. So this is going to incorporate several regularizations as well as initializations and then also use optimization techniques. And that'll be the topic of the next two to three lectures. We're gonna talk about some of these specific tricks for how to train neural networks file. So in this lecture, we will probably get today through two things. One is, how do I initialize the weights in a neural network file? And we're going to find that, that absolute intensity difference. And then the second thing we're going to talk about is something called batch normalization in this lecture. Then next lecture we're going to talk about other types of regularization is including something called stomped out that you may have heard about. And then the lecture after that, we're going to talk about optimizers like using momentum, RMS prop, and Adam, who here has heard of being Adam optimizer despite the appearance. Yeah, so several of you may know the Adam optimizer and know that it's much better than just vanilla. Gradient descent will go over the details of why that actually helps to train these neural networks. Right? So regularizations and specific train for neural networks. These are the chapters to look at in the Goodfellow textbook. We're going to start off by talking about initializations, which is something that we may not usually give much thought to. This is the initialization of the weight matrices. W1 is worth the price is P1, W2, and W3, V3. So the first thing I want to get in your head is that initializations matter. And what that will do for this is to say, maybe you might start off and training neural network by just be something surreal but something close to zero. And I say, okay, maybe the neural network will learn to like bigger. So what I've shown here is Python code, or a ten layer neural network. And each layer will have ten layers. Each layer is going to have 100 hidden units. Alright? And then I'm going to write a for-loop over my ten layers. I'm going to do something simple. I'm just going to write the neural network, which is going to be my waist times my hidden activations. And then after that it's going to pass through a ReLu. But when I first do my neural network initialization, I'm going to initialize each weight matrix so that its elements WIJ in a weight matrix W come from a normal distribution with zero mean and a variance equal to 0.01. So this is a really small distribution, or the distribution that will give you a relatively small values. All the values will be around, centered around zero and their variances is gonna be zero. What happens if we go ahead and do the neural network and run it forward? So just the first iteration of a forward pass with these small weights. What I'm going to do is I'm going to plot the following. I'm going to look at every single layer. So you remember that there are ten layers. So this is, Hey everyone. This is layer two. All the way to layer ten. Within each layer, there are 100 artificial neurons. I'm going to do is I'm going to take the mean of all of these artificial neurons. So if I take all 100 neurons in layer one and I take their mean, I get 0.04. And then I'm also going to take the standard deviation. So this plot on the left is the mean in each layer, and on the right is the standard deviation in each layer. So what you'll see is that as I go to layer two and I get a smaller number, and layer three. And beyond it eventually approaches zero. This is not that surprising because the weights are small numbers. It's a small number of times activations to bake you smaller and smaller, and that's why you go torques around. The standard deviation also goes towards zero. And what that means is, if we were to show you the actual distribution of the unit activations in each layer. So this is layer one. Layer two. The x-axis is the value of the activation, the value of artificial neurons. And the y-axis just counts. And so this is giving a distribution, a histogram of my activations. We'll see in layer one several or non-zero. But as you go deeper and deeper layers, all of the activations are just zeros with neural networks. Alright? This initialization leading to later layers having several activations. That was there a question? The question is what is on the y-axis? The y-axis will be counts. So the more, the better way to have written this would be to divide by the total number on the y-axis. And it will look like a distribution. But basically, if the y-axis is larger, it means more units had that value. So this is run across many different epoxy. So after how many different examples? So after having run our examples, I look at units that had a value between zero and let's call it 0.1. There were 67,000 of them. There are 50,000 with a value between 01100. Again. But just to show that in, as you go to a deeper and deeper layer, you have to pay six becomes there. I'm going to tell you, it's really bad for learning if the output activations or posters, or you might think, this is not our problem because when I do learning, the learning is going to make those weights bigger and bigger. But intelligence and someone tell me why? Yeah. Because it kinda just go so heart rate. So once it says kinda like the sigmoid will be in areas where the gradient is equal to zero. So there won't be any learning in the first place. That is correct at the high level. And then I want a bit more detail on why the gradients are zero in this case. Yeah. Perfect. Yeah, so let me just write what the students said. We know that. Let's say that we're at the output layer and we're computing our scores Z. So the Z is going to be our last layer, weights w ten times the activations from the pirate layer H9. Alright, so this is the computational graph. We're going to have a weight w ten inactivation H9. I'm just gonna ignore the biases for now and it gives me some z. All right? And then we may have an upstream gradients DL, DZ. And now I want to do WE tag. So I'm going to backpropagate to DL, DW ten. And we know that when I back propagate, the rule is I had the LDC times what's on the other wire, H9 transfers. Here's the problem, which is that H9 are all zeros. So if H9 are all zeros, DL DW ten is all zeros, nobody is going to happen for DL DW, for wait time for w. And so by virtue of having all of our activations equal to zero, we're actually not going to have any learning occurred in the number. The same applies if the same applies for all the other weights earlier that purpose. Okay. Any questions here? The question is, if we have a non-zero bias, does that mitigate this? It does not. So if you recall, for the bias, what happens is that the upstream gradient will just pass through a plus sign. And so if you look at the equation for DL DW, nothing about the bias appears. It's just an upstream gradient times the oh, I see Gesso, I'm saying. But then in this case, if there was a bias here, like a benign, than H1 will not be equal to zero. That is correct. So in that case, there could be some gradients on the P1s are the biases are also crystals. They're out if you initialize emphasis around, they'll also be yes. So, yeah, I'm always asking when I initialized to zero, are they all positive or are they Gaussian? And some are negative and they're Gaussian and some are negative in this example. Alright, so it's bad if the weights are initialized to zero. And here I've just written the code to do the backpropagation is look at the gradients. And what you'll find is that if I plot the distribution of the gradients look like. This is what they look like. And the key thing is that all of these are numbers times one minus seven. So all of these gradients are eclipsed is thereof that the gradients are close to zero than the goals aren't in. All right? So small gradients aren't the answer. The other thing is if we consider a very large weight initialization. So if you had done WIJ comes from a normal distribution with mean zero and variance one. If we were to go ahead and look at these networks using the same thoughts and said Here I'm showing the mean of the artificial neurons in each layer going from layer one to layer ten, as well as their standard deviations. You can see that in this case, the wafer to large, all the activations are going to explode. And if they explode, we know that this is bad for the gradients because we know that the DL DW is, I'm going to multiply the activations. At the activations are hundreds of millions or tens of millions. Those gradients are going to be really big. Big gradients are not good learning. So another, but you might have is, well, I was showing you this for Rayleigh's, but maybe you can mitigate how much the gradients explode by using cat age. However, if you use tan h, Alright, this is just asking you about tonight to general these candidates for the small initialization. The small initialization will still send all of your weights to zero. And then if you use Kenny to the large initialization, the activations for exploded ten age is bounded by one and minus one. So if you actually look at the distribution of the activations for a tan h network with large initialization, you will see that most of them are between -1.1."
"Alright, so we'll do 3C. And let me just drive the intuition that Eric was talking about. So as a 3C asks for non-constant periodic signal XFT for what period does Y of T equals X of T convolved with H of T equal a constant C. And then we tell you to think about the Fourier series and the Fourier transform. So we know that in the frequency domain, Y of J omega is going to be H of J omega times X of J omega. Right. And then from the previous part of the question, well, since we know that H of T is, H of T is a racked, right, then H of J omega has to be a sink. Right. So you should have gotten in 3B that we asked you. Yeah. In 3B, we asked you to sketch the amplitude response. So in 3B, we asked you to sketch the amplitude response. So it would be the amplitude of H of J omega. And you should have gotten that this was equal to sink omega big T over 2 pi, I believe. That's what our solution says at least. And therefore, H of J omega looks like the absolute value of a sink. And it keeps going on, but I'm just going to draw a few. Right. So this is a capital H of J omega. And we want that we know therefore, or we know from our frequency response that the magnitude of Y of J omega is equal to the magnitude of H of J omega times the magnitude of X of J omega. Right. And so if we, if we tell you that Y of T is going to equal C, then can someone tell me what we would expect Y of J omega to be equal to. Delta function. Yeah. Great. So Daniel, tell us a delta function. That's right. So Y of J omega should just be equal to C time to delta function. Since it's in inverse Fourier transform."
"And 40 percent of students say that it's just a right. And 44 percent of students say it's too fast. And 11 percent of students say it's too fast. Okay. Yeah, I want to say that it's too slow. And 34 percent of students say it's slow. 40 percent of students say it's just a right. Okay. 44 percent say it's fast. And 11 percent say it's too fast. Okay. Thank you. Okay. And the poll now. All right."
"And also there is less charge that needs to go into filling the capacitor because the capacitors are smaller. And so what this means is that across these mile in intersections, the action potential both travels faster. And even though it attenuates, it attenuates less slowly. And so it's able to make it to the next node of RON-VA, where as long as when we make it to this next node of RON-VA, the voltage is above the threshold needed to start a positive feedback loop, these nodes of RON-VA are filled with a ton of voltage-gated sodium ion channels. And so as long as the voltage is above threshold is going to re-initiate the feedback loop, that then will regenerate the spike and the spike can travel down the mile-innated axon until the next node of RON-VA, etc., etc. Right? And so that's the mechanism by which essentially you have a repeater, this node of RON-VA, you can think of that as a repeater that regenerates their signal every so often and allows you to then convey this action potential over very long distances, including axons that go from your brain all the way down to your toes."
"And another thing to make sure that makes sense is, what happens if I change lambda, all right? So this distribution in red is this exponential, is this function right here. We saw that at time t equals zero, right? F big T of t is just equal to lambda because I have lambda times e to the zero. So this value right here is lambda. Let's consider that lambda is big. Right so if lambda is big."
"And as the monkey is in the baseline state, then in this window, this is going to be the plan period for the lay period. We're going to measure spikes from all these 97 neurons and then the go cue comes and then the monkey is going to make a reach. So this is a reach. And really, the trick is going to be in this plan period, there's some interesting information here that I need to figure out how to decode to predict where the monkey is planning to reach. So let's revisit that neuron that we were just looking at on the prior slide, where remember for the right upright, the right and then maybe the downright target to the neuron fired at a higher level, but for the three targets, like the left, the up left, and the up target, the neuron fired at a lower level shown here in green. So the color reflects how hard the neuron is firing. Right. And so, sorry, I forgot to explain. So this is just looking at the targets in 2D space. So the x axis is x position, the y axis is y position. These are my seven targets that he's reaching to over here, these same seven targets. And we're saying that one way that I can represent the planet of the firing rate is that these targets have a lower firing rate than these targets that have a higher firing rate. Right. And so exactly like we said, if we have this one neuron and I measure that you're that is firing at 100 spikes per second, right. Then I can guess that he's probably reaching to this target here, which corresponds to letter E, because it's going to be E or A. Right. It's not going to be A because A is in this region where it's 40 spikes per second. Okay. Any questions there? All right. So then if you look at this, you can see that there's some ambiguity here. Right. If you measure that the neuron fires at 40 spikes per second, it's going to be very hard to know the monkey was going to see B, A or G, because all of these correspond to when the neuron fires at 40 spikes per second. And so then the way that we get around this is that we don't record just from one neuron. We record from 96 neurons or 100 neurons, 100 electrodes. Right. So each of these will be the tuning curve of one electrode. All right. And if each electrode, if each neuron had the same tuning curve as we show here, we would be totally doomed."
"And at the end of last lecture, we did the poll and most of you got it correct that Na plus is going to flow inside the cell. And we said that there were two reasons for this. First Na plus is a positive ion and positive charges are going to be attracted to negative charges and be repelled from other positive charges. So because this electric field points in, it's going to provide an electric driving force that pushes Na plus into the cell. And this we call the drift current. So the drift current was our electric driving force. That would push Na plus into the cell. And then we said there was one other driving force, which was a chemical equilibrium driving force. So Na plus is much more concentrated outside the cell than inside the cell. And so if I were to open up a channel to get chemical equilibrium where Na plus is equally distributed everywhere, then Na plus would want to flow down as concentration gradient to go inside the cell."
"And Daniel. Yeah, I was wondering about Friday, if that is going to be regularly scheduled office hours with the TAs. I believe that will not be I believe Friday is also off so UCLA has both Thursday and Friday off. or planning on holding anything there. If there's a difference, if there's a change in what I just said, I will announce it. Okay. Thanks Daniel. Those are good questions. Any other questions?"
"And for this homework question, if you differentiate then this expression with respect to the parameters and lambda, then you're going to be able to solve this optimization problem. Right, so I think it'll become clear when you look at the homework, but I just wanted to put that up front, so that you don't get stuck on that. All right. Any questions on any course logistics? All right. So it's been a week since our last lecture, so I want to remind you where we were. We were considering this case where we record from neural data. In this case, we record from two neurons, neuron 1 on the x-axis, neuron 2 on the y-axis. And we record from these neurons as we ask the monkey to plan movements to different targets. And so here we've shown three different targets, right as red X's, left as blue circles, upward reaches as green triangles."
"And here we're, we're reminding you of that solution. And then in lecture today, we're going to be talking about expectations. In the last question, the homework, you will also have conditional expectations. And so we've given that definition here, but we'll talk about distributions and give a probability refresher today in class. Any questions? All right, so we're going to get back to material. And so last lecture, we had finished the basic neuroscience by all of you part of the class. And now we're getting into actually modeling the action potentials that neurons fire. And we talked about how we might have a task where, for example, and we'll see this in this class, the monkey is controlling a tracer and moving it on a screen. And the spikes are going to be indicative of the movement that the monkey wants to make. And so we will be interested later on and how to interpret these patterns of spikes and use that to decode the movements that the monkey intends to make. And we had given a virtual lab tour last time showing some of the recording setup and whatnot."
"And I asked at the end of last lecture, we can obviously see that the red line is better than the blue, sorry, the red line is better than the green line because it passes through more of these data points. These blue data points seem to show an awkward trend. But then if we want to be able to state this mathematically, we have to be able to define how good these lines are via rigorous math. All right. And so the ideas we talked about here are the basic ideas of machine learning. And so I asked class last time, how is it that we know that a is good and b is, sorry, that red is good and green is bad in a mathematical way. And so I'm going to at the end of last lecture said, well, what we could do is we could look at the errors between the blue points and the predictions from the red and the green lines. And so let's say that this point here is my 10th data point. If I had the neural data at this value, right, my red line would predict that the velocity should be this value, whereas my green line would predict that the velocity should be this value, right?"
"And I think that students in the past may have been able to get. Okay, or under the rationale that if I add more features, I can do no worse right at least in training data that we talked about seminar lecture. Some students will do like why then and they can animate D Y bin. And then see at that helps. And so the rationalist follows maybe D Y bin has very little information about the kinematics. And so when you decode just D Y bin on its own, it doesn't do well. But it might still contain information that when combined with your original neural data leads to a better decode. So there's a field of electrical engineering called information theory. And this is sometimes called synergistic information. So very interesting term that says some data may not be linearly related to the kinematics. So somehow if I incorporate this data with another observation, then it could be that the data is more formative and that's called synergistic information. Okay. Did that answer your question Jonathan? I think so. So should I or like am I doing something wrong or like so I'm not doing something wrong if it's just like a bunch of dots. Yeah, so if you got that D Y bin decoded very poorly. First off. And then you went into class right now if you just decode off of D Y bin and you see that it is poor or you've already done it and you've written your report saying I decoded D Y bin and it was really bad. I would give you full credit for for part three. That's sufficient."
"And I want to pause here and ask if there are any questions recapping anything from last lecture. So Professor, when you were talking about how the action potential speed is correlated to one over RA times CM, do we not take into account RM in this scenario? Yeah, so in this scenario, this has to do, so when we say velocity, we are conceptualizing how along an axon, the action potential spike travels along the length. And so RM, the resistance here is certainly going to affect the voltage, affect how far it propagates. But RM here in terms of time only affects how quickly the voltage will change from inside to outside the cell. But then for action potential speed, we were curious as to how this voltage changes propagating inside the cell. And so that's why the parameters of interest will be the axial resistance and the membrane capacitance, but not the membrane resistance. So the membrane resistance does affect the overall action potential amplitude."
"And if OK, in the interest of time, I can I can state one hint and actually maybe just write out where you want to start off. So, for part, see, you were mentioning probabilities in terms of CDs, which, which is a valid approach for doing part a also in part a you could also have done it with the number of spike So, let me just say what I for part a I think you mentioned that it would be the probability that the CDF that the first. Sorry, that the first neuron. second neuron, its first ISI is greater than 60 milliseconds. And that's totally correct and you should get, you should get the correct answer that way. Another way that you could have written it is the probability that there are just that there are zero spikes on n1. At time t equals 60 milliseconds, the same time. And so, the reason that thinking about this helps is it helps to frame Part C. They want to know the probability that a neuron is detected on electrode one before electrode to. And so Let me first write it out in terms of, in terms of just thinking about in terms of number of spikes. And then, and then I think that will give a pathway for the answer and then after that I can listen again if you like chase to how you were trying to set it up and see if it's equivalent or not."
"And if we believe that you have been academically dishonest on an assignment or exam, I will follow up and submit the case to the office to the dean of students office. So again, please be academically honest in this class and ultimately fair to your fellow peers. So this is how exams are going to work. We, the teaching staff, recognize that during online instruction with remote exams, it's very difficult for us to prevent cheating on exams, for example, due to collaboration. So in trying to make things as equitable as possible, the first thing that we're going to do is make all exams open note, open book, and you're free to access CCLE on your computer. Exam will only be closed internet and that you shouldn't, you can't Google the answers to questions. And to try to discourage this, the TAs and I are also going to aim to write exam questions that I cannot be easily searched. To deal with this issue of collaboration, the TAs and I are also going to perform analyses on exam answers after that. And if we suspect any students of collaborating on the exam, we reserve the right to administer an oral exam to the suspected students and the results of the oral exam will supersede the results of the written exam. And if there are any other further policies, we'll announce those closer to the gate. In Jonathan. So I know that you said the exam is closed internet, but since we're also allowed to use the notes on CCLE, can we have several tabs of CCLE open like of like, oh, this is chapter one notes. So I don't have to flip through CCLE. Yes, just click on this tab. Yes, you're welcome to do that. So it is close and try to accept to access CCLE. You can have as many CCLE tabs open as you like."
"And in the prior parts for log p of y given ck. You all ended up using the Gaussian distributions. Now for Caleb's question as to what happens when this is poson. So for the poson distribution, what we do is. In the multi variate gas in case we model this covariance between neurons. And so there is some inter there are terms following how. Enter how the relationship between neurons in the poson case, we don't have that. So in the poson case, we just have a random variable describing a single neuron. So the way that we said this in the homework statement is that. When we take the probability of all. D neurons. So why to the J is. Your vector in our D and it contains your firing rates of the dean neurons. So why one why to to why D. And this part what we say is that. The why I the little why I the elements of these of this vector are independent conditioned on the class."
"And in this particular question, we should look at And so our axial is going to refer to this equation, where it's going to be our a which is some row, which is going to be a constant that's the same for for both neurons, and divided by the pie a squared so this will be the surface, the A here would be this radius right here. And the axial resistance is going to be a function of the area of the cylinder. And the intuition for that again was that if we have, if you have the same amount of ions there are less things in the way for your na plus to move through and so if there is more space than the resistance should be lower. And so that was the rationale for how we got this row pi squared. So this quantity is going to equal row times. which was our, where's the slide, yeah, little rA, which was our resistance in a particular, for a particular length in Cm, had the equation rho, rA equals rho over the area, which is pi times R squared."
"And it has this shape like in red, right. And then we also mentioned that it could be described by a cumulative distribution function, which is the integral of the probability density function. exponentially distributed random variable, then the probability that big T is greater than some time little t is going to be e to the minus lambda little t. All right, any questions on just any of the setup thus far? All right, hopefully relatively straightforward and you probably encountered the exponential distribution in your first probability class. All right, so last we left off then, we were on this slide where we were going to quantify some statistics of interest for the exponential distribution, in particular the mean and the variance. All right, so the expected value of the random variable t that is exponentially distributed we know follows this formula. And we didn't do this derivation since it's something that you've done in a prior class but we've put it in the online notes."
"And it's because there are two, like, there's, it would be an A multiplied by an A probability. Yeah. There's another, yeah, there's another A here. So then from this, I have to do the simplification from there. Exactly. And then, but this, at this stage, you're pretty much done because we know that for this to simplify to P of B, this is, if we have, we need a sum over A, a sum over D of P of B comma A comma D. And in general, this does not simplify to that. So P of D times P of B given A, D, that would give us a P of B comma D given A. Yeah. Oh, sorry. No, that's not even true. Because it's not D given A. So actually, in general, this does not, and so you can say, and you can write some text arguing out why that isn't the case."
"And it's like almost the same number. Um, I'm going to pause the video for a moment so I can just talk to you about the numbers."
"And it's like almost the same number. Um, I'm going to pause the video for a moment so I can just talk to you about the numbers."
"And lastly, as I said before, please feel free to ask questions and we'll do our best to answer those. All right. So I thought about what the motivation for this class would be, and there's actually been some recent news that you may know of that provides a pretty good motivation for this class. And so last year a company called Neuralink, which is founded by Elon Musk, held essentially a conference of sorts where they demonstrated some new technology that they are developing to try to interface with the brain. And we'll talk about some of that technology throughout the class. It speaks to the fact that what was previously something that was more in the research domain and the domain of academics is starting to be something embraced by industry, where perhaps next generation of industry is looking towards how do we interface with the brain through new technologies? And how do we use that to both understand the brain and engineer with the brain, build devices that can be either driven by your brain or right into your brain. All right. And so a lot of this builds off of some subtle work beginning in the mid-2000s. And we're going to try to address this through, unpack this through answering this question, what is Neuralink's processing? So again, I mentioned to you earlier, you may know what some of these words mean in isolation. Neural refers to neurons, which are the basic computing elements in the brain, and which we'll talk about. And the first two and a half weeks of this class discuss the basic workings of neurons. Signal processing, you all likely know from one of the first EEClaces E102, where you did a Fourier, and you were able to learn techniques that essentially take signals and extract out features and information from the signals that are relevant to solving a particular application problem. All right. So I have shown here three images. The first is an image of actual neurons in the brain. And what we showed here on this long device here is an electrode. The same type of electrode that you would use to measure the voltage between two points on a circuit. And so here this electrode gets very close to a particular neuron in the brain. And we're going to learn in this class that neurons in the brain actually communicate via electrical principles. And so we'll talk about how neurons actually shuttle ions from outside to inside their cell, allowing the voltage across the neuron to change and how that encodes information. Because the signals transmitted by neurons are fundamentally electrical, we can transduce them into signals that then we can operate on via circuits. And so here we're showing a circuit board that can perform operations on neural signal voltages that we measure from the brain. And then taking those transduced voltages, we can apply them to solve engineering problems."
"And so be sure to, if you don't do a significant attempt on these questions for homework number seven, be sure that when you look at the solutions, you understand how to do these questions because they are material that could be covered on the final exam. Tonmoi will be holding a final exam review session this Sunday, so in six days. And there's going to be a link, which is here. And I believe Tonmoi will either, Tonmoi will send an announcement on Piazza or CCLE and asking for what time works best for the Sunday review session. All right, so I encourage you all to attend that review session. And then the final exam is going to be held on Wednesday, December 16th, from 3 to 6 PM over Zoom. We're going to be sending on email with further exam details, but in broad strokes, the exam, the final exam logistics are going to be very similar to the midterm exam logistics. The final exam is commuative and so it'll cover topics across the entire class up to and including the inversion of Laplace transform. All right."
"And so by doing the regression over positions and velocities, you're getting values here, which are informative. But you know, you because we know that the position is the integrated velocity and that has to obey the loss of physics. We can put what we know from physics here and our physics knowledge is more accurate than what the least squares regression would pull out. So in that case, because the physics is perfect and the least squares is not, then we use the physics. Yeah, that makes sense. Thank you. Yeah. Great. Great question. Yeah. Professor."
"And so by doing the regression over positions and velocities, you're getting values here, which are informative. But you know, you because we know that the position is the integrated velocity and that has to obey the loss of physics. We can put what we know from physics here and our physics knowledge is more accurate than what the least squares regression would pull out. So in that case, because the physics is perfect and the least squares is not, then we use the physics. Yeah, that makes sense. Thank you. Yeah. Great. Great question. Yeah. Professor."
"And so f of t can be written as this infinite sum of exponentials, complex exponentials, where each is weighted by coefficient ck. And ck is the equation that we derive for computing the Fourier transform coefficients. And now this big T here is the period. And in the Fourier transform, we're going to set big T equal to infinity again, so the signal never repeats. All right. So we had started this example of the rect last lecture. What we did is we said we're going to have a rect and the rect is 1 from negative 0.5 to 0.5 and then after that it stays 0 and the amount of time that it stays 0 is, it's going to stay 0 until a time capital T over 2 on the right side and until a time negative cap T over 2 on the And so for this rec that stays zero until these two time points we have the slide that shows the Fourier series coefficients for this rect is CK equals one over big T the period, think of K over big T."
"And so for B, we can walk through that if that would be helpful. Yes, please. All right, great. So we have this graph, which is x1 to x2 to x3 to x4. And then we say a possible application of this directly graph is to model a stimulus that changes over time. And so in this case, we have x1 is Gaussian like this. And then xt given xt minus 1 is Gaussian with mean xt minus 1 and covariance, and variance sigma squared. And here this is for t equals 2, 3, 4. All right, and so then we ask what is the 4 by 4 covariance matrix of the vector x, which is x1, x2, x3, x4. All right, so can someone, it can be Edwin or anyone else tell me what a first step you thought to do here was? I started with working with the definition of the covariance and an useful thing I thought was trying writing the graphical interpretation of the probabilities."
And so for the Fourier transform the Fourier transform is going to be a special case of the Laplace transform where you can replace all the s's with j omega as long as the Laplace transform region of convergence contains the j omega axis. All right. We drew another example maybe another Laplace transform has a region of convergence shown here in purple. And this would not include the j omega axis. And so the Laplace transform that you find for this purple quantity you cannot just replace s with j omega and arrive at a Fourier transform. Okay. I want to pause here and ask if there are any questions recapping what we talked about last lecture. Chris. Hi. Could you please repeat what you were saying about the green and the purple interval?
"And so for this, we could calculate this straightforwardly. We just have to calculate eight of these probabilities and then pick the biggest one. All right. In the case where now we're controlling a continuous cursor moving on the screen, we're not choosing one out of eight classes. We're decoding a continuous velocity XK. All right. And so what we said last time we would need to do then is we would need to calculate the actual distribution of what the velocity should be XK, what the kinematics should be given all my neural data from time Y1 to YK. All right. And so that distribution could look like this, for example. And this tells me that given my neural data from Y1 to YK, I could expect my velocities to be centered around 10 centimeters per second. And there's a low probability that it's higher and the low probability that it's less than 10 centimeters per second. So likely my velocity is around 10 centimeters per second. And so this is a distribution of what my velocities will be given my neural data. When I decode, I need to pick one single value of my velocity. And at the end of last lecture, what we showed is if you have a distribution over your velocities to pick the one single velocity to decode to move the cursor, we would pick the mean because the mean is the minimizer is the one single value that minimizes the squared error of the random vector of the random vector Z or the random vector representing the kinematics. All right. So that's what we set up last lecture, which is that to go to the testing phase where I get new neural data and I want to move this cursor, what I need to do is I need to calculate the probability of the cursor's kinematics of its velocity, given all my neural data, and then my decode value will just be the mean of this distribution. So that's the setup that we talked about at the end of last lecture. Are there any questions here? Question from Grace. And today you should just be able to unmute yourself."
"And so for this, we could calculate this straightforwardly. We just have to calculate eight of these probabilities and then pick the biggest one. All right. In the case where now we're controlling a continuous cursor moving on the screen, we're not choosing one out of eight classes. We're decoding a continuous velocity XK. All right. And so what we said last time we would need to do then is we would need to calculate the actual distribution of what the velocity should be XK, what the kinematics should be given all my neural data from time Y1 to YK. All right. And so that distribution could look like this, for example. And this tells me that given my neural data from Y1 to YK, I could expect my velocities to be centered around 10 centimeters per second. And there's a low probability that it's higher and the low probability that it's less than 10 centimeters per second. So likely my velocity is around 10 centimeters per second. And so this is a distribution of what my velocities will be given my neural data. When I decode, I need to pick one single value of my velocity. And at the end of last lecture, what we showed is if you have a distribution over your velocities to pick the one single velocity to decode to move the cursor, we would pick the mean because the mean is the minimizer is the one single value that minimizes the squared error of the random vector of the random vector Z or the random vector representing the kinematics. All right. So that's what we set up last lecture, which is that to go to the testing phase where I get new neural data and I want to move this cursor, what I need to do is I need to calculate the probability of the cursor's kinematics of its velocity, given all my neural data, and then my decode value will just be the mean of this distribution. So that's the setup that we talked about at the end of last lecture. Are there any questions here? Question from Grace. And today you should just be able to unmute yourself."
"And so here I'm showing the cover of a seminal paper from 2006 in nature from the Hawkberg group at Massachusetts General Hospital and Brown University, where they had a paralyzed participant named Matt Nagel here. And what they did is they inserted these electrodes into his brain. And although Matt Nagel was paralyzed from the neck down after he suffered a paralysis in a knife injury, they were able to read signals from his brain emitted by these neurons and use that to drive and move artificial curses on the computer screen, essentially giving him control of a computer cursor. And so we'll talk about in this class some of the algorithms that are developed in the service of that. All right. So getting back to this question, what is neural signal processing? So we're going to take a bit of a tour of history right now and then we'll get back to this question. But for millennia, right, and people have sought to understand what gives rise to our ability to perceive the role that around us, to reason about it, and then to act. And these are qualities that make us uniquely human. And so for perception, this refers to the sensory inputs that we regularly receive throughout the day. One example, which I give now because it's something that we may experience later on in this lecture, is when we watch a video, like a video on YouTube, right? So when we watch a video on YouTube, we have visual information, which are the actual images within the video being streamed into our eyes. We also take in auditory information."
"And so here, let me add a bit more structure. Even if you've already emailed me, what I'm saying is, if you cannot take the exam during class time because you're in a different time zone, or if I have approved you to take the exam during a different time already, please send me a new email with the following. The email subject is going to be EC 102 underscore MT underscore reschedge. And that makes sure that I can search everyone and have everyone included. And then in the email, I need you to tell me what time zone you are in. And then the time that you cannot take the exam in specific standard time. If you don't provide this information, then I'm going to assume that any other time other than class time is going to be fine for you. And please send me this information by Friday so that we can reschedule all of these other exams. All right. And then this will be in the announcement that we sent out tonight, which is that Tonmoi is going to hold the midterm review session on Sunday."
"And so I'm going to launch this poll and it's anonymous. Please answer as you feel truly. And then I'm going to ask the TA to send me the results of the poll since I cannot see them right now. All right. And then TA, can you just send me the poll over on the chat when I enough have filtered out? Let me know if I need to end the poll for you to see the values. Or can you see them right now? Yeah, we can see the right now. And 96 out of four, while either two students have answered. And 97, yeah, and keep coming the percentages, please. Okay. One percent of students say it's too slow. Three percent of students say that it's slow."
"And so if I only cared about really calculating the Fourier transform of this aperiodic rect, what I could do is I could define T to go towards infinity. And so it would extend this zero time all the way to infinity and it would never repeat therefore. Right. And so the idea for calculating the Fourier transform of the signal is that we are going to take a signal and extend and make it period infinite. And so if we make it period infinite is going as a signal that is no longer periodic because it's never going to repeat since the period T, big T is infinite. All right. Any questions on the intuition here? All right. So our basic starting point is we're going to take our Fourier series equations, right?"
"And so if you think about someone with paralysis and we'll talk about this a bit later on in this lecture, their brains are totally functioning, but they can't move because for example, their spinal cord is severed, so they can generate the intention to move, but no movement will come out. And so, again, we'll talk about this in a bit more detail, but then for people with paralysis, what we can do is we can try to bypass this broken spinal cord by directly reading out from the brain. And if we can interpret those signals into movements, the movements that they want to make, like, moving my arm to the right or to the left, then we can restore some communication and prevent them to them. Okay, any other questions? Well, yeah. Is this course going to, like, roughly follow what we learned in 102, or is it kind of slightly by itself? It's more by itself."
"And so if you... So, you mean like Bayes' rule? Yeah, Bayes' rule would be the first one that I try also. So if we had that, we would need like a probability n equals n. Sorry, n equals n divided, given that n equals little m. And then the numerator would be a probability n equals little m given big N equals n, and then probability of big N equals n. And we do know these two. But we don't know- But right now, exactly, yeah, we don't know the denominator."
"And so in class we had mentioned and I think we asked you in the homework to just do it for velocities. So what you should do is when you compute a, I think we call it a s. This is a 2 by 2 matrix and it's done via doing the least squares between the a equals x of k times x of k minus 1 pseudo inverse. But now these x's of k is going to be just your 2D velocities. So it's going to be 2 by k minus 1. So after you do this least squares regression, a s will be a 2 by 2 matrix. So then you ask the question what does it mean to impute? What we mean by that is that now the whole a matrix is going to be 1 that is 5 by 5. However we're only going to put in the 2 by 2 matrix here because remember that this a matrix is relating px at time k plus 1, py at time k plus 1, the x at time k plus 1, py at time k plus 1 and 1 to the positions and velocities at time k. And so we want the position equations to obey physics. So the position equation will always be 1, 0, delta t, 0, 0, 1, 0, delta t, 0 and the one will always be equal to 1."
"And so in class we had mentioned and I think we asked you in the homework to just do it for velocities. So what you should do is when you compute a, I think we call it a s. This is a 2 by 2 matrix and it's done via doing the least squares between the a equals x of k times x of k minus 1 pseudo inverse. But now these x's of k is going to be just your 2D velocities. So it's going to be 2 by k minus 1. So after you do this least squares regression, a s will be a 2 by 2 matrix. So then you ask the question what does it mean to impute? What we mean by that is that now the whole a matrix is going to be 1 that is 5 by 5. However we're only going to put in the 2 by 2 matrix here because remember that this a matrix is relating px at time k plus 1, py at time k plus 1, the x at time k plus 1, py at time k plus 1 and 1 to the positions and velocities at time k. And so we want the position equations to obey physics. So the position equation will always be 1, 0, delta t, 0, 0, 1, 0, delta t, 0 and the one will always be equal to 1."
"And so in this example here, there's C1, C2, and C3, three possible reaches. P of X given Ck, then, is what the neural data looks like when you have data coming from that class. And so P of X given C1 is going to be the distribution of these red Xs because it's a distribution of data X, given that the monkey planned a reach to the rightward target c1. And we'll make one of these distributions p of x given c2 for these blue circles and a p of x given c3 for these green triangles. And so that is what we need to learn in the training phase. And then in the testing phase what we will then be able to calculate is given Given some new neural data that I don't know the answer to, so given some XJ, what target CK did the monkey plan to?"
"And so in training, that would be akin to learning these boundaries that we draw here in pink. And then when we go to the test phase where we want to now put our algorithm online and make new predictions without knowing the answer, what we do is we remember those boundaries. And now, depending on where the data, the new data point here as an orange square pops up, it pops up in this vicinity, then we can guess that the monkey planned an upward reach and if in this vicinity, the monkey planned a rightward reach, etc. Any questions here? All right, so then last lecture we talked about how do we set up this model such that we learn these decision boundaries in the training set. And so we said we were going to work with so-called probabilistic generative models. And we said that what we would want to do is we want to learn a distribution P of X given CK, right? Now, CK is one of the potential classes that the data could come from."
"And so in words, the expected value of my exponentially distributed random variable is the average time in between spikes, in between events, right? And if this is what our exponential distribution looks like, the expected value is going to be the mean value that my random variable big T is going to take on. It's going to take on low values with high probability and high values with low probability. And so the expected value is going to be somewhere in the middle."
"And so it doesn't just have to be a real value, it can be a real plus and imaginary part. So a could equal some x plus j times y or equivalently some r times e to the j data. Right. So it can be a complex number. If you look at the complex number representation as a phaser, what this tells us is that it can get scaled up or down. That's r and then this e to the j data means that it can get phase shifted left or right. The signal can be shifted left or right. Right. And so again, high level. And eigenfunction is when I put that function into the system, I get that function out scaled by a. And now I made this claim at the end of last lecture and I told you we would prove it in this lecture. And this is the first result for LTI systems. The eigenfunction is for LTI systems complex exponentials are eigenfunctions of LTI systems. So if I take any complex exponential. And I put it into an LTI system that complex exponentials going to come on the outside scaled by scaled by some value. All right. Any questions on this concept? Okay. So we're going to go ahead and prove it now."
"And so it would be 250 spikes per second. This is spikes per second. So that's the fastest that a neuron can fire. But if I just consider, for example, we were talking about listening to an orchestra and an oboe playing a concert A, right? A concert A is already a 440 hertz signal, meaning that the time between consecutive peaks is around two milliseconds. All right. And so this signal is changing faster than a neuron can spike. Right. And so this poses additional challenges for how such fast changing signals would be represented in the neural system. Okay. Any questions on this challenge? All right."
"And so it's going to be an example like this where we see the, it's going more and more, it's having higher and higher amplitude. Whereas if you're in the left hand side of the plane, so that sigma is less than zero, this is going to correspond to a complex exponential where it decays over time. And then similarly, again, we know that omega here, omega sets the frequency of the sinusoid. And so if omega grows, if you go up on this y axis, that corresponds to faster oscillations because now the frequency of your sinusoid are going to be higher. And so for a complex exponential with some value of sigma and some value of omega, if you plot that sigma and omega somewhere in this plane, you can conceptualize the signal. If the sigma value, if you're at this point of plane, then sigma is negative, so it's going to be a decaying exponential. And the frequency omega is relatively small, so it's going to also weight slowly. Whereas if you're up here, then you're going to have a growing complex sinusoid that has a pretty fast oscillation because you're high on the y axis, you're high on omega. Okay."
"And so it's not totally necessary. So Y1K, if we do divide by 25, that'll be neuron once firing rate in that 25 millisecond bin. Y2K will be neuron 2's firing rate, et cetera, et cetera, down to YNK, which is the 96 electrodes firing rate. So we have a question from Jonathan. Yeah, is this sort of analysis performed during the experiment or is it performed after the experiment? Or does it matter? Great. So for building the decoder, which is what we're going to talk about first, how we train it, we collect the data, and then we do this analysis after the experiment. However, it's entirely possible to also do this as the experiment is running, collect your data, and then update your decoder. So when you take these bins of neuron firing rates and you've already performed the experiment, are you putting a bin center around time point or is sort of like the bin, it's taking the time point and a number of time points before the time point."
"And so like I just mentioned, all lectures, as well as at least one discussion section, and oh, sorry, all lectures, discussions, and office hours will be carried out on Zoom. Lectures and at least one discussion is going to be recorded and uploaded to CSELE. We encourage you to attend live lectures. But like I mentioned, we're going to be recording the live lecture for the people who aren't able to make it to the lecture on time. And so because of that, if you are uncomfortable being recorded in a lecture, these lectures will be recorded for the sake of the entire class. And we ask that you off that of not being recorded by not attending a live lecture. But if that's not a concern for you, again, we encourage you to attend the live lectures. All right. And if there's someone who feels they learn better through live Zoom lectures by having your camera on, I know that I welcomed that. It's a lot better for me than just looking at black boxes with names. All right. And so feel free to do so if you prefer that. All right. And then like I mentioned, during live lectures, we're happy to answer questions. So to do that, you can raise your virtual hand in which case I will ask you to unmute. And then we'll have you ask the question, then I'll answer that. And then the other way is through the chat functionality. And so if you write your question to chat, then Tuan Moai and Tishank will be able to answer questions over the chat. All right. Before we just started, any questions on just any Zoom setup? Reveited things? All right, then. So I wanted to start off this class by giving you a sense of what this class is all about. Because when we hear a title like neural signal processing, maybe we may know what some of those words mean in isolation, but what's they mean when you put them all together? And what is going to be Tuan in this class? All right. So we'll go over through the motivation of this class and essentially the high-level areas that we'll cover. Some high-level thoughts is that first, this class is essentially the brain-meat engineering. So how can we, as people come from electrical engineering, computer science, develop techniques to try to understand how the brain does computation as well as interface with the brain to support human health? The other thing is that I know this class is in elected, and so although there's going to be a bunch of work in this class and we're going to go over this syllabus by the end of class, it's my sincere hope that this class is also fine. And so we'll talk again about what we're going to do in this class, but in this class we'll be playing with neural data actually recorded from the brain and building algorithms to decode that neural data. All right."
"And so maybe we have the case that the phase of the high frequency signals is close to the close is small so that their face doesn't get shifted a lot. And so maybe these stay here. Alright. So now because all the different cosines are being shifted at different phases relative to each other. When I go ahead and add them up. They won't perfectly add up to give me my original signal. Instead, when they add up, maybe I'll start to see some distortions in the output where here. Maybe the lower frequencies were delayed later so they only add up to give me my original signal when the low frequencies are also incorporated but maybe early on, I have just more representation from the higher frequencies. So when I add them back together that beats to some distortion in the output."
"And so now I'm going to differentiate this term with respect to sigma. So for this term, which we'll draw here and move. I'm going to have first this log normal term. And that log normal term has a bunch of expressions. This middle expression here, the D over 2 log of 2 times pi. This doesn't have any sigma. So I can ignore it. So I'm going to do this one first. Right. So here I'm going to get this term minus 1 half. And then we have here an X i minus new transpose sigma inverse X i minus new. We know that this is a scalar. Right. Because this is a row vector times a matrix times a column vector. And since it's a scalar and we know that the trace of a scalar is a scalar, I'm going to go ahead and just write trace of this expression. So instead of just running the expression, I'm going to put a trace around it. So I'm going to write trace of X i minus new transpose sigma inverse X i minus new. And then I'm also going to have a term here minus 1 half log a determinant of sigma. So I'll copy that of minus 1 half log determinant sigma. And I'm going to differentiate both of these terms. And then I'm going to have a plus. So this red term doesn't have a sigma. So differentiating with respect to sigma gives 0 and then I'll have a 1 minus ti. And then pretty much the same thing. I'm going to have a minus 1 half trace of every got to put my you want to see. 1 half trace of X i minus new zero sigma inverse X i minus new zero minus 1 half log determinant of sigma. All right. Any questions here. Okay, so if I look at these expressions for the 1 half log determinant of sigma. I'm sitting in pretty good shape because I know how to differentiate these. I have my look up table and here. The derivative of log determinant sigma with respect to sigma will just be sigma inverse. So these terms I can differentiate. But these terms. The traces of these vector times matrix times vector is not exactly in this form trace of sigma inverse times a. All right. So how can I transform these expressions to be in this form so I can apply this derivative rule here. You can write it in the chatter or razor hand. So charan says we can use this cycling property of the trace. And so when I want to use this derivative rule, I want the sigma minus 1 to be in front."
"And so on this Utah array, we'll have capital N equals 96 electrodes. So on each of these electrodes, we're recording spike, spike knee activity. And so here we have a vector that tells us the bin spike counts at a given time. All right. So this is this illustration here. The input to these decoders are a spike vaster where we have 96 neurons. And basically at some time point K, what we do is we define a bin with a window. And I believe in the homework, this window will be 25 milliseconds long. Right. And what we do is we count the number of spikes that occur on every single neuron or every single electrode in this 25 milliseconds. And if we divide the number of spikes by 25 milliseconds, that gives us the firing rate, nestinit of firing rate. Right. We usually just avoid the dividing by 25 since I've just scales everything."
"And so please send me an email this week if you plan on taking us off on this. We will we use that just to get a sense of how many accommodations we'll need to make and then we're going to send out more details closer to the exam date to handle those accommodations. Any any questions here? All right. I have to slide on academic integrity, which I give in all my classes. And it's my way of saying up front that I care a lot about academic integrity and that we all follow the principles of academic integrity and fairness and respect to our fellow classmates. And so I put up the slide to say that I take this very seriously. And if we catch any students of cheating or violating the principles of academic integrity, that I take that seriously. And I will follow up and report those cases to the dean of the students office. And we will follow their recommendation as they investigate the case and do what they do what they determine us to do. All right. So I just want to put that up front that again. If we if we catch you violating academic integrity, we will follow up on it and report their case to the dean of students. All right. Any questions here?"
"And so please send me an email this week if you plan on taking us off on this. We will we use that just to get a sense of how many accommodations we'll need to make and then we're going to send out more details closer to the exam date to handle those accommodations. Any any questions here? All right. I have to slide on academic integrity, which I give in all my classes. And it's my way of saying up front that I care a lot about academic integrity and that we all follow the principles of academic integrity and fairness and respect to our fellow classmates. And so I put up the slide to say that I take this very seriously. And if we catch any students of cheating or violating the principles of academic integrity, that I take that seriously. And I will follow up and report those cases to the dean of the students office. And we will follow their recommendation as they investigate the case and do what they do what they determine us to do. All right. So I just want to put that up front that again. If we if we catch you violating academic integrity, we will follow up on it and report their case to the dean of students. All right. Any questions here?"
"And so please send me an email this week if you plan on taking us off on this. We will we use that just to get a sense of how many accommodations we'll need to make and then we're going to send out more details closer to the exam date to handle those accommodations. Any any questions here? All right. I have to slide on academic integrity, which I give in all my classes. And it's my way of saying up front that I care a lot about academic integrity and that we all follow the principles of academic integrity and fairness and respect to our fellow classmates. And so I put up the slide to say that I take this very seriously. And if we catch any students of cheating or violating the principles of academic integrity, that I take that seriously. And I will follow up and report those cases to the dean of the students office. And we will follow their recommendation as they investigate the case and do what they do what they determine us to do. All right. So I just want to put that up front that again. If we if we catch you violating academic integrity, we will follow up on it and report their case to the dean of students. All right. Any questions here?"
"And so that corresponds to a delta function right at omega zero. All right, any questions there. Right, so you can see here. We're using the inverse Fourier transform equation the Fourier transform of the impulse response which is called the frequency response. we know the Fourier transform of e to the j omega naught t, which is a delta at omega naught, we can compute the Fourier transforms of a few more signals. And so I've written these out because these are relatively straightforward given everything we've shown already. We know that cosine omega naught is simply the sum of two complex exponentials at omega naught and minus omega naught weighed by one half. We know that from Euler's formula. And so if we wanted to calculate the Fourier transform of cosine, then we're in good shape because we know the Fourier transform of each of the J omega naught t."
"And so that'll be the entire work load for this class. Here will be the Zoom link for this class, which we'll use for lecture. I will be recording lectures and also uploading them to CCLE in case people can't make it. And the office hours for this class are just going to be the same as my ECE-102 office hours. And so if you're not taking this class in currently with ECE-102, those office hours are at this link here. And they're going to be on Thursday from 12 to 2 p.m. Okay. Any questions to start off? All right. Yeah, so this class is something where I want to focus a large part of it on how to think independently and how to use tools at your disposal to answer questions of interest. And so towards this end, the structure of this ECE-189 is that we're going to focus on a particular problem."
"And so that's why I think it's- For each channel. Yeah, for each channel. That's why without the NA plus K plus pump, I think it would just require a bit more thought to think through that hypothetical situation if it would be possible. Yeah. Yeah. But I guess that actually makes a lot more sense as to why it's important that the pump is there."
"And so that's why it makes sense that the Fourier transform of the function by tau. What that corresponds to is a complex exponential e to the minus j omega tau. And we saw using our convolution theorem, as well as our time shift theorem, that this result is consistent with previous things that we've derived. All right, then we try to take the Fourier transform of the signal of just a constant one. And we saw that if we try to evaluate the Fourier transform which is 1, then 1 is going to have a Fourier transform which is 2 pi times delta of minus omega, but then delta is even, so that delta of minus omega turns into delta omega, and that's applying the duality property that we derived last lecture. Andrew?"
"And so the input filtered by this convolved with this delayed impulse responsible will also just be shifted. And so if I shifted the impulse response by three, my output will also be shifted by three. Right. And we said that this is called a distortion list LTI system. And so, a distortion list LTI system is one where the output is going to be the same thing as the input, except it might be scaled by some amplitude K, and it might be shifted by some time. TD, and so TD would be some delay, for example, the delay and the impulse response for making a causal like on on this slide over here. deriving what h of j omega was for a distortionless LTI system and we kind of went through this really quickly so I'm just going to walk us through this again, which is that the distortionless system has this equation that the output is the input that is shifted by some time TD and scaled by some amplitude K. So what is the frequency response, big H of j omega for a distortion list LTI system. And to calculate H of j omega, you all know that the frequency response, H of j omega is going to be the Fourier transform of the output, divided by the Fourier transform of Y of t, is equal to k times big X j omega, and then because it was shifted in time that corresponds to a multiplication in the frequency domain by this complex exponential. And then after I have this, then I can just solve for big H of j omega, which is big Y over big X, and that's equal to some constant big K times a complex exponential. And then the magnitude of H of j omega is just equal to K."
"And so the practice midterms, the last year's midterms, the past year's midterms on CCLE also included material on Fourier transforms, which you will not be tested on in this midterm. So this midterm will only cover material up to and including Fourier series. All right. And so we'll probably finish Fourier series on next lecture or maybe even today's lecture. And then with respect to the timing, we're going to send out an announcement on CCLE closer to the date about the timing of the exam. And when we expect you to finish an upload to grade scope by, right? Any questions on those core statistics? Okay. And then I want to take a poll on the pace of the class. So again, like I mentioned for the homework, this is a unique version of a class where we're teaching this solely online and over zoom. And so I wanted to launch this poll to get your feedback on how the pace of the class is going thus far."
"And so their phantom factor is indeed greater than one. All right. Okay, any questions on any course with just 64 we get back into material? All right. So our last lecture we defined the Poisson process and we defined it as a process where we had events given by the big T's and the events have exponential into our rival times or ISIs. All right. And so those are these little T's. These are all independent and identically distributed and the distribution is an exponential distribution with parameter lambda. All right. And then we said the Poisson process is that or this expression here, NS, what it does is it accepts the time S and then returns a number of spikes that happened up into, up into including that time."
"And so this is also my slide where I'd say, please don't put us in that situation where we have to do this. Please be academically honest and that matters a lot for this class. So we will have a midterm exam and a final exam. And during this time of online instruction, online instruction is very difficult for us to prevent cheating on exams. We acknowledge that, for example, due to collaboration. And so to try to make exams more equitable, all exams are going to be open note, open book, and you may access notes on CCLE via your computer. It is closed internet in that we don't want you to be googling for solutions. However, the TAs and I also aim to write exam questions that could not be easily googled. Finally, we're also going to, after the exams are collected, perform some analyses on the exam answers. And if the TAs or I suspect any students are collaborating on the exam, then we reserve the right to administer an oral exam to any students who are suspected of collaborating. And the oral exam results will supersede the written exam results. If there are any other policies that we'll have towards the exam, we'll announce that closer to the exam."
"And so this one we're just gonna do, this one we're going to do and the details will be clear, but we're going to give a much more extensive discussion of partial fractions later on. And so, in partial fractions what we do is we have a yj omega here, for example, which is two over one plus j omega. Two plus j omega. So we know a Fourier transform pair which is e to the minus at u of t has a Fourier transform 1 over a plus j omega. And so what I'm going to do is I'm going to assume that this expression can be written as the sum of two fractions where the denominator of one fraction is 1 plus j omega and the denominator of the other fraction is 2 plus j omega. So I'm going to assume that this can be written as the sum of two fractions where one fraction has a denominator 1 plus j omega and the other has a denominator 2 plus j omega. If I gave you a fraction like this, right, you would know to simplify it, and I asked you to simplify it into one fraction, what you would do is you would multiply this left-hand sign by 2 plus j omega over 2 plus j omega in the numerator and denominator, and then for the right-hand side, 1 plus j omega in the numerator and denominator. And Tomoy tells me that you all did partial fractions in discussion last week, so that's also great. And so for this partial fraction we're just going to do it a very straightforward way, which is, and again in Laplace transform we're going to give you several methods to do partial fractions, but what we're going to do is I'm going to assume this to be true, that I can write this as a sum of two fractions, and then what I need to do is find out what a and b are such that these two expressions are equal to each other. So what I'm going to do is I'm going to multiply, if this is the left-hand side of the equation and this is the right-hand side of the equation."
"And so we can see that these two terms both cancel out to be equal to one. And so therefore we get that this is equal to e to the minus mu t plus s minus mu of t. Let me put parentheses here. Alright, and so from here, you can see that clearly this probability is a function of the time t. And so since this probability is a function of t, and little t again is the value taken on by little t1, then the distribution of t2 depends on t1, and therefore t2 and t1 are not independent. And so there we have given a formal mathematical proof to show that T2 and T1 are not independent."
"And so we did do something similar for YK given xk following the exact same logic. We found that YK given xk has a normal distribution with mean dxk because cxk is observed. And then the randomness in this equation, if xk is observed, now only come from QK and QK has covariance q. And so that's the covariance right here. All right. And then we were also going to, for convenience, just say that the initial state x1 was also normally distributed with mean new one and covariance v1. All right. So the first thing that we wanted to do now in our common filter setting is train the common filter or train learn the parameters of my system, a w, c, and q that maximize the likelihood of the data. And so what we did, what we started to do then is we want to write the likelihood of the data, which is a probability of having observed my kinematics and my neural data at every single time step. And then of course, this will be a function of data. And I get to choose data. I get to choose data to make this likelihood as big as possible. All right. Any questions on this part of the recap? All right. So then we start to write out the log likelihood last time. So we drew off the graph of the dynamical system where we have our states, the kinematics evolving through time. And at every single point in time, there is this observation process, the yk equals cxk, which lead xk to be a parent of yk. And we know based off of the graph theory lecture that if I want to know the distribution of say x4 given x3, x2, and x1, we know that x4 is going to be conditionally independent of anything before x3 given that I observe x3. So we were able to write that p of xk given xk minus 1, xk minus 2, all the way to x1 equals p of xk given xk minus 1. And also p of yk given all the prior states as well as the prior observations. So the prior kinematics and the prior neural data, this is just equal to p yk given xk for the same exact reason, which is let's say equals 3. And I want p of y3 given I know x3, x2, y2, x1, and y1. But everything here is conditionally independent of y3 given that I observe x3. Okay. Okay, any questions there?"
"And so we had a lot of discussion at the end of last lecture about why this probability can be rewritten as probability number, letter C here. That is the probability that we have zero spikes between time T and time T plus S. So we're going to continue today, actually writing out this probability, right. And this will be the last thing that we do for inhomogeneous Poisson processes and then after that we'll tell you about how to code them up and then that should give you everything you need to do homework number three. So our question is, are the ISIs of a homogeneous, of an inhomogeneous Poisson process independent. And so, let me just, we have in the prior slide that we want to calculate the probability of T2 bigger than S given T1 equals T."
"And so we talked about how the grading scheme is set up and how their bonuses on pizza. Right, so now I just want to talk in more detail about exams for this class. And so this is a slide here on academic integrity. I want to put it up to communicate to you all that I take academic integrity very seriously. I believe it's important that our assessments are fair. And when we make choices on what to do for this class, we do it with fairness in mind. And so in this class, if we suspect you of academically dishonest behavior, please know that we will, that means the teaching staff, we will follow up on that and report that to the Dean of Students."
"And so we talked about our basic cosine wave. And then last lecture, we ended talking about this complex sinusoid, which is this function AE to the J omega T plus theta, which from oilers formula, we can compose at we can decompose as a real part, which is a cosine omega T plus theta as well as an imaginary component, a sine omega T plus theta. And again, with complex numbers, we think of them as a collection of two sets of numbers. In this case, two signals. So the cosine omega T plus theta is this solid line right here. And the second set of signal is the a sine omega T, which is this dotted line. All right. And that's how we should conceptualize such signals in this class. Any questions here? Right. So for, there's also an exponential signal, which is E to the sigma T. You all know what E to the sigma T looks like when sigma is greater than zero, this corresponds to a growing exponential with time."
"And so we'll call this new neural data Y tilde. Okay. And all we do to get the kinematics is we take Y tilde K. And we multiply it. We pre-multiply it by the matrix L. And that will give me my decoded kinematics. And so in the 189 project code. That corresponded to this for loop right here, which starts off by iterating over new test trials, our test. For each new trial, we get out the neural data that we've never seen before. That's why test and we've been it. And then to get out my decoded positions, I do L times Y test. And so that gives me X test, which are my decoded positions. And then, and then I compare those decoded positions eventually, or I guess in the code first, we plot those decoded positions. That was this plot that we saw last time where the center out trials are pretty good, but the center back trials are not good. And then, and then we can compare those to the actual true positions that we recorded in the experiment to see how off we are in that led to our mean square error metric. Okay, so I just want to ask any questions on any of that since that's the key component of this project. All right. So now I want to talk about extensions of this least squares algorithm. So you'll recall all two meetings ago."
"And so we're going to get the real component will be the portion that multiplies the sine. And then the imaginary portion is going to be the remainder, the part that multiplies G of T cosine 2 pi K over big T star T DT. All right. I was going to show you the first two properties but when it started doing the conjure parts. I was a little confused how the math was working out, so I just want to see those worked out. Oh great. Yeah, okay so the last three properties on slide 41. Perfect. Okay, thanks. Well, since essentially the even oddness switches for the real and imaginary parts, so I think the first part should be like the real component is equal to the negative, the real component of C of negative K. Yep. And then, and then for the imaginary part, same deal but remove the minus sign from the front."
"And so we've talked a lot about how action potentials are generated, and we have a hint of several things, and we've talked about how the surprising edge is due to the influx of sodium. And we've talked about how the falling edge is due to the efflux of potassium. It's interesting to think about how they originally came to these conclusions and how they originally mapped out the dynamics of the action potential, which will, again, give us a even clearer picture of what's going on during an action potential being generated. And so there were experiments, which we're going to talk about, but researchers back in the range of the 50s and 60s were interested in learning exactly what types of flow of ions led to action potentials. And they had some clues, for example, if they had low extracellular NA plus potential, and that would be to a low action potential amplitude, which made them think that the rising edge of the action potential is caused by the influx of NA plus. And similarly, they had evidence about K plus being responsible for the falling edge. But then the reason that this is hard to study is because of these voltage gated ion channels."
"And so what that means is that. If I take the probability of this entire vector given the class, I'm in ck. This is going to equal p of y one given ck. Times p of y to give them ck. All of these little why ones why to's why these these are just scalars. They're single neurons. And all of these have a poson distribution. Real quick, these things you could differentiate between the superscript and parentheses in the subscript. Yes, the superscript and parentheses, the J here is referring to which trial we're looking at. So. All of these have a superscript J saying this is a neural data on the j trial. I committed them at a convenience, but let me just put them in to be absolutely clear. So for the j trial, we're saying that the neural data that we see on the j trial, which is a vector in our D for dean neurons. For the naive base plus on model. Can be broken up into a product of probabilities, which is a probability of the first neuron, the first element."
"And so what we then want you to do is you take this matrix a s and you plop it over here. And so this 2 by 2 entry here will be a s. Does that answer your question, Sean? Yes. I just have, I'll see you. Why don't you just include, also include the position when computing the matrix. That's a really great question. Yeah. So we could do that. We could do a being 5 by 5 and just stack all the positions and velocities together and do the least squares regression. The thing is that's going to give values in this matrix. And I think the values in this matrix like these will be close to 1 and these will be close to delta t, but they won't be exactly those."
"And so what we then want you to do is you take this matrix a s and you plop it over here. And so this 2 by 2 entry here will be a s. Does that answer your question, Sean? Yes. I just have, I'll see you. Why don't you just include, also include the position when computing the matrix. That's a really great question. Yeah. So we could do that. We could do a being 5 by 5 and just stack all the positions and velocities together and do the least squares regression. The thing is that's going to give values in this matrix. And I think the values in this matrix like these will be close to 1 and these will be close to delta t, but they won't be exactly those."
"And so when I flip it, this impulse response ends up being over here. I haven't drag it yet. So it's leading edge corresponds to the impulse response. Flip and drag that time T equals or out. And if I advance this by one second, so that, sorry, if I delayed this by one second, so that now the impulse response I'll draw this in orange was at this location. That would be my flippin drag impulse response at time T equals one. Move it over one second if I moved it over. So it was over here. That would be for T equals to. And then we said for the flippin drag, what we would do is we would multiply the input, which is the black rectangle. And my impulse response. Multiply them together and then find the area under the curve. And that would be my convolution. And so when I just flip my impulse response and I'm at T equals or when I multiply my orange. Sorry, let me actually let me erase this orange one. So it's less confused. When I multiply my green. And I'm at the black rectangle. And my black rectangle, they're not going to overlap. And so they're going to be just a zero signal. And I've integrated that. It's going to be zero. And so at time T equals zero. My convolution is going to be zero. And then if I were to drag this rectangle to the left, meaning that T would be negative. And it would never overlap. And so it would be a zero convolution at the result of the integral of a zero signal would be zero for all this time less than or equal to zero. Now the interesting thing is when I start to drag this rectangle. So it starts to overlap with the black one."
"And so yeah, this is a very interesting observation that she has no problem opening and closing the fists of the robotic arm. However, when there is in particular a red cone, she struggles to grasp it. I've seen some talks and talk to the postdocs who are involved with this study. And they did an analysis and it's interesting when she sees the red cone. There is a large change in the firing rates of neurons in her motor cortex. And because something very special about this red cone or not special, but something different about this red cone causes her neurons to fire so differently. That throws off the decoder and that's why she's no longer able to grasp. To try to achieve the support, they also book a virtual and reality environment where they showed her a red cone in the virtual reality environment. And even in that environment, she still had this big change in firing rate that caused her to not be able to grasp. Any questions here? Yes, pressure. Was that was she only unable to grab the red cone because she was able to shake the guy's hand. That's kind of an object to look at."
"And so you can see the notes for the integration by parts from which you can derive that the expected value of the exponential distribution, expected value of T is equal to one over lambda. All right. And so we mentioned that, or right at the end we were saying, what does this expected value of T mean in words? And so this expected value of big T, right, big T is a random variable that for the exponential, for the Poisson process is going to be describing the distribution of these inter-spike intervals. And so the expected value of big T is gonna be the expected value of these inter-spike intervals, which means it's going to be the expected amount of time I need to wait before I see my next spike, all right?"
"And so, I think everyone knew that, but just wanted to state that to be obvious so that no one would be panicking. And the TAs will be going over ��� the TAs will be going over related questions to these during their discussions this week as well ��� related questions to these OREA transfer questions. Yes, Al. Yeah, I guess that's kind of a part of what I was going to ask, if we can go like over an example like of something like a problem like 2A. But you're saying that the TAs might go over examples like that tomorrow? We can go ahead and go over 2A."
"And so, last lecture, we talked about how, if I asked you to compute the integral we would have sine squares times cosines that we would have to integrate. However, if we do this in the frequency domain by taking Fourier transform we find that this output to find the output is almost is very straightforward. And so all I have to do is take the Fourier transform of capital H, which we did over here, and then multiply them together to do this multiplication, we drew this diagram last time to show that this is X of j omega. This here is H of j omega, and then their product is here, why have jam. Why have j omega and take the inverse Fourier transform by recalling that So the inverse Fourier transform is just cosine of one times t. And then similarly for this term over here. All right, so with that we were able to compute what y of t was. Any questions from that example that we did last lecture? All right, so we're going to do one more example, and then after that we're going to get back into our discussion of filters which we started last lecture. And so in this example we have an input to the system which is e to the minus t for t bigger than or equal to zero, so multiplied by the step function. We're going to put this into a system with this impulse response. And so we want to know what the output is as well as this Fourier transform. And again we're going to use the fact that convolution in the time domain is multiplication in the frequency domain. So from our lookup table, we know this pair that e to the minus a t times u of t has Fourier transform one over a plus j omega."
"And so, um, I, um, I would like to draw. I'm going to just try this one more time, because it's going to help with the explanation. If it doesn't work, uh, I will give up on, on trying to share my screen. Uh, I'm glad that, that the problems are happening now and not during lecture. Um, okay, so hopefully, okay, so, okay, I think things are potentially working. Um, why can we ignore W for the intuition sake? So, the, the picture I have in mind here is the following. What I want to do is I want to get a regression between xk minus one and xk, right? And, uh, there are going to be data points around here."
"And so, um, I, um, I would like to draw. I'm going to just try this one more time, because it's going to help with the explanation. If it doesn't work, uh, I will give up on, on trying to share my screen. Uh, I'm glad that, that the problems are happening now and not during lecture. Um, okay, so hopefully, okay, so, okay, I think things are potentially working. Um, why can we ignore W for the intuition sake? So, the, the picture I have in mind here is the following. What I want to do is I want to get a regression between xk minus one and xk, right? And, uh, there are going to be data points around here."
"And so, would be the constant. And so next we, we use the fact that we learned that if a signal is periodic in time, right, it's sampled in the frequency domain. So a periodic signal periodic X of T in time is going to be sampled in the frequency domain. So it's going to be an impulse train. And so we know that the Fourier transform X of J omega is going to look like a bunch of impulses based at some separation given by given by the period of the signal. So from this, we can see that if I want Y of J omega to be equal to a delta, right. And I know that Y of J omega is H of J omega times X of J omega. And that H of J omega is zero at these particular points. If I design my periodic signal X of T to be periodic such that it's samples all occur at the zero points of H of J omega. And I multiply H of J omega and X of J omega all I'm left with is a delta at zero. And it's inverse Fourier transform is going to be a constant. So that's the key trick for this question. Are there any questions on this intuition and then after that, we'll do the math, which answers Eric's question. Why did you focus on the magnitudes instead of just the original function? Was there a reason for that? Yeah, you're right, Daniel. I didn't need to focus on the magnitudes, especially since we want Y of J omega to be C times delta omega. So actually, I sort of just drawn H of J omega being a single mega T upper two pi. And so this would go negative and positive and negative and positive. Actually, let me change that now so that it's not confusing. And I draw I drew this one because I thought it was a picture I saw in the solutions. So since in part, they asked us to plot the magnitude. And the same intuition holds, which is the product of H and X will only equal a delta function, as long as all these replica deltas for as long as all these other deltas occur at the zero points of H of J omega. And Professor, I had a question. So this graph that you have right now, what does this graph represent? Because this is not the magnitude of H of J omega. Yes, so. Yeah, so H of J omega in the prior part is also multiplied by by a phase component. And so at the same time, though, that phase component is not going to change. So this sink will still be zero at these points. So I haven't drawn H of J omega exactly. Yeah, that's steric, rent. So what I've drawn is is the."
"And so. One way in which I can try this is I can split this up into two halves. From zero to big T over two. X of T e to the J times two omega not T dT plus an integral from T over to the big T of X of T e to the J times two times omega not times T. And so here I'm just doing an example for one setting of K. And again, this is to give us the intuition as to what the answer looks like. After which then what Eric said before is the general showing this. So at this stage can someone tell me how this might how we might show that that these two some together equals or. Oh, sorry, I missed the minus sign. Thank you. I just saw that in chat. Everywhere. That minus sign is actually very important. Can you do a substitution exactly the bells exactly. Yeah, so I'm saying that if I integrate one part, the other part should be negative of that. And so if it's going to be negative of that, I just want to do a substitution to see if I can combine these two together. Right. And so."
"And that factorization is that we're going to iterate over all the random variables and it's going to be a product of their probabilities given their parents. And so we had done a few examples where we drew a graph and we were able to write out the factorized distribution. And then we wanted to look at these graphs and gain some intuition over them as well as answer a few questions. The first question is are certain nodes in the graph independent and the other was are they conditionally independent? So we're in the middle of going through these three examples of the potential ways that you could hook up three nodes with links. All right, and we're asking for each of these examples what the factorized distribution of what the factorized distribution of A, B and C was. If A and B are independent and then if A and B are conditionally independent, it's given C. Any questions plus bar? All right, so we did example one and today we're going to continue off on example two."
"And that sort of tripped me up and I was hoping you could clarify the variables and maybe how to proceed with the problem. Yeah, so why I is a scalar. And so I so shouldn't be a vector of size D. Let me, sorry, let me grab my iPad. I should have had it already so that I can write out some things. And I just realized if I do connect my iPad, I have to get my AirPods working. Otherwise my iPad audio will. Will make so let me just do this. I hope it just takes 30 seconds, sorry, everyone. Thank you. All right. Can you all hear me? Yes. Okay, great. The AirPods are working out."
"And that this probability was from the poll, this, the answer is C. So let me write that down. It's probability that N of T plus S minus N of T equals zero. All right, and I'm going to, we had this notation from Wednesday's lecture last week, but recall, we defined this quantity mu of T, which is the integral of my rate function lambda r from time 0 to time t. All right, and so let me just write down that again over here. We have that mu of t is the integral of my firing rate over time from time 0 to time t of lambda r, or let me do t plus s and t here, since that's what we have, this expression."
"And that's going to be the topic of this 60 minutes segment with Scott Pelley. So I'm going to play the news clips that that aired on 60 minutes with respect to a robot controlled brain machine interface here. More than 1300 Americans have lost limbs on the battlefield. And that fact led the Department of Defense to start a crash program to help veterans and civilians by creating an artificial arm and hand that are amazingly human. But that's not the breakthrough. We don't use that word very often because it's overused. But when you see how they have connected this robotic limb to a human brain, you will understand why we made the exception. To take this ultimate step, they had to find a person willing to have brain surgery to explore new frontiers of what it is to be human. That person would have to be an explorer with desperate need, remarkable courage, and maybe most of all, a mind that is gained. All right, so that's the intro. And then we're going to go on to now the clips from their clinical trial. And so this participant, her name is Jan, and actually the person screwing in the circuitry that connects to her, her, her electrode array that records signal from the brain. That's one of the leads of the clinical trial, Jen Collinger. Plugged her brain into the computer, and this is what we saw. So I can move it up and straight down and left and right and diagonally. I can close it and open it and I can go forward and back."
"And that's what you're looking for. When you're looking at Omega, the angular or natural frequency. Alright, so And so, we're going to multiply an impulse train. Omega not, and then an impulse train with impulses separated by Omega naught, and that creates the replicas at Omega naught. Right. And so here we're showing this convolution. Again, just to get the flip. Not again but to get the flip and drag intuition which is, if I take this impulse train and flipping flip it right if I flip it it's even so it's going to be exactly the same. Now when I do the drag this impulse, when I drag it to the left and the right is going to trace out this triangle. All right. And then if I drag this impulse to the right, eventually it's going to trace out a triangle that's going to be this triangle and if I drag this impulse to the left, it's going to eventually trace out a triangle and then the same for all And you can eventually get these triangles replicated every omega. Rampton. Um, so you said that f of j omega must be zero above to pi be and below negative two pi be. Is that always the case for the periodic signals. Yes. So, you do mean periodic in time."
"And the information we have about this graph is that P of A, B, C has this decomposition. So we want to introduce P, A, B, C somewhere. So we're going to write this as sum over C of P of A comma B comma C. And then we'll use the factorization of the graph. So this is sum over C and then we'll have a P of A times P of C given A times P of B given C. Right? At this point, we can see P of A has no dependence on C. And so this is equal to the P of A times the sum over C of P of C given A times P of B given C. And just like last lecture or just like the last example, when we get to this answer here, these will be independent. And if this expression here equals P of B, right? But in general, this is not equal P of B. So in general, this does not equal P of B."
"And the intuition for the Fourier transform of. So let's say it's a rect. And we'll talk about this more in class. Let's say that the rect existed from negative t over 2 to time t over 2. All right. And so if I were to then go ahead and make a periodic extension of this signal, it would look like the following. We repeat every capital T. All right. But I could also modify this signal by making the zero time longer."
"And the reason that this could be a really cool brain-mishing interface is because you could imagine a system where then someone who is paralyzed is looking at a screen full of targets. And the target has a letter. So this could be a communication prosthesis where they're trying to type. And instead of needing to imagine a reach towards each letter, which would take more time, the person controlling the brain-mishing interface could just plan to reach to each target, plan to reach to the target with a T, then the target with the H, then the target with the E. And we could decode those automatically if we know how to decode planned reaches. Any questions there? All right. So this is a video of the task that I showed last time. I'm just going to show it again to refresh memory. All right. So again, when the target is small, he's planning to that target and then when the target becomes big, he reaches. But that plan of tippy is what we're going to decode. Any questions here? All right. So what we do then is we implant the Utah electrode array. We talked about in a prior seminar lecture into the motor regions of the monkeys brain. And so this area here is the motor cortex. And we're going to aim for a particular area called the pre-motor cortex. The pre-motor cortex is a part of motor cortex. And what pre-motor cortex represents are actions made, are things related to movement that are made prior to actually making the movement. And so plan activity, for example, before you actually make your movement is represented strongly in the pre-motor cortex. All right. So I'm going to then show you what the spikes tend to look like for a typical pre-motor cortex neuron. So this PMD here is an abbreviation for pre-motor cortex. And what we would do is we would have the monkey make many reaches to one target. In this case, let's call it the upright target. And when he makes these reaches, what we're going to do is we're going to record from the neurons on that Utah array. And so what we're going to show here is just one neuron where we do many trials. So remember also last lecture, we talked about how when we want to get the firing rate of a neuron, a measurement, because the neural data is noisy, we need to do repeated trials and keep measuring what the spikes look like to get a less noisy view. And so what happens is we measure from one neuron in every single row of this spike raster that we see here corresponds to a trial. And what you'll see is that in this phase where the monkey is planning, that's when the small target is shown and the monkey plans to it, there is some increase of activity for this neuron. And then when the monkey actually makes the movement, which corresponds to when the go to is given the monkey actually performs a reach, you see all of these neuron, sorry, this one neuron is firing many more spikes and it does so consistently trial after trial. And so if we just calculate from these what the average spikes per second are, what we'll see is that when the monkey plans this upright target, there's a brief increase in activity when he plans and when he moves, there's a big increase in activity that then comes down. Okay, any questions on this plot that I just explained here? Alright, so what we could do is this is for just upright reaches, so this is for upright, we could do this for reaches in all directions. And so I could have the monkey reach up many times and record the activity of this neuron. And what you're going to notice is that when he goes to different directions, whether it be like for example upright versus let's call it left, there is very different to plan activity. So when he goes upright or maybe even right, during the plan activity shown here in green, you see that it increases to some intermediate level."
"And the red line has a smaller error to the blue point than does the green line. All right. So let's say this blue point was for k equals 10 to 10th data point that we have. This error at salon 10 from my blue line to my, sorry, from my blue dot to my red line is smaller than from my blue dot to my green line. All right. And now it might not always be the case that every single data point is closer to the red and the green. We also, let's say we have this purple data point over here. Let's say that this was the 11 data point. So we have Y1, 11 and then the X time 11, right? This actually has a smaller error to the green line than it does to the red line. Right. So this epsilon 11 is smaller has a smaller error under our green bad model than under our red good model, right?"
"And the x and y position here will be p of x2 and p of y2. And then the way that we get the velocity is that we simply do the simple oiler approximation of the derivative. So if I want to calculate the velocity in this 25 milliseconds bin, I would take the x velocity would be px2 minus px1 divided by dt, which is 25 milliseconds. And that would give me an estimate of the velocity. And then we would do this regression. Then the neural data from 25 to 50 milliseconds would be regressed against this velocity and this position. And there are some details about the timing that are important to get right. And so we walk through all of that in homework number six. And I want to say one more thing I responded to Jonathan's earlier question, which is we learned a decoder post-talk after the experiment, after we've collected the data. But then of course, after we had that decoder, then we can decode new neural data on the fly to then move a cursor on the screen."
"And then for B, it's kind of the same thing since it's memoryless, but C I think was the one that I wasn't the most sure about. And the way that I reasoned about it was that, I guess since you're seeing if one fires and two doesn't fire at a certain point, it would be the integral of the CDF of the first neuron times one minus the CDF of the second one. And then, or no no no, sorry. I think it'd be the probability that the first neuron fires at any given time, times, the CDF, or the one minus the CDF of the second one. Yeah, so, first off, for number one, your approach is correct. For number two, you said something about it being memoryless, but if you meant the, if you meant, actually, sorry, let me read question B, given that no neurons are detected in the first s seconds. Yeah, so it should just be the independent increments property. I'm not sure if that's what you use there. Yeah, yeah. Great. OK, yeah. So for part C. So part C is a bit more tricky."
"And then h of j omega is going to be a constant 2 because of linearity of the Fourier transform, and then e to the minus 2t is going to give us a Fourier transform of 1 over 2 plus j omega. And so from these we can calculate y of j omega, the Fourier transform of the output, by simply multiplying these two together and we get that the output Fourier transform is 2 over 1 plus j omega times 2 plus j omega. And that is the output Fourier transform. So computing capital Y j omega was very straightforward. Any questions here? All right, so then to calculate the y, little y of t, the inverse Fourier transform of this expression, we typically don't want to do that inverse Fourier transform integral, that's tedious, and so we would use a lookup table."
"And then hopefully that'll give you a pretty good jumping grounds to do the rest of the project. Which again, should not be extremely time consuming. I would say it's about equivalent to two ECLE 102 problem sets. And you all, you have several weeks to do the project. All right. So lastly, left off, we were talking about the problem that we want to solve, which is equivalent to that video that I showed you at the very start of class of the 52 year old women who was controlling a brain computer interface, the cursor on the screen to type on a keyboard. Right. So we said that this is called a regression problem. And what we want to do is we're going to have data where we simultaneously record the position of a cursor on the screen. And the information that we're going to get out is velocity."
"And then my description here was to say if you were interested in trying to push it further. You try can tell me in D Y bin as an additional 192 features on top of your original Y bin and maybe that does better than Y bin alone. But answered Jonathan's question if you decoded off of D Y bin and you saw that it was awful and you. Then your conclusion could be if I just decode off of D Y bin, I don't have good performance. Okay, cool. Thank you. I also ran into the problem. So all I changed was the Y bin. I changed Y bin into D Y bin like the with the formula and also Y test into Y D Y test with the formula. And I ran the code and it was like it was seg faulting where I was supposed to calculate mean square error. Oh, not loud was seg faulting or or it was like it says like you access like too many you access out of the array of. I'm not sure if this is just like an answer if I could it the other part wrong the D Y's wrong, but yeah, yeah. Sorry, I'm just curious. Did not lab the matlab crash and you had to reopen it or did it just throw an error? No, it's just through an error is it okay? Okay. Yeah, I have had matlab seg fault in the past, but I found that it usually takes quite a bit to get it to site fault. And so I think that tells me the magnitude of the error. Yeah, so I think that. To debug this. So what I would do is I would go into your for loop and and put a break point so with the visual editor for matlab, you should be able to insert a break point. If not, you can also just insert a statement here that looks at a single trials, you could say like if I equals one meaning the first trial."
"And then sorry Brandon you're saying something. Yeah. Do you think you could just scroll up and I can probably find the thing. Yeah, sure. Right there, 21. 21. Yeah. So, I was just curious. We know that the outside is more positive compared to the inside, right? Correct. And then, so, is there a preference as to why we define VN as the inside of our cell versus VNS outside."
"And then that the number of spikes in any given window or the variable N of S as independent increments, which means the spikes in non-overlapping windows are independent. And so starting from this definition, recall this is different than how we did homogeneous Poisson processes. In that case, we built a Poisson process by concatenating exponential interarrival times, and we derived these three properties. For inhomogeneous Poisson process, we're saying that it's a process with these three properties. And so then last lecture we were interested in asking, are the inter-arrival times exponential? Right, so we derived the inter-arrival time of just the first spike. And we found that it was not exponential. Right, so that tells us the ISIs independent."
"And then that's sufficient for this question. Okay. I see. So that answers my next question because I got to that place for part G, where it was this sum over D for P of D times P of B given A of D. And my question was, can that simplify to probability of B given A, and I don't think it can, right? It can, yes. So for that one, that is law of total probability. So if you have, can you repeat the sum that you have? I had sum over D of probability of D multiplied by probability of B given A comma D. B given A comma D. So can that simplify to probability of B given A? This one cannot. You would need of, if this was, if this was D given A, then this would equal sum over D probability of B comma D given A. And then this would simplify to probability of B given A."
"And then the phase of j omega is what's multiplying the j in the exponential, which is minus omega Td, the amount of delay we have in our system. All right. So in the distortionless LTI system, this is what its frequency response looks like. It has a magnitude which is this K, and so if I plotted the magnitude response, it would big K. And then the phase response is going to be negative omega Td. So that means that as omega increases, it's going to get more negative. And the slope of this line is going to be minus Td. Okay. And so if you deviate from this amplitude and phase spectrum, for example, if your amplitude spectrum look like this, right, or the phase spectrum look like this, then your output is no longer going to be distortionless."
"And then there's a higher frequency cosine. And then, let me do this in a different color. Purple, maybe even higher frequency cosine. this rect right here, right? The phase tells me how much each of these different cosines is gonna be shifted, right? Because phase gives a shift in the cosine. And so when there's phase distortion, the way that I picture it is that the cosines are being delayed relative to each other such that when you add them back up, the signal looks distorted."
"And then those correspond to neurons for whom we can know exactly when they spiked. All right, so from there, we'll get into neural encoding and decoding. This class will primarily be about neural decoding. And we're going to give some more details on how an algorithm works, but then either at the end of the sector or off the next lecture, we'll actually derive the decoder that we're going to be using in this class for the project. All right. And so these neurons, their fundamental currency of information is their spikes. And neurons represent and transmit information by their sequences of spiking. All right. And so this is how information is both encoded from the brain. So encoding means how does how do neurons represent stimuli from the outside world. And so this encoding could be, for example, light, right? You could be looking at a picture and a light of different frequencies is coming into your, as I coming onto your retina, and then being sent into your brain, and then how the neurons represent that light or sound intensity, those would be neural encoding problems. All right. And so in neural encoding, what we try to do is we have a model where the neurons firing rate, so they'll call the neurons, or try the neurons spiking response, why? This is going to be some function of stimuli s. And so that stimuli could be like modatory sound, or again, light or video. All right. And so neural encoding is trying to learn how the neurons represent your stimuli by learning this function path."
"And then to get the partial fraction expansion we said that there are two steps first we need to find the polls, which means finding the zeros of the denominator of the best, which is a of s, and then after that we have to find the residues. So, getting the polls lambda one to lambda and all that is is solving for the roots of this equation. Right. But then the new thing that we have to do is be able to solve for the residues are one to our. And so we gave three methods to do this. And we're going to get back to these methods today so we started off by saying there's a method one which is typically labor intensive and we won't do it. And method two is the most commonly used method which is called the cover up procedure. Right. And so, in the cover up procedure. We went through this a bit more slowly today because this is the technique that you'll most frequently use when inverting the Laplace transform. of s factored into its form such that we have our three poles lambda one, lambda two, and lambda three. And then this is equal to three partial fractions each with the residue r1, r2, and r3. All right, so the idea behind method two is that to solve for one residue, let's say r1 term. But it's going to cancel out with one of the s minus ones in the denominator and so these two terms are going to cancel out. But now for R2 and R3, they're also going to be multiplied by s minus lambda one."
"And then we had also talked about how, if you have any general differential equation, or this expression over here. going to be equal to this quantity here, which is a rational fraction. The numerator and denominator are both polynomials in S, and therefore it stands to reason that we have to know how to deal with these expressions and simplify them so that we can invert these Laplace transforms. So that led us to this thing called partial fraction expansions, where we said we could take a rational fraction where we have ratio polynomials and we could reduce it into a bunch of roots of A of S, the denominator here, and the numerator has a constant which is called a residue. So we said that this rational fraction can be written as this partial fraction expansion, and we were in the middle of doing the partial fraction expansion in the case where n is less than n."
"And then we said that we were going to start to compute some or to derive some properties of the Fourier transform, which are going to be critical for us using the Fourier transform. And so last time we derived that the Fourier transform is linear. And this is a really simple proof. And we also derived this time scaling property, which is that if I have a signal F of T and I either compress it by multiplying the T argument by a where a is greater than one. Or I expand it by multiplying it most playing T by an a that is lesson one. And the transform will also become scaled. And so I want to just recall the intuition of this to make sure that we have. Have intuition over the results that we're seeing. And so this is my signal F of T. Right. If I compress my signal, which means is less than one right my signal might look like this. Sorry, that is less than one means to extend the signals. Sorry, let me if we expand our signal. So the a is less than one, then our signal might look like this. And then if I compress my signal a is greater than one, then my signal might look like this."
"And then we said we had the monkey or the human do experiments. And at every single point in time, we recorded the number of spikes that neuron 1 fired and the velocity of the cursor. And so this could be for k equals 1. Here for k equals 2, we have this neural activity and this velocity for k equals 3. We have this neural activity and this velocity. And we want to learn a relationship that tells me how to get the x at time k from y1 at time k. So we want to learn some function f that tells me the x at time k from y1 at time k. Any questions here? All right. So last lecture, we decided we would just go with a simple veneer model. And that's what we're going to do in this class. We'll talk about later on in this seminar lecture how to make this non-linear, which is actually very easy after we do this simple veneer example."
"And there's a picture you can Google it up from this clinical trial of another participant doing the same fist bump thing with President Obama. And that's I think really cool. All right, I just to also convey that there are really interesting problems that come up with brain computer interface control or brain machine interface control. Here's another video that you might find interesting. Help with some of the things that Jan has trouble with. Okay, for example, sometimes when she looks right at an object, she can't grab it. Okay, I went to take the cone away to go ahead and close it. Sure. So that's the cone away. There's no problem. But as soon as I put the cone there, she can't do it. Why is still a mystery. All right."
"And there's an imbalance of these ion concentrations at rest. We're going to talk about how those concentrations are set this vector, but because there's an excess of positive charge outside and an excess of negative charge inside and positive and negative charges attract, right? Then we're going to have a sheet capacitor form where these positive and negative charges are going to try to get as close to each other as possible. All right. And so this is going to cause there to be an electric field that points into the cell. And last lecture, we had done this poll question where we said, let's say that we open up a so-called ion channel and the mechanics of the ion channel will also get into more detail this lecture. But this ion channel is unique in that it lets through just one ion. It lets through Na plus ions. And Na plus ions are more concentrated outside the cell than inside the cell. So 10x more outside than inside. We asked the question, if I were to open up this ion channel, so now that Na plus ions could flow through, we asked the question, are they going to flow into the cell or are they going to flow out of the cell?"
"And there's going to be a poll that we send out tonight that let's you indicate what times are, what times are available. And then from there, Tonmoi will select the time most students can make. All right. And then I just wanted to chat. Someone asked if this is a standard or daylight or standard time now. So we just had our change of time this past weekend. Okay. Any class announcement, logistic questions? All right. Great. So we're going to get back into material. So last lecture, we spent quite some time talking about the intuition of Fourier series and then also deriving the results of the Fourier series, which is that F of T. If it's a periodic signal with some period big T zero."
"And these two equations, they had parameters A, C, and then the covariance of WK-1, which is big W and the covariance big Q. All right. And we said if we're given neural data, the YKs and the kinematics, the velocities, the XKs, how do I take this training data and use that to learn A, C, W, and Q. And so last lecture, what we did is we did the same thing that we did in for homework number four, which is that we wrote the likelihood of the dynamical system, the linear dynamical system by writing out its graph structure and then writing out the joint probability of observing all the data given the parameters. And we saw that because of the graph structure, where there are these conditional independencies, the likelihood factorizes into essentially three probabilities, the probability of X1, which we said was going to be a normally distributed random variable. And then a bunch of products of P of XK given XK minus 1. And this had a normal distribution as well as the probability of YKs given XKs. And this also had a normal distribution. So because everything has a normal distribution, we were able to write out the likelihood and then we were able to differentiate it with respect to AC, W, and Q to get the optimal parameters. And this slide here is a summary of what that derivation was. And so we talked about if you had your kinematics data X and your neural data Y and you can catenate them for all time, then these are the equations that give you the best AW and AW, C, and Q. And that completes the training process. So I want to pause and ask if there are any questions from last lecture on the training process of the common filter parameters. All right. So now at the end of last lecture, where we left off was that we are now going to get to the testing phase. So from training, we've learned AW, C, and Q. And now I want to predict how to decode the optimal velocity or how to decode a velocity XHK given all the neural data I've seen thus far from Y1 all the way up until YK. So in homework number four, for a given trial, we just had one neural observation, which are the bin spike counts in some window. And we want to calculate the probability of every single class. And there were eight classes for eight different directions I could choose."
"And these two equations, they had parameters A, C, and then the covariance of WK-1, which is big W and the covariance big Q. All right. And we said if we're given neural data, the YKs and the kinematics, the velocities, the XKs, how do I take this training data and use that to learn A, C, W, and Q. And so last lecture, what we did is we did the same thing that we did in for homework number four, which is that we wrote the likelihood of the dynamical system, the linear dynamical system by writing out its graph structure and then writing out the joint probability of observing all the data given the parameters. And we saw that because of the graph structure, where there are these conditional independencies, the likelihood factorizes into essentially three probabilities, the probability of X1, which we said was going to be a normally distributed random variable. And then a bunch of products of P of XK given XK minus 1. And this had a normal distribution as well as the probability of YKs given XKs. And this also had a normal distribution. So because everything has a normal distribution, we were able to write out the likelihood and then we were able to differentiate it with respect to AC, W, and Q to get the optimal parameters. And this slide here is a summary of what that derivation was. And so we talked about if you had your kinematics data X and your neural data Y and you can catenate them for all time, then these are the equations that give you the best AW and AW, C, and Q. And that completes the training process. So I want to pause and ask if there are any questions from last lecture on the training process of the common filter parameters. All right. So now at the end of last lecture, where we left off was that we are now going to get to the testing phase. So from training, we've learned AW, C, and Q. And now I want to predict how to decode the optimal velocity or how to decode a velocity XHK given all the neural data I've seen thus far from Y1 all the way up until YK. So in homework number four, for a given trial, we just had one neural observation, which are the bin spike counts in some window. And we want to calculate the probability of every single class. And there were eight classes for eight different directions I could choose."
"And they'll want to be driven out by the diffusion because K plus is much more concentrated inside and outside. And so if I just told you I opened up this channel, you couldn't say if K plus is going to go in or out because the two currents are across each other and they both contribute. And so to actually answer this question to break this, this individuation, I would have to give you more information about the strength of those currents. And so, well, I'll show some equations later on that get it when these two aren't equal every. Yeah, thank you. Great. And a question from David. Yeah, is the opening a permeability or is there actual physical opening? The opening is believed to this still in the open area of research is believed to be a changing confirmation of these ion channels, which can be triggered by different processes that we'll describe later on today."
"And this center holds queue disappearing after wish the monkey then knows that it's time to reach to the target. All right, and so there are two types of activity here. There's plan period activity, which is the neural activity during the monkeys plan to reach toward the target. And then the actual movement period, which corresponds to the monkey's actual physical reach to the target. All right, and last lecture, we said that for this first discrete communication prosthesis we're going to build, we're going to decode off of the plan period activity. So why is this well, we will do movement period activity, brain, brain computer interfaces next. But you can imagine if you were looking at say a keyboard, right? And we had the characters, etc. If I wanted to type on this keyboard and I were to move my hand, I would have to reach from location to location to type out a word. But what if I just imagined that I wanted to make a reach to the w or reach to the p or reach to the t, right? I wouldn't have to actually make a physical reach. And just across the planning it, that is represented in motor cortex. And you can imagine that this type of prosthesis could be much faster because if you have a discrete number of selections that you can make for the alphabet, it would be one out of 26 potential selections. And you can just plan to the t, plan to the age, plan to the e and you can type faster. Then if you were to physically make reaches to each of these target locations."
"and we call omega naught is equal to two pi over big T. Right? And so an impulse train in the time domain has a Fourier transform, which is another impulse train in the frequency domain. And because we know that in time or to sample a signal in time, what we do is we multiply that signal by a Delta train. Right? If I sample a signal in time, if your signal is sampled in time. That means that in the frequency domain. The spectrum of the signal is going to be convolved with an impulse train because multiplication and time is convolution in the frequency domain. And so this is this periodic sampling duality that we derived last lecture."
"And we called this current, let me draw it in the color purple. This is called the fusion current. And it's the chemical driving force. All right. Okay, I wanted to just recap this and ask if there are any questions on this. Because this will be an important principle for the rest of today's, for some topics in later slides this lecture. All right. No questions. So then we talked about how we're going to view the action potential from multiple different levels and give some concepts about the action potential that we would unpack it bit further. And the first is that if I were to have Na plus go into the cell, right, if Na plus we're going to go into the cell through these ion channels with both the electric drift current and the diffusion chemical driving force pushing Na plus into the cell, when I have positive ions going into the cell, that's going to increase the membrane potential."
"And we derived that C k was equal to one half sink of K over two. All right. And so these C k are going to be these coefficients C k for the square wave. And what this means is that for this square wave, if I wanted to write the square wave as the sum of complex exponentials, then I could write the square wave, F of T, as being equal to the sum from K equals minus infinity to infinity of C k e to the negative J, that's right, e to the J k omega not T. And here again, C k is one half sink K over two. And so this equation, sum from K equals minus infinity to infinity, where C k is one half sink of K over two times this complex exponential e to the J k omega not T. All right. This expression here is going to be the sum of complex exponentials such that when they sum together with these coefficients, one half sink K over two, they add up to the square wave. And so then I showed you some simulations. I'm going to paste this."
"And we predict that cursor's movements, those movements are entirely nearly driven. And so when we really decode the neural activity to get the kinematics, we put a hat over it to denote that it's decoded and the decoded kinematics will be x hat k. So similar to before in the training phase, I want to learn a relationship between yk and xk. That's my training phase. In my testing phase, now I only get yk the neural data. And I want to predict what the kinematics of that cursor are x hat k. Sorry Jonathan, I'm trying to get it again. So do you think that 25 millisecond ad hocly or that's the amount of data that's required to decode this kinematics with high fidelity? Yeah. So great question Tomway. So the 25 milliseconds we choose from empirical results, so actually in empirical results, it gets even your BMI, your decoder performance gets even better as you make dt smaller and smaller."
"And we see how information from past generations, for example, the ancient Greek, like the writings of Aristotle, Plato, Socrates, even ancient texts, texts like the Bible, for example. Are all things where we have information about what happened in prior times and, and are able to build off of that information subsequently. See a question quickly. Sorry. I'm looking to see how to get on mute. Melissa, can you unmute? Yeah, I can unmute now. What is the structure of your recorded? I guess the lectures will be recorded and they'll all be uploaded to CCLE. Any other questions? All right. So again, as we're able to write down information, we can now propagate that to future generations, but not everyone had access to these writings. And so another major part of advancement in technology was what I'm showing here, which is a good and bird press. This printing press allowed essentially written down information to be democratized and to be accessible to many people, allowing ages such as the enlightenment where we really saw an explosion and ideas and information. One very important, one very important advance towards modern technology that we know of today was the ability to begin to write down and store information electronically."
"And we showed some videos of this decode. I'm going to show the video just one more time for the. Medium and fast BMI trials. Purpose is just recollect this and then happy to take any questions on this. And so this is for the medium speed trials. And so you can imagine there's an optimization game that can be played here. I could try to decode even faster in which case. The amount of time that I have to look at the plan activity and then decode it to be one of the K targets become shorter. Then I have less data and I would be more inaccurate. And so there's a trade off that you can get here between accurate accuracy and speed. And last I could also show this video here where the trials were very quick. All right. All right. So any questions here? Any questions on the BMI task? Sorry. I keep on the neural communication prosthesis or brain computer interface, the CI task. And sometimes I say BMI also, which is brain machine interface. So I use those terms interchangeably here. Any questions on this task?"
"And we're going to be using them extensively in this class. So it's worth reviewing some of their properties. So a cosine is typically written in this way. You'll see it written two ways, actually. It'll be A times cosine omega. That's this discrete letter T minus theta. Or you'll also see it written as A cosine 2 pi f T minus theta. And so omega is, in this case, equal to 2 pi f. And the difference between these is whether we're talking about frequency or angular frequency. So omega is called an angular frequency. And it has units of radians per second. Whereas f is called frequency. And it has units of hertz, which is equal to 1 over seconds. Now, how we relate the frequency to this concept of the period, big T0 is the following. The frequency is equal to 1 over big T0. Where big T0 here is a fundamental period of the signal. So if the signal repeats every one second, big T0 equals one second, then the frequency is 1 hertz. If it's faster and it repeats every 0.5 seconds, then the frequency is 2 hertz. So it happens, it repeats more frequently. And then when we convert it, angular frequency, we just multiply everything by 2 pi. All right. So let me draw up the sine wave. Or we'll do the cosine wave here looks like. This is, hopefully, review for most of you. So if I have a cosine wave, and I'll just draw this following. There are a few terms in this expression that we need to point out."
"And what you want to know is this audio wave goes into the air, last cochlea. And then eventually the air, the hair cells in the cochlea are going to send information about the sound wave being heard to neurons in the temporal lobe. Right. And so let's say that it sends it to a neuron here, which I just denote by a circle in the temporal lobe. All right. And so ideally we can just record from this neuron and then we would be able to treat a relationship between the frequency of sound heard, the stimuli and the neuron. All right. But it actually isn't that simple. And so one of the first challenges that we'll find is that spike sequences reflect both in intrinsic neural dynamics and temporal characteristics of the stimulus. So temporal characteristics of the stimulus, this just refers to how this stimulus is changing over time."
"And when sigma is less than zero, then it's a decay exponential through time. So now what we can do is we can start to combine these signals together. So one thing I could do is I could take a cosine omega T plus theta that's here in this solid line. And then I can multiply that by E to the sigma T, which if sigma is greater than zero, corresponds to a growing exponential. Right. So now E to the sigma T multiplies this cosine. And so each of the sigma T can be thought of as changing the amplitude of this cosine over time. And so sigma is greater than zero, meaning that our exponential signal increases. Then it's slowly going to cause this sinusoid, this cosine, to also increase an amplitude. If sigma was less than zero, then the cosine would be large at negative time and then decrease our damping in the amplitude. And so one common terminology is that this exponential, which essentially sets the amplitude of the cosine, is sometimes called the envelope of this signal because it envelops the signal."
"Any other recap questions here? That's a great question, Brandon. All right, and so the last slide that we ended on last time was talking about the nodes of RON-VA, which is how action potentials can propagate over very long distances, even though the voltage is attenuating and can convey that information with high fidelity. And so because information is not certain, the amplitude of the action potential, only on the fact that action potential happened, we learned that the nervous system uses these nodes of RON-VA to essentially regenerate the signal every one to two millimeters. And so at first off, we talked about how the mile in sheep's, what they do is they effectively increase the width of the cell wall. And therefore, they decrease the capacitance because now our positive and minus chargers are further away from each other. This distance here has increased. I think I called this, oh yeah, deep. And so because of this lower capacitance, there is faster propagation."
"Any questions before we start with material? All right. So, we're going to continue where we left off. Last lecture, we spent some time finishing discussing and proving much of these properties about Fourier transforms. Right. And there is just one more property will get to today which is the integral of a signal what is its Fourier transform and we'll derive that today. And we've also along the way derived a bunch of Fourier transform pairs. step, or rather, we'll discuss how it's derived. We won't test you on it because it's a generalized Fourier transform that we don't want to emphasize too much. But in general, when you take the exam or do the homeworks, we'll provide a Fourier transform table that contains properties as well as Fourier transform pairs and you're welcome to use all of these without proof. All right."
"Any questions here? All right. So then we could also have a complex exponential. And so a complex exponential is of the form E. And then it has a purely real part, which is the sigma T component. That's the growing or decaying exponential. So this is equal to a growing or decaying exponential times our complex sinusoid, e to the j omega t. And so what this would look like is a complex sinusoid, again with the real component being this cosine, the dotted line being the imaginary component. And so the solid and dotted line are what you get from e to the j omega t. And then this amplitude is going to be scaled by e to the sigma T. Any questions here? All right. So I'm going to now show you a picture which will return to later on in this class, which is if we have a signal x of t equals e to the sigma plus j omega t, the exact same signal that we had here."
"Because with 50 spikes per second, I got that there would be 20 millisecond intervals. Got it. Yeah, so 20 milliseconds is the average interval. But if you just want to find the percentage of spikes that would violate a one millisecond refractory period you actually only have to compute the first probability, because you want to divide that by all potential spikes, some of which may have a inter spike interval that's greater than 20 milliseconds. And so you would only want to calculate the the first expression that you mentioned there. Let me just pull up my iPad, and a share screen here so I can just. I can just write out everything so that I make sure that we're all on the same page. Okay. All right, yeah, so for question 3b, just like you were saying, we would want to calculate the probability that our exponentially distributed variable has. We want to calculate the entry, the area under the curve from zero to 0.001 seconds which is one millisecond. And so, all you would want to calculate, we want to divide this by the probability of the ISI, of all potential ISIs, which is just, would just be the probability that t is less than infinity, but this denominator here would just be equal to one, and so we would just want to calculate And we derive that this is one minus the XP of land to T."
"Bradley? Yeah, that's what I did. Perfect, yeah. And then we have, so if we have a minus sign here, then we're going, well let's try to then get the imaginary part into a C minus K term. So can someone tell us what to do for the of CK. Wait, I have a thought about what you just wrote down. If you, if in the green you equated the real part of CK to the negative of the real part of C negative K, is that equality correct? Should there be two negatives if it's more equating to the real part of C of K? Oh, you're right, yeah, it should just be minus real part of c minus k. Yep. That's my bad."
"But for part C. If we want to know the probability that electrode one detects a neuron. So that would be the probability that a neuron is detected on electrode one before electrode two, right? That's saying that at some point in time, N1 of T is equal to one because we've detected a neuron on electrode one, but electrode two has not yet had a neuron. So, n two of T is equal to zero. Does that make sense. At a high level. Great. Yeah, so then the caveat here is that we want that spike to have only been for electrode one and not electrode two. So, n1 of t equals one, n2 of t equals zero, but we also want that overall there's only just been one spike."
"But we hope that the piazza form will be lively. And like I mentioned last time, we give bonuses based off of participation on piazza. And so even though you can consider anonymous to classmates, your posting is not anonymous to us. And we make that setting so that we can assign bonuses based off of how much you participate on piazza. And the TAs will be checking piazza also regularly to make sure that any questions that couldn't be answered by students can be answered by teaching staff. If you have a question that isn't appropriate for piazza and it's related to class material, I just asked that you email, Tonmoys, Shashank and I and me together. We do this so that no single TA gets overloaded because in prior classes, sometimes once TA is very responsive and then all the students learn to email that TA all the time and that TA becomes overburdened. And so we just asked that you send us any class related questions to all three of the teaching staff if you don't post it on piazza. And then of course, if you have any personal matters, please email me directly there. Any questions here? All right. Some last notes. I think we mentioned this last time."
"But we hope that the piazza form will be lively. And like I mentioned last time, we give bonuses based off of participation on piazza. And so even though you can consider anonymous to classmates, your posting is not anonymous to us. And we make that setting so that we can assign bonuses based off of how much you participate on piazza. And the TAs will be checking piazza also regularly to make sure that any questions that couldn't be answered by students can be answered by teaching staff. If you have a question that isn't appropriate for piazza and it's related to class material, I just asked that you email, Tonmoys, Shashank and I and me together. We do this so that no single TA gets overloaded because in prior classes, sometimes once TA is very responsive and then all the students learn to email that TA all the time and that TA becomes overburdened. And so we just asked that you send us any class related questions to all three of the teaching staff if you don't post it on piazza. And then of course, if you have any personal matters, please email me directly there. Any questions here? All right. Some last notes. I think we mentioned this last time."
"But we hope that the piazza form will be lively. And like I mentioned last time, we give bonuses based off of participation on piazza. And so even though you can consider anonymous to classmates, your posting is not anonymous to us. And we make that setting so that we can assign bonuses based off of how much you participate on piazza. And the TAs will be checking piazza also regularly to make sure that any questions that couldn't be answered by students can be answered by teaching staff. If you have a question that isn't appropriate for piazza and it's related to class material, I just asked that you email, Tonmoys, Shashank and I and me together. We do this so that no single TA gets overloaded because in prior classes, sometimes once TA is very responsive and then all the students learn to email that TA all the time and that TA becomes overburdened. And so we just asked that you send us any class related questions to all three of the teaching staff if you don't post it on piazza. And then of course, if you have any personal matters, please email me directly there. Any questions here? All right. Some last notes. I think we mentioned this last time."
"But when he reaches the left, the plan activity stays low. For up left, it also stays low. And it seems to increase more when you go from a leftward reach towards a rightward reach. Okay, any questions there? Alright, so this is going to be the key feature that we use to then do a decode. Because now if I record from this neuron and I ask and I don't know what target the monkey is planning to reach to, but I record from this neuron and the neuron is quiet since it isn't firing very much, then I can guess that the monkey is probably going to go left or up left or up. But if the monkey if the neuron is firing many spikes per second, then I'm going to guess that the monkey is planning a rightward reach or an upright or a downright reach. Okay, any questions there? Alright, looks for the graph on D. Yeah, looks down like on sort of line on the top. Oh yes, this is showing the monkey's hand position. I think it's showing his X hand position. So this is the X position of his hand. And so it's showing that he starts off in the center. And then when he actually makes this reach up into the right, that his X position is going to increase rapidly and then go to a steady state value. So it's going to increase until it stops here at this X position corresponding to where this target is. Any other questions? Alright, so now we are recording from many neurons with this Utah ray, we get 96 neurons."
"But, strictly speaking, we're not doing marginalization out of a random variable. We're still calculating the probability of two events here. We're not taking the probability of two events and then marginalizing out one of them. So that's why I said it's not the law of total probability. Okay. Okay. But it seems the calculation only makes sense in this kind of way. Yeah. Yeah. Yeah. So this is a reminiscent of the law of total probability. So we can transform this less than or equal to sign here into the integral over a density."
"by multiplying it by my impulse train which are impulses separated by time big T. What that means is I'm going to take my spectrum for FFT, so here I've just drawn this as a red triangle here. And then I'm going to convolve it with an impulse train which means that I'm going to create replicas of it at every omega naught. Alright, so I might pause and see if there any questions for this recap from last time. Alright, so Okay. So, this is the slide that we ended on last time, which was to say, if we take a signal at 50 and sample it by multiplying it by the impulse train. Then, the Fourier transform of the signal is going to be the Fourier transform of FFT replicated every integer multiple of Omega naught. We saw from this that we can deduce the intuition for what rate we have to sample at. So, if Omega not is big."
"Call from wireless. Sorry. Okay, get someone else picked up. So. If the amino acid has low energy binding site. Yeah, the E in this slide is energy or electric field energy. Sorry. Yeah. Okay. Energy. So if there's a low energy binding site, remember that to pass through it has to shed its waters of hydration and NA plus holds this waters of hydration more closely because it has the same amount of charges potassium but in a smaller. And so NA plus holds many more waters and so if the energy site is low energy, then it's not favorable for it to release all of its waters around it. However, potassium has relatively less water around it. And so it can shed its waters to bind to a low energy binding site. And so that's how K plus can pass through a low energy binding site but NA plus cannot. Does that make sense? Yeah, I'm just thinking about how it works like. Does like the high energy binding site kind of takes the iron out of like the water shell or."
"Can I see? Yep. Okay, I'll set it in for part F. So the right for part G. So this is where we're plotting the ISIS distribution and I know we'll probably use like a histogram and we use the exponential. So I was able to kind of get it going. And then I guess my issue was not really so I know like the political swell that was applicable for one of the previous parts. So for fear because it's normalized, I saw like a Python on the other some post there's like a setting where you said like area equals one or something equals true. Yeah, norm equals true, I think. Yeah, so we do PLT dot hiss. And the first argument we just give the ISI the list of ISIS or the array of ISIS that you have and then common norm equals true. Okay, gotcha. And then we did give also a common dint equals 30 to have a plot to have it make a histogram with 30 bins. So I see that makes sense. So see originally I was like for each of the ISIS I was looking at like mainly created like 50 bins and then I was you know, say, okay, this less than this, creating this, I put in this pile. I was trying to like a bar PLT dot bar kind of from scratch. So for this is we want to have to really do any dividing and sorting it kind of does it for us."
Can you all hear me? Can you all hear me?
"Can you explain again why, since they're both thickness of like 1000 micrometers, right, when you do the calculation, can you explain why it's better to have the mile and sheath than versus the bigger diameter? So I'm just wondering how to, how to show this without showing the actual answer by sharing my screen. What you should see is that. The other one in in we accept both solutions so in one case you might find that you have in a little are for the radius in the diameter, sorry in the denominator, or else, or else, a radius squared term in both cases, it's the what the neuron that has more myelination is going to get a much bigger change in speed, since you're changing one of the denominators by a factor of 25, compared to about a factor of two, or a factor of four if you had the R squared in the denominator. or four if we had the r squared in the denominator. Okay, thank you. That helps a lot. Okay, great, yeah. All right, while we're on this, can anyone unmute if you have any further questions on question number five and then after that we'll continue to go down the list. I actually had a question on this one as well. Okay. This one is like the last part of five."
"charge internally will also, I guess, we can electric field. That's correct. Yeah, and that's why the. And that's why in the picture they go from like three plus signs here to just two plus signs to indicate there's less positive charge outside and I get therefore since the voltage is at the voltage was at zero, there would be no electric field, the voltage has gotten closer to zero so the electric field is weaker. Okay, and then the, sorry, the blue, is the blue line the electric field? The orange line is the electric field. Okay. The orange line is the current due to the electric field. Okay, got it. Thank you. Great. Thanks for the questions. Professor. Yeah. Hey, Brandon. Oh, I should also mention something I just looked it up, grace as to because you have the question about the Ion channel that I didn't know the answer to. And the reason why it's called hydration is because this gap is too narrow for it to pass with this waters of hydration, not because of the hydrophobic forces. Okay, thank you."
"Cool. Arthur. Hi, Professor Tao, I had a question on number five on the homework really quickly. Great. Let me just pull it up. One second. Sorry, so the homework is just downloading for me right now. It's taking some time. I'm not sure why it's taking so long. Let me just try to save the pdf. Cannot be opened. Okay, sorry. Let me just open up an old homework number five from homework one from prior year. All right. I wanted to ask about like for the myelin sheets for like part B or does the myelin sheets around the axons and neurons strictly improve the conductance of the action potentials or is there like drawbacks to it? Yeah, so for, Sorry, you're saying this is for 5b, right?"
"Do you understand what I'm getting at? I'm not 100% sure. Jonathan, let me just draw diagram first because maybe this helps. And then if this doesn't help, then let me address your question again. So if we have spikes, what we do is, if this is zero, time zero, this is time 25 milliseconds, this is time 15 milliseconds, this is time 75 milliseconds. So this activity between zero to 25, we'll call Y1, Y at time 1, this activity will be Y at time 2. And so we just increment every 25 milliseconds and there we count the number of spikes, we get the firing rate, and they just go in 25 milliseconds increments. Okay, that's fine. Thank you. Okay, great. Jonathan. Yeah, it's on one."
"Energetics is a mix of energetics and size. So for the potassium channel is energetics because the lining side is low energy enough such that sodium will not shed its waters of hydration. But then for the. For the sodium ion channels, which have a high energy, binding site is also size related. Since potassium is bigger and can't pass through. I see. Could you explain again how the energy thing works? I'm not very familiar with all the dining stuff. Yeah, of course. So here's that slide. So what happens is that. For a low energy binding site. So that could be like a binding site to a carbonyl like this oxygen that doesn't have an explicit negative charge like this one is a high energy binding site. But binding to this carbonyl group would be a low energy bonding site because it has a slight negative dipole. But not an explicit negative charge like this. Oh, mine is here. Sorry, let me just see if I can silence this one."
"Essentially the common filter is a technique that is applicable whenever you have time series data and there's some dynamics or there's some structure across time where you want to model that. And so anytime when data is modeled to evolve through time in principled ways the common filter can help you incorporate that evolution through time those dynamics through time into your actual inference, interior predictions, interior decoder. So it's a tool that hopefully that isn't just useful for this class but could hopefully be helpful for you in other context applications or classes. All right. So we are going to go into these next slides I covered the common filter to have very high intuitive bubble and we're going to provide you with the algorithm so that you can code it up on homework number six and then after that we're going to go into the detailed derivation of the common filter because as we'll see the algorithm for the common filter the equations at least look very complex and so when we derive them we'll see where they come from and of course that they are not magic."
"Essentially, I want the first term and the second term to cancel out. The first term and second term look very different right now. This is zero to T over two. T over two to big T. And so let's take Kai's recommendation and do a substitution. So I'm going to define a tau. And I want this tau to move these integration down to zero to T over two. So I'm going to say that tau is equal to T minus big T over two. So if tau is equal to T minus big T over two, then I can apply this substitution to this integral over here. So let me write out the next step. This is zero to cap T over two. X of T e to the minus J to omega not T and T plus an integral. And now I'm going to do my substitution. So I'm going to be integrating over a detail. And because I'm integrating over detail, my integration bands are going to go from zero to T over two. I'll have a zero to cap T over two. And I'm going to have an X where now everywhere there's a T, I'm going to put tau plus big T over two."
"Eventually, you could decode kinematics using the equation x k hat equals m1 times x k hat of time k minus one plus m2 times y k. And that's how the common filter uses the m1 matrix, so we pass kinematics and the m2 matrix to consider the current observation, the neural data. Any questions on any setup of the common filter, any part of the derivation, any part of intuition on the common filter? All right. So in homework number six, you all implemented the common filter and we're able to calculate the mean square error. And now show you the video of what the common filter looks like when we ask one of the monkeys to control it, and early in real time. So I'm just waiting for the zoom when they catch up with my eye. Okay. So this video is going to be the vanilla common filter that you all implemented in homework number six. I'm going to bring the same task that we've shown before where his goal is to reach to one of the targets, but if he hovers over any target for 500 milliseconds, it's going to select that target. And if he selects a target that is incorrect, then you'll hear the negative sound, but if he selects the correct target, you'll hear a positive sound. Okay. I think maybe the sound isn't working for this video. I apologize. All right. So 20 trials are so in. We see the success rate. He's over 90% success rate. The bit rate is 2.2 bits per second, which is respectable, but actually less than the winner filter. So looking at this video, either in the chatter, if someone could raise their hand, could you tell me what you feel are the pros and cons of this decoder in terms of how it looks? Yeah. William. Kind of feels like the movements are too big every single time, like, yeah, over shooting."
"Eventually, you could decode kinematics using the equation x k hat equals m1 times x k hat of time k minus one plus m2 times y k. And that's how the common filter uses the m1 matrix, so we pass kinematics and the m2 matrix to consider the current observation, the neural data. Any questions on any setup of the common filter, any part of the derivation, any part of intuition on the common filter? All right. So in homework number six, you all implemented the common filter and we're able to calculate the mean square error. And now show you the video of what the common filter looks like when we ask one of the monkeys to control it, and early in real time. So I'm just waiting for the zoom when they catch up with my eye. Okay. So this video is going to be the vanilla common filter that you all implemented in homework number six. I'm going to bring the same task that we've shown before where his goal is to reach to one of the targets, but if he hovers over any target for 500 milliseconds, it's going to select that target. And if he selects a target that is incorrect, then you'll hear the negative sound, but if he selects the correct target, you'll hear a positive sound. Okay. I think maybe the sound isn't working for this video. I apologize. All right. So 20 trials are so in. We see the success rate. He's over 90% success rate. The bit rate is 2.2 bits per second, which is respectable, but actually less than the winner filter. So looking at this video, either in the chatter, if someone could raise their hand, could you tell me what you feel are the pros and cons of this decoder in terms of how it looks? Yeah. William. Kind of feels like the movements are too big every single time, like, yeah, over shooting."
Exactly. Is that a question I'm prone to? Is it OK if I share my screen like the plot? Yeah. Let me go ahead and I'm going to tell anyone who's watching this after. I'll just pause the recordings during share screens because it shows the students code and whatnot and so I want to just be sensitive to that. So I'm going to pause recording now.
Exactly. Is that a question I'm prone to? Is it OK if I share my screen like the plot? Yeah. Let me go ahead and I'm going to tell anyone who's watching this after. I'll just pause the recordings during share screens because it shows the students code and whatnot and so I want to just be sensitive to that. So I'm going to pause recording now.
"Feel free to either raise your hand or write it in the chat. Great. Okay. So Andrew says explain that cosine is bounded by negative one and one. Okay? How on with the same thing? So yes, we know that cosine omega c t this is a function whose maximum value is one and whose minimum value is negative one. And so for and therefore the absolute value of cosine omega c t has to be less than or equal to one. Right? So that means that I can write that this is less than or equal to the absolute value of x t times the maximum value of this term and the maximum value of this term is just equal to one. Okay? And then I'm going to use my last relationship here. Here I have assumed that x of t is bounded so x of t can be written as less than some m x. And so therefore this term absolute value of x of t is also less than or equal to m x. And so this is less than or equal to m x. Right? So we show that as long as x of t is bounded, then when we plug in the algebra for this system, we also get that y of t is less than some constant m x. And therefore y of t is bounded. And since bounded x of t leads to a bounded y of t, we can say therefore a m radio. The a m radio system is stable. Right? Okay, any questions here? All right, no questions. So then I'm going to go on to the square. So the square is y of t equals x square. And so therefore I can write that the absolute value of y of t is equal to the absolute value of x squared of t. Right? And this is equal to the absolute value of x of t squared. All right? And I want to show that the system is by those tables. So I'm going to again assume that x of t is less than or equal to m x. If that's the case, then this expression, the absolute value of y of t is less than or equal to m of x squared, which itself is some finite constant. Right? It's a number of times itself, m x times itself. And so since I can bound y of t by a constant, then y of t is also non infinite. And therefore I can say that this system, the square is also by those tables."
"For anyone who didn't recall when we did this slide in lecture. Copy. The derivation of these is exactly analogous to the oops The derivation of these is exactly analogous to the derivation of these properties when we did the Fourier series. Maybe, maybe I'll do the first, I guess there are eight so I'll do the first four to get us started on this and then, and then I happy to talk about the other ones but actually, I'll do the first four, maybe a few less if people have questions but I don't see too many questions right now so why don't I go ahead and do the first four. So 2A1 asks, determine which of the signals whose Fourier transforms are depicted in figure 1 satisfy the following, x of t is even. So then we will look at our Fourier transform properties, and if x of t is even, then we know that capital X of j omega is even."
"for C1, this would correspond to the complex exponential e to the j times 1 times omega naught times t. So it'd be the complex exponential that has a frequency of omega naught. This omega naught is equal to 2 pi because omega naught is 2 pi over the period big T and big T here is equal to 1. And so that means that the Fourier series coefficient for the frequency at omega naught, which is two pi, is going to be equal to sinc of one. And sinc of one is just equal to zero. All right, and so the Fourier series coefficient at omega two pi is just going to be equal to zero. And it's the same thing at four pi, six pi, etc. The Fourier series coefficients are all equal to 0. All right, so if big T equals 1, all the Fourier series coefficients are 0, except for the 0th Fourier series coefficient, C0, which is equal to 1. Okay, so I asked at the end of last lecture does this make sense that if my Fourier series coefficients are zero except for C zero, then that means that my signal is just equal to one, because we know that f of t is equal to some of CK each of the JK, But now everything is zero except for C zero. And so we would just have a C zero times e to the J times zero times Omega naught times T. And this would just be equal to C zero which is equal to one."
"For his Wednesday office hours, I wrote it was from 9 to 11 p.m., but of course, that's after the final exam. That should read 9 to 11 a.m. And so I've corrected the announcement on both CCLE and Piazza, but these announcements were also emailed out. So I just want to make sure that that correction is clear, that on Wednesday, the day of the final exam, Tom Boyd will have office hours from 9 to 11 a.m. All right. The logistics of the final exam are going to be very similar to the midterm exam. First, the final exam is Wednesday, December 16, 2020, from 3 to 6 p.m. We are going to upload the exam to CCLE at 2.55pm, again to give anyone who wants to print out the exam some time to do so, although you're more than welcome to do the exam on a separate sheet of paper. And then the exam must be completely uploaded to Gradescope by 6.10pm, so we're giving you 10 minutes after the conclusion of the exam at 6pm to scan in your exam and assign all of your pages by 6 10 p.m."
"For you we have to switch to the person process right for you sorry, what is the expected number of spikes that will be fired before one season, ISI, greater than the mean ISI. So, so this one actually requires some thinking outside the box beyond the process. I had it set up as like the expectation of an of s, given, t is less than one over lambda big T is less than one over lambda. Is that what I'm trying to solve for Okay. One piece of greater. So, I believe if you go that route. You."
"For, for, for C I mean sorry. Yeah, it says it on his notes. Okay, let me pull that up really quickly so that I can see what he said. It's the notes from... 4-20 or 4-18? It's the notes from April 20th. Okay. Oh, that's an E, not a C. Oh, sorry. I confused... Oh, yeah. Oh, okay, okay. Okay, that makes sense to me. Great. Yeah. Okay. And so actually for for E."
"Given any input and the way that I do that is I complete this convolution integral. And so this convolution integral again, and is usually, we talked about notation last week, or usually, you might see this denoted X star H of T, which is the rigorous notation. But usually in this class, we'll write this as X of T. Convolved with H of T. Since that's how most of the role that had a convenience. Right. So any questions on these concepts? All right. So we talked about the intuitions of what convolution is doing. Admittedly, this integral can be complicated. And so we talked about how to intuit it with this remonion sum slide, which is something that again, we encourage you to review those parts of the lecture videos and materials to make sure you understand those concepts. And we also talked about how to compute it using the flippin drag technique. And so last I left off lecture, we had given these example questions, and we had written the solutions of these convolutions. I'm going to do just one of them to recap how we do the flippin the drag method. But then I want to take any questions on what people, if people had any questions about how to solve these convolutions. Let's say I wanted to convolve this input X of T and this impulse response H of T. And so what I would do is I would take my impulse response. Here in green, and I would flip and drag it."
"Good morning. Can you guys deal with what we talked about in prior lectures from last quarter? Alright? Yes, Daniel. Daniel's question is he's been copying Dr. White House their cells instead. Okay. Fantastic. Yes, they're vomited TAs or equal to its documentation. Able to read that before. Okay. So last lecture, you're talking about batch normalization distraction. I'm going to have to convince me, ask you, so please don't distract other students. We're talking about batch normalization factor. And we talked about batch normalization is going to help with this problem called internal covariate shift. Where the issue is that we're going to be calculating gradients of the loss with respect to all of the weights. And for each of those weight updates, what we do in the training that we take. Sorry, I see a video there also. Please guide students here are paying tuition to take these classes. I know that this might be for some YouTube video or something, but please for the students I'm going to ask you. I'm working. I'm sorry. Can you please all right. I'm just asking why we didn't use it. Like Wow. All right. Sorry about that. Everyone wanted DMCA, YouTube. They do post it and it's not respectful. I appreciate it. Alright. Because he was asking you a question about not being a full tell you right now. If you took the fact that the TAs are very helpful. Alright, that was 10 min material. So we're talking about batch normalization and batch normalization. What happens is we're trying to help this problem with internal covariate shift where we update all of the weights and redo it, unfortunate or we do it at the same time. And what that means is that I may be making a change to W3 to try to make my loss folder. W2 might also be changed. And maybe when I compute this gradient, DLD or W3, all the values H2 or 0-1. It's on adjusting to try to handle this and the scenario where each switch 0-1. If I go ahead and I also changed W2, maybe W2 is such a large change and other values of H2 are determined by ten. And now my W3 doesn't matter that much. So what we wanna do is we want to reduce this problem of an internal covariate shift by normalizing the statistics. These hidden activations to be in the range of having zero mean and variance. And therefore, even though we're updating all these bits together, even though we're updating all these waves together, hopefully those changes that we did. So basically the more relevance to decreasing force. All right? Any questions there? Alright? Okay, so this moves the conversation. What we do is we take one unit at psi. So there's going to be both a subscript and superscript in these examples. Xii, try here is the neuron upper layer. So if we take a hidden layer and there are 100 units, that's ten, or x2 would be the second unit in this layer. And then the superscript j, x i superscript j is going to, the j is gonna go over exactly. Alright? So you can think of looking at this layer in the neural network. And we're going to have a mini batch of examples. They're going to be images from C4 prime images. And let's say that there are 100 examples, right? So we pass all those images. And for each example, we're going to have a corresponding activation of what this artificial neuron is during that exam. So that you can compute the mean and the variance. We can go ahead and across all of those examples. That's why there's the one over n sub j equals one to m average the activation and inactivation. And then the standard deviation or the variance is going to be the sample variances and across all of those examples. All right, any questions on that operation? And then lastly, even though this will normalize the means and variances to zero mean and variance one. We are also going to scale and shifts the normalized activations. This allows fashion occupies an additional degree of freedom where it says, you know what? My copy. A good idea to make all the activations have zero mean and unit variance, have different needs or different variants to capture different features of the image. And so it is better for the activations that have a different bean or different variants. We allow there to be two of them learnable parameters. So for every single unit, alright, we give it a gamma and beta are. The Beta is going to be how much we shift the mean by Gamma I is going to be how much we scale it by which we'll scale the variance by gamma xy squared. All right? So we're going to have a high level of batch normalization is going to first make the mean zero and variance one. And then after that is going to have to learnable parameters that could make them being different than zero and the variance difference of one. So desire. Any questions here? Great. So the question is during training, do we need to compute a new view of sigma i for every four tasks? And I forgot to look at the homework to see what we expected there, but we do make the trait and fast one UI and sit nice spread across the entire dataset. Or is it for each batch? Okay, so tomboy says for each batch, so for each back to the computer do view and a Sigma I squared. Any other questions? The question is why the label here, know why? That's a great question because I've used that before. Why did I here is the scale that the patients. So you can imagine that if I take this layer and I pass it through a batch norm, what is going to return is the same sized vector. But these are now all the Y-i. So this is y one, this is y two down to 100. So this will be the re-scaled batch norm. Pitchers are the more ions. Question is, how do you know what value to shifting scale it to? We don't know if that was going to be burned by the algorithm. So these will be displayed Iceland Gamma i's are going to be fit during stochastic gradient descent. The algorithm will in a data-driven way to get your losses gets possible. The question is, can I talk about why we then normalize and scale of the data? So you might ask this question because you think that is something that peaceful purposes having done cache warm. Because now the teachers can have different meanings in different areas. And so the reason or one way I think about this is what this operation does, is it biases and that works towards 01 activation saying, can you solve this problem with a nice or ovarian just want activation. And then if you can't, if it really helps to make the bearings different than what it can mean, different things in one than zero. But if the question is, do we initialize the beta and the gamma eyes? Yes, I believe that the TAs can check me on this, but we should initialize them to Beta I equals zero and gamma equals one, which means that it would be mean zero, variance one. The students question is, can we think of this as a two-step process which is first to normalize and then scale and shift. Yes. Alright, so with that last lecture, we left off with drawing out this computational graph of the batch operation. So here is the denominator of the Z-score, which is square root of sigma phi squared plus epsilon is done by this here. So the Sigma i squared plus epsilon I square root, I square root it and then I put it in the denominator, that's the inverse x I minus mu comes from here. And multiply these together and that gives me x psi hat. So that's this. And then I multiply that by gamma i. That's this node right here. Beta i, which is this node right here. Alright? Remember, in this computational graph that I've drawn here, there are. So remember that this is for just a computational graph for one unit. You didn't ride, right? Is just one of these artificial neurons in layer. And then just for one example j. So one example j, and that example will be one. This is a computational graph for one unit and one example j. What you'll notice in this graph is that everything has a subscript pi. So this graph will look the same for every single other unit as well. So on the next slide, I'm going to drop the subscript i for convenience because the subject I will always do that. Yeah. I'm looking at something up front, right? Yes. That's correct. And the axon is just a small number that prevents that the variances are owed. You don't want this to be infinity. So the epsilon is a small number that prevents that. Other questions, right? Yes. So Daniel asked when we talk about X-i, is that input or is that an actual neuron doesn't actual neuron. So let me ask you this, have this be for next year. That notation, we're gonna be using the exact same notation as in the IOC Saturday paper. So usually Zai was reserved for inputs and HI, in our class has been the given units. But here x, y will be the artificial neuron activity. And then y, instead of being labels are going to just be the output of the DashCon later. Alright. Any other questions? Okay, Let's go ahead and do this back propagation. So you're going to assume that we have some hops, Srini gradients, this upstream gradients, I'm going to call d L, d y. And then I'm talking to the subscript, I'll just keep the superscript case. So DL DYJ, right? And when we do backpropagation, we need to know how to backup to the parameter, my parameter to the beta and a gamma. And then we also need to know how to back propagate all the way to begin, right? Because these inputs, the x i's, are the actual values of the artificial neurons. In layer. I need to know their greatest so that get back up to earlier layers. So we're going to start off with are up-to-date. And the first thing that we're going to do is backpropagate one step. So we have a plus sign here, which means that gradient just passes through, right? So the gradient at this wire or at this parameter beta i, it's just going to equal d L d theta. Sorry. There's gonna be DLD y j. That's the backpropagate. Then you're going to notice something, which is, if I read the gradient DLD Beta, I get, I've just dropped the eyes for convenience. I have a sum for j equals one to n over all of the DLD Wij's. Whereas if I'm doing it this graph for just one example, I just have a DLT YJ. And someone tell me why the total gradient for DLD beta here is gonna be the sum across all my examples. This is not an easy question, so please take it to the comparator. Object does have to do with Watson of derivatives. It does. So remember that what I'm doing is I'm drawing this graph for one unit and one example, cherry, let's focus on the fact that I'm drawing this for one example. So let's say that my dad had 100 examples. I'm going to be running this computational graph, these computations for j equals one to 100. So because this is a graph of just one example j, but I have 100 examples. I'm going to have 100 replicas of this graph. So let's say I'm going to draw another one. Let's say I'm going to just replicate this graph. So I'm going to draw exactly how it looks, but not label everything. So what I've done here is I've just drawn a replica of this graph. This graph is going to also exist for unit I, j plus one example. And this is the output for the j plus one example. This graphic, this also for the j plus one example. However, when we look at this Beta I, write this Theta I is just one parameter that is the same across all examples. For all examples, I'm going to multiply by gamma i and then add beta, which means that this beta i, which is added at this node, It's also going to be added this node. So this is going to be the same exact data that comes from I, j plus one example to shift the activations. So there's also an upstream gradient, d L d y, j plus one that brought back propagates here to be beta i. And then remember we talked about how when you have a converge onto one parameter, other gradients, sad. So that's why DLD beta is going to be the sum of the losses across all of my example. These are, sorry, the sum across all of my examples of these gradients, DL, DYJ. Quick reminder letter I and j represents is going to be one unit in the network, meaning one unit in a layer. Meaning I is going to reference e.g. if I equals two is going to be the second artificial neuron. And then J or a trust my m training images. So if I have 100 C part-time images, j goes 1-100. Indexing does images. Yes. So Daniel is saying across the batch, is it the case that the same upstream gradient is obligated to pay the high? That is correct because that's going to just be the sum across all of my examples. Whereas when I back propagate, we're going to tell us to derive what the backdrop for the x j's the plank wholesome. And then just to be clear, moving forward, we don't have to worry about the price because there's a subscript eyes, because there's going to be the exact same graph for everything. But I'll defer it on this one slide. I said we're going to drop this off to apply for convenience. But indeed there is a Beta, a Gamma, there's an IBC be on everything. That's fine. Let's keep going on oh, actually, before I do that, any questions on this. So I'm told me to make sure that you all understand these because this has been, uh, there's been a barrier. If somebody asked you about this. Yeah, The question is, can I explain what task converge again? So basically, let me draw it in the following way. There's going to be a beta i parameter. Beta i parameter is always going to be added to gamma xy prime irrespective of which example RON, there's no j index. So this beta i is basically a parameter that sits here and it's the same data I added here to every single activation across my examples. And so the conversion paths are that beta i, which if there were 100 examples with branch out to all hundred of these graphs, is going to sum up the gradients DL DYJ from j equals one to j indexes the data instances with this index. Indexes which artificial neuron in our layer of yard. So we can just go ahead and set it for the rest of this class. For the rest of this lecture, I is going to equal to, so we're going to be looking at batch criminalizing. Just a second neuron in this layer. Split it up. All of our data is that we started off all of our data to every single word. So the question is, what is the difference then between the neurons? Because neurons in general have different weights, the upstream to the earlier observations will be different than one another. So because of the weight matrix has different weights for every different you're on the right. My question here. Here's the individual neuron with an array or just to, just to make sure that's clear. What is the batches of students question? So the Batch refers to if m2 equals 100, then I have a batch of 100 images. That means that I have 100 images of cats, dogs, airplanes, whatever. And for j equals one to 100 for each image, I'm going to pass that into the network, and that's going to cause x2 to have a different value. So u to the main activity of this second neuron is going to be the bean across my 100 examples, and that's the variance is gonna be the sample variance across those 100 examples are saying, then that means that each different batch will have a different view and a different Sigma squared? That's correct. Yes. Yes. Oh, sorry, I'm sorry, I misspoke there. So batch norm doesn't question, does dashboard happened after the non-linear activation? Nowhere. So today the answer is yes. People have done experience and experiments and they see that it's better to put back on after the acquisition box. In the homework, we're going to follow what IOP and said that he did in 2015, which pushes, which puts batch norm right before the non-linear activation function. Great. So Tim's question is, well, the weights and the biases will also change the mean and the variance of the data. So isn't this kind of redundant? The answer is that while in theory it is possible for the weights and the biases to have it set so that it learned mean zero variance one features. In practice, we know that the weights and the biases or just be updated by a gradient descent. So when do you can think of this? As? You can think of this is, you can think of this as a technique that biases or network towards variance 01. Because it's highly improbable that stochastic gradient descent would find that solution. We're going to see this comes up later on in an architecture to call residual network the price net. And that's another thing where even though in theory a neural network because of residual layer by adding it really helps to train the wall. Alright, so that's the backdrop to DLD data. The question is, why don't we learn the betas and gammas and then fix them as hyper parameters for future training. You couldn't do that. But it seems to be more constraint than if you just let the network to learn two betas and gammas. And so I would expect you to achieve much better performance if you let it in Canopy or the sound optimal, sub-optimal. Okay, I'm gonna move on for now. But if you have a question about this, please feel free to come see me during the bridge. So DYJ backpropagation these theta i's. And then because again, this beta I is the same beta i for all of my examples, then the gradient is gonna be the sum across all my examples of the beast DL DYJ is the gradient over here is also going to be a DL DYJ backpropagating through a plus sign and then we can back propagate out to gamma i. So let's do this in a different color. I'm back propagating out to TLD Gamma. And we know that when we back propagate through multiplication, you take the oxygen gradient and multiplied by the value on the other wire. The gradient here is gonna be my upstream gradient, which is DL DYJ. And the value on the opposing wider dispatch had j, right? So it's going to be DL DYJ times x hat j. That's where we get this term. And then again, because of Gamma I is the same gamma i across all my examples are going to be summed from j equals one to 100. These are all scalars. Yeah. So, yeah. You may have also wondered what the outside j on the left or the right, because they're scalars. It doesn't matter if you go on the left progress. Alright? That gives me this gradient over here. So this is the purple gradient, my gradient DLD beta. Now I'm going to backpropagate to this wire right here. I'll draw this one in bike loop. I'm going to put the gradient over here just so that this case is encoded. And this is going to be DL DYJ times what's on the other wire, which is gamma i. So this gradient here is DYJ gamma hard. I'm sorry, I'm going to drop the subscript sides, like I said. So the gradient of L with respect to d x hat j is going to be DL DYJ times Gamma. So this is my blue gradient. Any questions about the purple? Alright? And then after that, we're going to do one more back propagation step. So we're going to backpropagate and I'll do this in green to this right here. If I want to compute DL DHA, alright, I'm gonna take my upstream gradient, which is this blue one. I'm going to call this D L dx j. So this is going to be PL d x hat j times the value on the other wire, which is b to the k. So this is b j. So when you see me write it out right here, I have the x hat j, that's this term. Then b j here is going to be the value of this wire, which is gonna be one over the square root of sigma squared plus, alright, and so that's why this term here is equal to DJ. And I've just expanded it out. Any questions on those pretty Let's do one more actually here, I'm going to back propagate to DLD BJ also. So my two D, L, D DJ, I'm going to take my option gradient, which is dy dx that J, it's going to be DLT x hat j value on the other wire, which is here a j. Right? So those are radiuses that propagated all the way to these workers. Any questions on any of those gradients? The question is, do we care about the wire that's coming out from the J? Which wires that yes, we do care about these. We're gonna do them on the next slides. Are there questions on dissenting of these gradients? Next five years, next time. That is correct, yes. So we will get to that in later slides. So we will have to write a DUDX I turn. Okay. Other questions like that. Right? So if I understand rocks this question correctly, it says that theta i and gamma i are shared for all the J computational graphs. Xy is not exciting as a superscript j plus one and epsilon I j. So those are not shared. The question is, what is the proper order of are you talking about the terms that multiply? Right? Great. So the question is, here I put BJ on the right-hand side, but here VJ is on the left-hand side. And the answer is because they're scalars, you can put them on the left or the right equipment. Yeah, so the question is what if you were doing, was doing this for the matrix that you would have to get the ordering correct. I'm going to follow the denominator layout. But we won't do it with a matrix because it turns out that if you try to do this normalization with the matrix, you need to compute a matrix inverse across or parameters. That's very expensive computationally. So that's why we always break it down into just a scalar computations. All right, we're going to move on then. So we're going to start off then on this next slide with DLD BJ. So that was this gradient in green that we calculated on the previous slide, DLD BJ, That's right here. And this thing was the L dx j times a j. And a j is just x j minus mu. So that's why I write that. Djs x or x j minus mu times d L dx factor. Okay? Now we're going to finish the back propagation along this part of the graph. So the first thing that I'm going to do is I am going to backpropagate from here to here. Can someone tell me what that backpropagating gradient will be? Someone else other than Daniel. Welcome back today. I know that's perfect. Yeah. That's correct. So remember that when we backpropagate, right, to take our opportunity gradient times local gradient. So this operation here is saying that d j is equal to the local operations inverse of d j is equal to one over CJ, right? And therefore, if I take the vocal brilliant, I'm computing d b j C j. And this is equal to minus one over c j squared. What that means is by backpropagate here, it is going to eat both. The gradient is gonna be d L d b j times minus one Over the value of C j, this whole quantity squared. So I've written that expression out here. What is CJ equal to C j is equal to the square root of sigma squared plus epsilon. And if I go ahead and square that, I get a one over sigma squared plus epsilon, and then there's a minus sign here. Alright? So this minus sine one over sigma squared plus epsilon is my minus one over c j squared. Then this is DLD, DJ is this term. So this is my upstream gradient. This component here is my local gradient minus one over some cases there. Any questions? Alright, now we're going to backpropagate through the square root symbol. So, well the square root operations. So if I back propagate here, what do the exact same computation of a local gradient. So here the operation is C j equals square root of EJ. Alright? Therefore, D C j d e j equals 1/2 times the square root of this gradient here is gonna be my upstream gradient, which is my purple thing. Minus one over sigma square plus epsilon times this. And I'm going to further multiply that by 1/2 square root of e j 1/2 is over here. The square root of e j is equal to square root of sigma squared plus epsilon. And so that's why then she Min squared plus epsilon goes to sigma squared plus epsilon to the three-halves because I'm adding here a square root. That will be my gradient. Backpropagated to d L, d e, j, k. We're doing a bunch of stuff here. Any questions or anyone want me to explain any of these treatments? Can you raise your hand if you're following so far? Awesome, that's good. So let's go ahead and do one more thing. Which comes back to the student's question over here about backpropagating through. So they also knew the gradient here, DL DHA. Dha is what we computed over here. And it is equal to dy dx times dy j. So we know that this gradient DL DHA. And what I wanna do is I want to backpropagate to D L, D view. Alright. Do DHA. Let me write it out here. The LGA j was equal to d L d x hat j times DJ. I remember j is equal to 1/1 over the square root of sigma squared plus epsilon. All right? So DLD a J equals one over sigma squared over the square root of sigma squared plus not fun. That's this term here. Times dy over dx j. So the first thing that we're going to do is we're gonna take this gradient and back, propagate it to you. Let me do this in a color red. When it back propagates to view you as being subtracted from x j. You haven't minus sign. And so this expression here, which is equal to this gradient is also going to get a minus sign. Alright? So backpropagating DL DHA here gives this expression here. But this is not the complete derivative. Because new comes in somewhere else in this graph. And this is therefore incomplete. I see red. I need to read without the summation. Yes. So this ambition here though, will be the same argument as before. And so that's why I'm just going to be summation there. Right? Yeah. The question is sigma squared also will apply to every example. Jason will also have a sum j equals one to m. Yes. Yes. Okay, So there's one more thing that we're missing here, which is that when you look at the operation of batch norm, Sigma I squared is also a function of you are. Alright. So your eye is going to go through this function. I'm going to call it the sample variance function to tell me what the value of Sigma I squared is. So there's also going to be a function called sample variance. Sample variance. Compute Sigma I squared. And it has inputs x, j. As well as to do so because this view is the same, right? To get the total gradient DLD in you, I also have to backpropagate to this new and then sum those gradients together. So I need to also compute a plus d L d sigma squared and then a d Sigma squared d nu by my chain rule. Again, that's because sigma squared is also a function of. Alright, so to do that, I'm just going to write up the equation here. So the sample variance equation tells me that Sigma squared is equal to one over n. And we have a sum from j equals one to n. And we have an x, j minus mu squared. If we take d Sigma squared DMU, what we're doing is we're doing d Sigma squared DV view. And this is going to be taking the derivative of this expression with respect to mu. So I'm going to get the two to come down. So I'm going to have a two over m. And then sum from j equals one to n of x, j minus mu. And then I have to differentiate with respect to D, which is gonna give me just a minus one-half. So this is going to give me a minus sign over here. What that tells me then is that this gradient here is going to equal DLD sigma squared. And then for d sigma squared d new, I'm going to plug in this expression right here. So let me erase this. And I plug in d Sigma squared DU, which is going to be this times two over n. Sum from j equals one to n x j minus. And then that is my gradient. Questions here. Daniel's question is, isn't new, something computed as a result of the data? Meaning it is calculated from the data. It's not really a parameter. I'm not going to do gradient descent on it. So what do I have to calculate DLJ? And you constructed the student's point earlier, which is that in the computational graph, you depends on x j. We need to compute a DLT view so that we can back propagate it to x j, x j. We need the gradient for because that's going to give you the greatest upstream. Other questions here. Alright, so that's actually brings up a bit to get to more babies here. So we have all these gradients. Now, what we wanna do is we want to ultimately backpropagate to DLD acts. Like you're just answering for Daniel's question, influences u and Sigma. So I also have to know how to backpropagate the gradient DLD due to D L dx and then DLD Sigma squared dy over dx, which means I have to compute what DLD Sigma squared. So these are gradients, DLD a and DLD that we already know the gradient. There's a question here. For the DOD view. The customer, is there a reason that what the question is, is the reason for this summation the same as for the beta i's summation? Yes, it is, yeah. Because u is going to be the same view for every example. Alright? So now we need a backpropagate to d sigma squared. We know DLD EJ, we did that here, gradients. So let me write that in blue. We have DLD j, that's the gradient right here. And what we wanna do now is one of that property, that is sigma i squared. So this is just a plus sign. So the gradient passes through. And then for the same reason we typically keep talking about the Sigma iceberg serves every example j. So it's gonna be the sum of the DL ij's. And therefore, this is a backpropagated gradients for DLP sigma squared. This expression is exactly what we calculated on the prior slide in blue. And now overdoing here is something that across all of my examples, j equals one. All right, so DOD sigma squared is basically for free because we already knew this gradient calculated through a plus sign. Alright, now we come to our last thing, which is we need to get dx j. We need this gradient right here. So for this gradient, recall we had a DL, DHA that's in green over here. We're going to have a blue gradient, del d Sigma squared. We'll have a purple gradient, DL d u. And remember that all of these are function of x. So u is computed from X bar. The sample mean, sigma is computed from X by the sample covariance, or sorry, the sample periods. And so what we do here is we're going to write the chain rule. So first off, D L dx j is going to be a component from the green backpropagating gradient that's propagating through a summation. So it just comes through, That's the steel DHA. Then for the Sigma squared term, remember that there is the sample variance function that is affected by x JMU. So to backpropagate here, I needed to do DLD sigma squared times the local gradient or the sample variance is sigma squared x j. That because that's going to give you this term. And the last thing is the backpropagating front of you. So mu goes back through a sample mean function for which x is also an input. And so to backpropagate through this view, I'm going to have to calculate DLT view of times d Mu dx j. Alright? All of these are gradients that we've already computed or ones that you can see. So I'm going to just, I'm not going to compute d Sigma squared dx j, but please do that as practice on your own. That would be taking this expression right here and differentiate it with respect to x j. And then when you add all of these together, because these are all convergent pass on the state x j. So the law of total derivatives apply. Mp get the total gradient d L, dx j. Yeah, So you're saying you appears here and remember, you also has XJ affects this new shouldn't there be another cluster, the new term there isn't in this case because this GLP view term came from this slide and this slide DLT knew already took into account the effect of both of these pathways can do. So. We've already talked about gradients. So great, yeah, so Anna's question is, when I propagate, my back propagated through this view, I've got a minus sign. When I talked propagated through, backpropagated to the x j, I didn't have it. And that would just be because right here will have a j equals x, j minus mu. So because the minus sign is on the new, then that's why that minus sign come from the view, but not for the extra question. Any other questions on that? We have seen that should be right. Back with some great boxer. This pointing out that this deal dx j is for one example of j, right? And so what's going to happen when I back propagate to train the parameters of my networks. Well, in that back propagation code, you're going to have a dy, dx j. Let's say this is let's say that there are, I'm sorry. This is, this is a scalar, so this is an R. But let's say that we had 100 units. So we would have a big X being a, a matrix, which is the number of examples, which was little n by the number of artificial neurons in that layer. Let's call that 100. And then this would multiply a big dealt with you. So you would not sum them together. It would be concatenated as a batch, just like in homework number three. And you've got complicated. This is great. The question is, is this just be seen as an example of continued back propagation? Or do you need to know this for the exam ladder? So you need to be able to backpropagate through these types of operations as well. And there's good chance that there might be an exam question on that. Alright. Thank you, Tom. I said Tom, I sa"
"Great. All right, so now we want to calculate, let's say, let's call it CK star. So CK star is going to be the complex conjugate of CK. that Ck was equal to its real part plus j times its imaginary part. Then we know that Ck star is going to be the real part of Ck minus j times the imaginary part of C k. And so relationship. Does anyone who did this question want to tell us what the next steps that you did are? And feel free to unmute yourself, you don't have to raise your hand. I rewrote the real part of C sub k as the negative of the negative of the real part of C sub K, and then that allows us to do some substitution based on what we saw earlier. Yes, so we can write this as the negative of the negative part of real of C minus K. Did I get that right Bradley."
"Great. Yeah, so let me just draw out what Kai was saying, which is the correct answer. If this is our spectrum, f of j omega. So the exact same as omega, let's say that the, let's say that F of T had a Fourier transform that looked like this. So this is going to be capital F of j omega. Where F of T has a Fourier transform capital F of j omega. And so what Kai was saying is when we compress the signal, right. And we cause a signal to actually have higher frequency components, meaning it changes more quickly. If we were to think of trying to reconstruct this signal with co-signs. If I want to get these wiggles and squiggles out, I would need to use co-signs that are pretty high frequency. All right. And so cosine with higher frequency is going to have a larger value on this omega axis, which is quantifying frequency. And so we would expect then. If a is greater than one for the spectrum to be expanded, because I need to use higher frequency co-signs. To make my red signal, whereas my blue signal, my extended signal."
"He just makes movements continuously from targets to target. And simultaneously, movements, we record neural activity. All right. So we have two observations. The first observation is the monkeys observed kinematics. And we write that as respect to XK. K here denotes time. So it's K instead of T because we're going to discretize the time. And then PX and PY, these are the monkeys X and Y position at time K. And then VX and VY, these are the monkeys X and Y velocity at time K. These are the position and velocity of his hand. So as he reaches, we track a little bead on his hand and we're calculating the position and the velocity of that little bead. All right. And so simultaneous to this, let's say that we have one new TOR ray implanted."
"Hello. As far as follows, homework number four is due this Friday. We announced last time this homework has a lot of coding and it's also optimization at the end. So please be sure to get an early start on it. I will be sending a midterm announcement later on through and learn and Yasser today, please read it carefully because there are a bunch of details that we have to get right with a class this large. And so first off, because we have, we had a fashion that only takes 420, but we have over 370 students enrolled in this class. We are going to ask some students to take the exam at one of the smaller classrooms and voice call, and we'll do that by last name. So we're going to send out an announcement with those details. In that way, students can be more spaced out of this room as well. Some logistics details. That'll all be in more detail in the email once we finalize those locations. Midterm logistics details that you are allowed for two chips, each piece of a 0.5 by 11 inch paper. And you can fill out both sides with whatever you want. And so you have eight total size. The midterm is going to cover material up to and including this Wednesday's lecture. And so it'll cover any food up to including convolutional neural networks, which we're going to start off with today. We're going to get to today. You can find past exams. I'll put it in there. And so we've uploaded all of the exams that we've got very different. But this practice question, you may bring a calculator to the exam even though the define the exam so that it can be done without a calculator. But you can't bring any other type of computing device to the exam. You can do the exam and pen or pencil because the TAs and I are going to be scanning your exams after after we complete them. And then lastly, if you are in the MS combine program, this announcement that was sent out, we'll have more details on this anyway and how we'll distribute the exam and then walks that asks you to upload it. Any questions on homework or midterm logistics. Alright, and then just a reminder that there's going to be a midterm exam review session this week. On Thursday is going to be 6-9 pm on young CS 50. I don't see us 50. The TAs will have a Zoom room open, but they won't be monitoring the chat and that's a recording of it will be posted. And we recommend that you at least attend to or watch that review session. Boxes as we strongly recommend. Alright, any questions? We will get back into material then, our last lecture, we talked about these different optimization techniques that we could use for neural networks. We talked about the idea of adaptive gradients at those two are combined into an optimizer called Adam, which many of you have already heard of. And it's a really great fit. The first-order, the classic gradient descent, that includes both the momentum and the second moments of the adaptive gradients. I wanted to just start at the beginning of the semester asking if there are any clarifications on any concepts from first-order optimization, last factor. Alright, so there we left off by sector we are. Once you give a high-level overview of what second-order optimization is and also talk about why it really isn't using neural networks. And so be recalled that you are finishing on just differentiating first order and second quarter gradient descent methods. In first-order methods, what we do is we make a linear approximation as some parameter setting theta t that's given by this equation, which is just the first-quarter Taylor expansion of the loss J Theta t. And if we were to do is to cast a gradient descent step That's first-order. Then all we do is be updated data by just stepping in minus epsilon times the gradient direction. And if you plug in this update step to estimate the loss of index. Setting up a theta. Your linear approximation is always going to estimate that your loss will decrease by some amount, epsilon times the gradient transpose. Right? And when you take a small step, this approximation is closer. But when you take a large step by step to this blue point right here, let me redraw this loss function. Just take it a bit more clear. Maybe the loss function actually looks like this. If we take a large gradient steps from theta t, theta t plus one, we are going to have expected that our gradients that are lost would have gone from this value here down by this linear approximation. And so this is my epsilon transpose Gy that I expect my loss to decrease by if I make a linear approximation with the first-quarter method. But if we were actually to step two, theta t plus one or loss will actually be higher than, alright? And that's because the linear approximation no longer holds when you take, it holds more poorly when you take a large step size. Any questions here? Alright, so we were saying, Well, isn't there more information that we could use to take a better step? Let us a second-order methods. So the idea with the smallest, we drew these pink curve last time. And we said, if we're in a relatively shallow, reasonable loss, that we want to take larger steps. Whereas if the loss is really steep, like this example over here, we want to take small steps, alright? And we can formalize this by saying, well, to know how shallow or how steep the loss surfaces, I can compute its curvature, the second derivative. And so that's why these are called second-order methods because they make the second derivative. So if the curvature is low, which means my second derivative is small, then we want to take a very large step in gradient descent. And at the second derivative was large, that means I have high curvature. I would want to take small steps. And so the idea behind the second-order method is to start off at your parameter setting theta. Then you make a quadratic approximation. Where again, because the quadratic approximation includes the second derivative, it estimates the curvature at dislocation. And then after you make this quadratic approximation, the way that you stopped is you just stepped to the bottom of the parabola. So if we were at the speed of t, I will make this quadratic approximation turn red. And then my step would be to say I'm going to start to the bottom of the parabola. And that would be how I update from beta T over here to Theta t plus one. T plus one would be over here. So you can see if the curvature is shallow, then my parabola is going to be wider. And if my parabola is wider, when I stepped to the bottom of the parabola, I'm going to take a bigger step. But if I pluck summation here in purple is a very narrow parabola with my curvature. When I stepped to the bottom of the parabola, I'm going to end up taking a very small. That's the intuition of a second-order method. Yeah, perfect. That's a great question. Which is the question is, is there a hyperparameter in the second-order method? There is. Because the hyperparameter for Vicky descent first-order was epsilon. That tells me how big of a struct to take. But in this case, the step size is dictated by the width of the parabola to the bottom. Alright, so that's the intuition of a second-order method. We have here. A derivation will go through the derivation because it will remind you of some things that we've done before. We're going to talk about why we end up not using these methods too much in deep learning. So in going for the second-order optimization, we take this first-order Taylor expansion, which only includes the first-order derivative, and we now make it a sucked into order Taylor expansion. So now we did the second derivative term and the second derivative. So this is the same as from the prior slide. The second derivative term will include this Hessian matrix. This is this matrix that we define what we're talking about gradients. This is the multivariate analogue of the second derivative, right? So if you just take a scalar quantity with respect to a scalar, possibly the derivative twice In order derivative, the multivariate case, there are many dimensions to take best second-order derivative. And so this question is this generalization of the second-order derivative? We usually write it as this notation where we're doing our nabla twice. I need the second derivative. And you'll recall is just matrix where along the diagonals is derivative of loss with respect to Beta one. But the second derivative, the 22 element, would be d squared j theta. With respect to d, beta two squared, et cetera. And then the off-diagonals. I don't have that much straight, so I'm just going to stop sharing. This would be a dy squared J of theta with respect to d theta one, d theta two. And this term here would be d squared J of theta with respect to d theta two, d theta one, and so on and so forth. So this is the Hessian matrix. Again, the multivariate generalisation of the second derivative. So this is the second order Taylor expansion. And so this equation here is this red parabola, my aquatic, my quadratic approximation of the loss function around where my current parameter setting. So if I want to do a step to the bottom of the parabola, what I have to do is I have to take my parabola, set the derivative equal to zero and then solve for theta. Don't want to take our parabola equation right here. We're going to set the derivative equal to zero and then solve for the Theta that minimizes this quadratic approximation. Any questions? Alright, so for this, we're going to use two gradients that you all know already. The first is if we have gradient with respect to Theta, theta transpose G, that's just going to be equal to g, The gradient. And then we also have this one. If we take the gradient with respect to theta of theta transpose h, This is going to be that H plus H transpose Theta. That's the gradient that we had derived from. Bye. Okay. Great. Wonderful. Yeah, so the question is, in this Taylor, Taylor series expansion, we would have a theta minus theta naught times the gradients. And then for the second order term, we would have a theta minus theta naught squared times the second derivative. So where's the squared here? So it actually exist because we have a theta minus theta naught transpose. And then this is the other one, theta minus Theta. But then you have to do the order of operations correctly because they're matrices and vectors. So these two are my theta minus theta dot squared. Sorry, can you repeat the question? Yes. Thank you. So a student is just clarifying this parentheses here. Each of parentheses, beta minus theta naught is not a function. H is multiplying this vector theta minus theta naught. So this is a matrix vector multiplication not assumption page. That's a great point. I've, I've never I've never received that question before, so thanks for clarifying that. Alright. Okay, so let's go ahead and do this gradient. So we have j theta naught. We're going to differentiate this with respect, to, differentiate this function with respect to Theta. This has no theta, so it goes away. This thing here is, I'm just going to call this g for shorthand. This thing is equal to Theta transpose g minus theta naught transpose G. If I expand out this multiplication. So Theta transpose G, I differentiate with respect to Theta, I get HE out. And so that's the G here. This being doesn't have a Theta, so its derivative is around. Last one here is this gradient. And so here, if we differentiate with respect to theta, then we're going to get a one-half parentheses, H plus H transpose instead of minus beta naught. H and H transpose for the hush gender is a symmetric matrix. So H plus H transpose will give me two H. That is going to cancel this one-half h of Theta minus Theta naught. So when we take this gradient and set it equal to zero, we get this equation. And now to solve for what the Theta is that minimize this equation. You get this beta naught minus the inverse Hessian times. Right? So this is essentially the Hutchins in the first-order, right? First order approximation for gradient descent, we would have data is updated as theta minus epsilon times the gradient. And then the second-order, we replaced that epsilon with this inverse hashing. So theta is theta minus H inverse gradient. Question. The question is, can I explain what happens to the second term of j theta, this term? Just want why would I differentiate this? I get just this is not the question. Oh, this term. So this term just goes to zero because this is J evaluated at a particular thetas are up. But I'm taking the gradient with respect to theta because I want to know where I'm going to write. This forms can also be pretty intuitive, right? Remember, we said that what's going to happen is if by curvatures fall or sorry, if I lost surfaces shallow, right? If it's wide, which means my curvature is small, I'm going to take a larger step. Alright? Curvature being small means the second derivative is small, which means a heterogenous small. But here I'm inverting the hashing, my curvatures wall. When I take one over that, I'm going to get a big number and that's going to lead to a large staff data. Whereas if my curvature is high and h is height, one over that is put to get really small number. And this is exactly what we want. It measures the curvature using the Hessian. It takes a step in each direction based on the questions that I kept getting. Rocks. Just question is, is it always going to be guaranteed that the Hessian is not invertible. So no, so the Hutcheon will be estimated from samples. If we have enough samples, in general, it will be invertible. However, it may not be a well conditioned matrix, which means that it might have small singular values in some directions. Those cases, there are ways to condition e.g. by kind of like an aldehyde to make this invertible. So the DLP assume that they even brighter. Right? So, sorry. So the Hessian will not be a 4D tensor because it's still a scalar with respect to a matrix. Oh yeah, you're right, sorry. It's adult. Yeah, you're right. So rock to this, pointing out that this fashion, if theta is r matrix, will be a 4D tensor. Because at first, the first derivative will be a matrix and the second derivative will be the derivative of a matrix with respect to the matrix which would be affording texture. That's correct. Yeah, because if theta where r matrix ingredients would also be matrices, so there's be a 4D tensor type of 2D matrix which will give you a 2D matrix. Sorry. Yeah, Jake is asking you about this step here, which when I take the gradient of if I were to just differentiate, I'm just going to use, I'm going to use x instead of theta. I do x transpose H x equals H plus H transpose. But then in this case, the Hessian is a symmetric matrix because of the way it's structured. So if I differentiate a scalar with respect to theta one, theta two is equal to if I take that loss function and differentiate with respect to theta two and then theta one. So because it's symmetric than these two simplify to equal to h x. And then this two cancels out this one. Great, Yeah. Other questions. Alright, so this is called Newton's method. And this seems like intuitively, it does a wonderful thing and we don't have to do all this optimization of epsilon, although again, Adam, Adam and Adam and RMS possible by our lives there. So the question that you may have is, why don't we always use Newton's method, right? But think about it for 10 s and I want to see if anyone has ideas for why we don't use Newton's method in practice for neural networks. Wonderful said. Calculating the cash-in and computationally inverting it is unbelievably expensive. That's correct. So let's look at this in three points. The first is memory. So it is computationally expensive to store the hash here when we have large networks. So storing the Hessian is expensive. Let's say that we have a neural network with 1 million parameters. We're going to talk today about seeing answer. We're going to see our CNS sometimes have hundreds of millions of parameters. Let's just say we have. N equals ten to the six. So 1 million parameters in our neural network. Let's say that each number was floating point single precision. So four bytes per number. If you compute how much memory it requires to store the gradients, we need to store 1 million numbers each four bytes, and that comes out to 3.8 mb. If you want to store the hessian, you need to store 1 million by million matrix. And you do the math on how much memory that requires. This is 3.6 tb. So first-off sorting and hashing, even a very, very large number is prohibitively expensive. Second, inverting the hashin is expensive. So inverting the hashin, we know that matrix inversion is order n cubed. I said you can imagine this is going to get slower. Let's quickly, much more quickly I certain number of parameters increases, right? And then an empirical result That's relates to all of these is that to estimate the Hessian or approximate it well, typically requires a very large batch. Right? That makes intuitive sense because we have so many numbers did you get an estimate? So we need a lot of examples to be able to estimate those numbers. Well, for all of these reasons. Nazi second-order method to use. Even though that, even though they're very great idea, All right. Any questions here? Right, great. So this question is actually getting us to the next slides, which are supposedly is, what if someone came along a way for us to estimate h minus one in a very short amount of time to be end up using it. And the answer is yes. So the following slides are not tested. This is really just for your information and I want to point you to ECE 36, B, and C If you want to learn about these things. But because it has CNS, so prohibitively, essentially, there are going to be ways that people have developed to be able to do Newton's method called Quasi Newton methods. And so BFGS. Okay, actually that's less than I would've thought. Since all sides never asked me how many have heard of. So BFGS is named after these researchers who found a recursion to approximate the inverse Hessian. And because it's a recursion and it can be written as a bunch of outer products. We actually know how recursion for the inverse Hessian, so we never have to also inverted. And so this is one method that can be used to still do the order dispatch with an estimate of the Hessian from this recursion. Then there are other techniques like limited LBFGS, which, which limits this recursion. And then there are also other methods that you might hear like conjugate gradient methods that tried to do as so-called free optimization. Essentially trying to approximate second order optimization without ever having to compute the Hessian or storage. So those are all beyond the scope of the class, will never be tested on that. But I just want to point them in case you're interested in looking at this further. And again, ECE to 36, the entity will go over these in more detail. Okay, great. On one says it's 236. See, that goes over these B does not. Okay. Any other questions on second-order methods? Great. The question is, for a second-order method, will it be problematic when there is a saddle point? Problematic in the sense that you may not find the direction. It's a second. Yeah, so true exactly at that subtle point. And there's no noise in your estimates, then you would just stop there. Other questions. Alright. So that's the other optimization. Sorry, no team working off medication. There's something called the fundamental problem of deep learning. And we're going to talk about it more and more as we start to talk about practical convolutional neural networks as well as recurrent neural networks. But essentially this fundamental problem of deep learning is that we have stacked many layers for neural network. And we know that eventually there's a loss function. And let's say there are, say, 20 layers, right? We know that we have to backpropagate through these 20 layers. And we have already seen examples, e.g. in the initialization where. Maybe these weight matrices are relatively large. And after you're back propagating across 20 of these weight matrices. If those weight matrices are large, after 20 repeated multiplications, your gradients could explode. Or if the interest rates are small, they could vanish. Alright? If your gradients explode or banish. That means that for your update at the first weight matrix DLD w1, either at they explode, w1 is one step up in some crazy direction, nobody will occur. And if DLD W1 is zero, then it doesn't learn it all because the W1, alright, so the fundamental problem of deep learning is making sure that when you get gradients they haven't exploited or famished. Here's a really straightforward way that we can avoid exploding gradients and it's called gradient clipping. So here's the idea. We do is every iteration of gradient descent, we take the gradient and we look at the norm. The norm if it exceeds some value, some hyper-parameter, I'm going to call flip. Then what we want to do is we're just going to scale down a gradient so that is no longer bigger than, Alright, so basically if our gradient look like this, but clip and the magnitude of this is say 100 But clip was equal to ten. We were to scale down our gradient so that it only looks like, alright, so when we do gradient clipping, all we do is we check to see if gradient is that this magnitude is larger than clipped. If it is we up to, we update the gradients polar. So we take our gradient which is bigger than clipped, we normalize it to be unit vector length, and then we multiply it by. So this will scale down the gradients and practice. This actually helps quite a bit with exploding gradients. All right? Any questions there? Yes. The question is, what's the difference between clipping and reducing the step size? So the clearest way to see the difference is that these gradients, remember our backpropagated. If you reduce the step size, you're just taking a smaller step for every single parameter. But when we backpropagate it, the gradient, if it was clipped over here, will be smaller than what I've got propagates to the prior step. Alright? So instead of having everything would be large and then try to typically small step where there'll be creating a balanced, maybe these didn't explode, and these did explode. If I tip it every step in nothingness. The question is, when we have a vanishing gradient, doesn't that actually mean since DL DW goes to zero that we reached optimum? Not necessarily. So it'll be a local optimum, but it can be a very poor local optimum. And so we'll see sometimes we have vanishing gradients just because of, you know, like a poor initial position like the waste or too small. The question is, is gradient clipping done when we are updating the weight vectors or when we are calculating DL DW. I believe it's typically done when calculating DL DW view. So if you put this gradient, I believe you would use it for backpropagation. Can you check me on that? Just to be sure? Yeah. The question is, is there something like batch norm is something similar for regularizing the gradient norm? Not that I know of. Something like batch norm would help us to get a deeper. So it has non-explicit but some intrinsic effect on the gradient norm. Sorry about that. Great. Thank you. That's because there's plenty out of the city's planning I wrote greater than or equal to. This thing still has a normal credit. Yeah, the question is, can I go over and exploding gradients and what that looks like? Yes, Let me pull up the fire lecture. Actually. I'm going to use this example from when we did initialization. And there was this case where we had a ten layer neural network and initialize the weights to be too large. And so when We initialize the weights to be too large. We saw that the activations exploded. And this is just an example of an exploding gradients because the activations have gone to, let's call this 120 million in value. When I do my backpropagation. Didn't, never mind, I thought I had this slide, but when you do a backpropagation, you're starting off with the softmax scores. And then to backpropagated to the next weight, you need to multiply it by the activations transposed. So there's an example where it then your gradients are gonna be like 100 million. And Mr. Bass arm, I didn't know anything. You're not going to stop and tell her the whole question of the questions. I'll explain the gradients, right? Is that right? Yes, it'll help to some extent because I'm a range of activations, a reasonable range. So to avoid, like the exploding gradient is used to answer your question. I've said it isn't a panacea. So we're going to learn that for networks like we can only train them up to the order 15 layers, maybe 18 layers. Because after that, you shouldn't have that one. But even if they did have cash, they wouldn't shrink. So we'll have to think of other ways to avoid these exploding and vanishing gradients. Alright? So those are challenges to be aware of. Vanishing gradients is just when gradients go to dances around. And they're going to see this as a really problem when we have recurrent neural networks and we're going to actually architectural solutions to address this. Alright? That is it for optimization. Any last questions on order? Adam, Momentum, Nesterov, momentum. Alright, we're gonna get into convolutional neural networks. Convolutional neural networks are kind to this modern revolution and keep learning and they are one of the workhorses of the cell we're going to talk about. Architecture is motivated from how it works, what the architecture is. Never going to do case studies, ImageNet winners. So look at how we set the various hyperparameters of these compositional numbers. So this lecture will correspond to chapter nine and a deep-learning textbook. Alright? Just like we said in lecture one, this is going to be our motivating example where we're looking at this ImageNet competition. And on the y-axis here, we have the top five error rate. Remember that there are 1,000 image classes instead of C4 time where you have ten and you get to take five guesses. And if anyone of those five and you get it correct. And so that's what top five error rate is. And the competition started in 2010 and it was in 2012, where you can see the error drops by a significant margin. And this architecture here, AlexNet, only convolutional neural network in the competition that year and did everything by almost 10%. So from here on out, all of these architectures, VGG and add CF night you will not rest, are all convolutional neural networks that optimize architectures hyperparameters to eventually get the error down in 2015 to 3.57. And this is actually superhuman performance. So a few grad students at Stanford one weekend came in and they tried to do the tasks themselves with taking a guess at the top five labels. And they were at 5%, right? So by 2015 with this resume, it actually achieved superhuman performance on image classification. Alright. We're going to first talk about the inspiration for the architecture of a CNN is also CNNs have been around since the 1990s. So the first number even going to look at is this one in 1998 called Linux from younger. And in addition to this, I forget if I might have cut it for this year, but there's this architecture from a researcher named Fukushima called the NeoChord neutron. This was in the 1980s. And this has many similarities to CNN's as well. So you can even say CAMs really started in the 1980s that this NeoChord controller, alright, so you should look at this picture and not know what is going on at all. We're going to unpack that entirely. But we're first going to talk about where This architecture came from. And then again, we'll talk about exactly what the architecture is. I came from biological inspiration, particularly some of the seminal experiments, experiments done by Hubel and Wiesel in the 1960s, got them the Nobel Prize. Where it looked at how neurons in the visual or animals responding to stimuli like moving borrowers. And Hubel and Wiesel's work, as well as lubricant, other neuroscientists in the 70s like Tony motions, led to several conclusions about the digital system. And these are the three things. The first is bad. B1. B1 just means primary visual cortex is the first part of their cerebral cortex that processes images from the outside world. E1 has a retinotopic map. I'll explain what that is on the next slide. Is that the one has broadly two clusters of cells. One is called a simple cell. And these simple cells can be approximated by a linear model, followed by thresholding operation. And then the other is that D1 is composed of these complex cells that respond to that, that are invariant in the position of the feature. Alright, so these three principles are going to be less R-CNN architecture. Okay, so first off, you want has a retina topic. Now, what does this mean? Basically, it means that if I'm looking at somewhere, right, things that are close to each other in my field of view are also closely represented together in my tray. So this circle is my field of view and so it's breaking up the space I'm booking or into, into these different regions. So 11 is like the students out there, 90 students to my top-left. Maybe one, maybe three. Here is tomboy and one hears rocks it right here at the bottom. Alright? So Conway and rocks ships are sitting next to each other, each other in space. If you were to open up my brand and record from my visual cortex neurons, you look for the neurons that represent tomboy and rupture. They would also be close to each other. They're actually, this is, I should have said 3.4, because the brain also has this property where things in the right visual field or in the left hemisphere of my brain, largely these in the left visual field or in the right hemisphere of my brain. So if Tom way was three interactions with four, they are representing close to each other. Also in 3.4, goals have been drawn represents any questions there on topic now. Alright, so that's, that's another topic. Yeah. Okay. Then they're really simple cells in the visual cortex. And so when you tried to model the activity of these biological neurons, the researchers would do is that they would model them, but basically vocalize linear filters. Basically just have to focus on linear filters here. These linear filters can be or can be implemented by an operation called convolution, which we're going to talk about. But convolution sum is a linear operator and any linear operator can be written also as an affine transformation. So basically the simple cells are modeled by either convolutions are matrix-vector multiplies, whichever one is easier for you to think of now, followed by some thresholding operation. And a thresholding operations says, if you're above that value, report the value. If your report is there. So this is our greatly. Alright. Then CNN's also the visual cortex has these complex cells that encoded in variance. So P2 features. And the way that the early Shannon's tried to model this is through something called a pooling layer, and we'll talk some more detail about that later on. But this pooling layer is supposed to help to represent some of the data. Just tell them variant arises in or is observed in complex cells. Any questions on any of these three principles? The question is, what was the inspiration for using linear layers for this wholesales? So when the researchers recorded the simple cells, they sent me in their experiments, what they would do is they would e.g. take a bar and show it to a cat and move the, move the bar around. And they would report and listening on these neurons that are moved around where simple cells Keynes is, they want to say, Okay, how does this neuron, the cat visual cortex, respond to this bar moving? So if your bar moving is x and the caps neuron is why you need to find some relationship between x and y. And what they found is that many of them could actually just be described by a linear relationship wx followed by a threshold. So I'll write that as thresh WX. And now it looks a lot like bailey W. Great session bear either x, y or the XYZ coordinates of the butter on it. Also. When we talk about how these neurons are representing the bar, this is deeper to the coordinate velocity refers to all different types of kinematic variables are actually also in today's orientation. Sometimes if the bar is tilted by 25 degrees and it leaves, the neuron responds, but it's tilted by -45 degrees, it doesn't respond. Alright? So given the inspiration of this CNN from sy"
Hi David. Hi Dr. Cao. How are you doing? How are you? I'm doing well. How are you? I'm doing well as well. I just woke up actually. Okay. I had a couple of questions regarding last homework.
"Hi, good work. Hi, I don't know if you can. Good, how are you? I'm good. Happy to take any questions any time. I don't have any questions for regarding the course material. I didn't have a question or two that I wanted to ask. It wasn't related to the course. I'm going to go ahead and wait until the end maybe or something. I'm happy to take those questions. My question was about number two. In part B, it says that the peak to peak amplitude is 110 millivolts for this new alien neuron. I just had a general question. I'm wondering if I'm interpreting the material we had in class correctly. For a regular action potential, it goes from negative 65 millivolts up to is it positive 55?"
"Hopefully of interest to you, definitely of interest to me, but hopefully I can convince you that it's interesting. It'll be in the area of so-called brain-machine interfaces. And we're going to try to use the tools that we have at our disposal, particularly from EC-102, as well as calculus, to try to solve a problem in this particular area of study in this particular area of brain-machine interfaces. And so the project will be that I'm going to give you some neural signals that were recorded from neurons in the brain. And we're going to ask you to translate those neural signals into movements. That sounds like a daunting problem if you've never been introduced to the area of brain-machine interfaces. But that's what the structure of this seminar is going to be. We're going to discuss what brain-machine interfaces are. We're going to discuss how neurons work. From there, we're going to discuss what kind of tools we need to solve this problem."
"However, in our lookup table, we don't have something where the denominator is a product of these a plus j omega terms. So I'm going to tell you how to do this inverse Fourier transform by a technique called partial fractions. We're not gonna go into it in much detail here because this is gonna be a critical topic for Laplace transforms. And in Laplace transforms, we're gonna give you three or four methods to do partial fractions."
"I can see it on my computer, poll and progress, posts and panelists cannot. Okay, never mind. So, I don't think I can do this poll. Honestly, who has? I have it. If people are happy to share the, they start the project, you can write yes and chat. I'm not just wanting to get it. I just, I understand if not, it was Thanksgiving. It was Thanksgiving weekend. Okay, so based off of this. The project is doing a week. We'll, we'll give an extension on the project until. A Friday of next week. And I will then take the time to answer any questions you have on the project as well on Monday at next lecture. I just want to make sure you all have an opportunity to ask those questions so that the project is not."
"I hope you're all staying as dry as possible. That's urinalysis important again, the first is that homework number five is due this Friday, March 3rd, up with the base scope. It is our last homework assignment. And I hope you all will the sheet that over the course of the CIFAR ten dataset, we've gone 35-40% accuracy, which was an assignment to with just a softmax classifier to oversimplify percent accuracy with CNN. So almost doubling the performance by using neural networks. On Friday of last week, we uploaded the project and its accompanying data sets to learn. The project is going to be due Monday of finals week. So that's gonna be March 20, 2023. And on the project, you're going to be allowed to use whatever code you desire to use. So that includes applied towards TensorFlow and Keras. You can use any deep-learning library you want. You all haven't had exposure to deep learning libraries. So what we're going to do is on Wednesday. The next lecture. Having two of my PhD students cover how Keras and fights work, work. And you'll see how these deep learning libraries really make implementation of neural networks straightforward, such that you really don't have to understand how neural networks work to be able to train them. Of course, because all coded everything from scratch. You'll know how everything is working beneath the hood. But yeah, we'll have these two PhD students on Wednesday cover the particular syntax and how to get started on the project using Python. With that lecture, we're also going to provide some starter code to implement either CNNs or RNNs with Pi, torch and Keras, alright? And then in the project guidelines, if you haven't read them yet, you also give you the option to do a custom projects. You give guidelines for what that customer project should cover. And I just want to highlight. So you all know that if you want to do a custom project, you need to send that request to me. Nobody can describe it. Are there any questions on the project or the guidelines that we uploaded? Right. And then lastly, the TAs are working on grading the midterm exams. And our goal is to release midterm grades back to you by the end of lecture on Wednesday. Any course, logistics questions. Question is for the project, are there any architecture? So you have to please take a look at the project guidelines. Basically, for the project will require you to connect something that we took her in class, that is Post CNN. So the most natural one will be the one that we, We'll begin today, which is RNNs. But you're also welcome to implement things that we may not have covered in class. So in class we'll also cover variational autoencoders, generative adversarial networks. We may also cover transformers. We don't have that material for today, I did, but I'm working on a new lecture that hopefully will make it in time for, for this iteration of the class. So you don't have to expend something that's after CNN's in the project. Other questions, great. Rock asks, Can we use late days on the projects? Are not allowed to use on the projects. So the projects all have to be submitted by Monday, March 20, 2023. Any other questions? Alright, we are going to get back to material. So our last lecture was quite some time ago. So just to recap, what we were doing is we are beginning or we were welcomed into the survey of neural network architectures. We talked about the AlexNet, the first CN end up on the ImageNet competition. We've talked about ZF nets, which is basically AlexNet foot with hyperparameters optimize and not one in 2013. In 2014, there were two architectures that were remarkable. One is VGG net, which we talked about at the end of our last lecture. And that was this neural network architecture that went deeper to 19 buyers. And remember, the key features that they did was they made every convolution, a three-by-three convolution, and started to stack them. So there would be for ductile three consecutive convolutional layers. Alright. And then last lecture, we were just about to delve into Google neck. And so we just started the motivation of the woman, which was a few things. I wanted to first propose a new architecture. That relies on this thing called the inception module. And the basic idea behind the inception module is what we choose hyperparameters for a CNN, you can choose the filter within SOC as a filter with a title. But which one is best? We're not sure. Why don't we do a bunch of these different ones and then let the network pick out where the most important features. Alright, so that's number one. The second motivation is that they wanted to reduce the computational expenses of these neural networks so that they're not merely an academic exercise so that people can train them if they don't have huge computational servers. So Google has only 5 million parameters. And so that's far less than AlexNet VGG net. We're also going to discuss one key innovation that they make any architecture that keeps them computational expense dance. So this architecture won ImageNet in 2014, and it did so without training on a GPU. Alright, this is in contrast to VGG net, which had multiple GPU servers and it still took three weeks. But pupil that is able to train without a GPU. Any questions on motivation? Alright, so let's see how Google net does this. And this is a basic building block if you go out with just the inception module. So in this image here, this thing here is the input from the prior layer or the actual charge. Sorry, this is the input from a prior layer. And then what you see is that this input gets sent to convolutions of different sizes. So Google net will do one-by-one convolutions, three-by-three convolutions by by comma, five-by-five convolutions and max pooling. And you can think of each of these different parallel blocks as extracting different features of the data. Then what people will do is it will concatenate all of these features together. And I'll send that to the next layer. The next layer just to see the output of the one-by-one, three-by-three or five-by-five convolutions as well as the max point. And then it gets to determine which of those because it wants to use to be able to maximize, minimize the loss. Alright? Any questions there on this basic idea? The question is, we don't do pulling after convolution here. At least in this illustration. We just do pulling on the previous layers and quick to maybe try to extract and variance position invariant. Student asked, how is this less computationally complex than same VGG that because it sounds like we're now, instead of doing just three-by-three convolutions, doing a bunch more convolutions and concatenating them together. So we'll get to that in just one button. The question is, what is a one-by-one convolution? So a one-by-one combo regime will be a filter that has a width of one and a height of one. But remember that gap is matched to the input. So let's say the player input had a depth of 256. So it'd be basically a one-by-one by 256 filter, as you can think of it as combining across the features of cost. But the third dimension up here and protect, sir? Sorry, can you repeat the question? Yes. Perfect gases didn't ask. Here we're using combinations of different sizes were built. These produce different size feature maps. And then if they do have you concatenate them so well, adapt right now. So let's say that our total net receives an input from the past layer, that's 28 by 28 by 256. Alright? It does these convolutions. So the first question is, what's the size of the output of a one of the, when I say 128, I'm going to have 128 filters here. Each of them are one-by-one by the prior layer. So these are one by one by 256. These are 192 filters that are three by three by 256, etc. The output size of the 128 one-by-one convolutions. This is straight forward. We know that w l equals Wn minus Ws plus one plus two times pad. Alright? We're not going to pad here, right? So if it's no padding is just gonna be WN exercises one-by-one w at this one and we have a plus one, right? So the output size will also be 128 by 128. Sorry, I misspoke there. The output size of these one-by-one convolutions will have the same width and height. So 28 by 28, because there are 128 filters open, 28 by 28 by 120 years. Okay? So the output of these 128 one-by-one convolutions, it's gonna be 28 by 21, 22. Any questions there? Okay, so to be able to concatenate these tensors together, the size of these convolutions also have to be 28 by 28 and width and height. Otherwise I can stack them together. So the next question is, what pairing do we need to keep the output size consistent for the 192 filters that are three-by-three. Alright? And we know that if we set hat equals one, right? We said add equals one. Then at the output here will have the width and the height. So 28 by 28 by one. So basically what we do in Go on that is that for these convolutions was set equal one for the three by threes, and then paddy equals two for the five-by-five. And that guarantees that all of them have a width and the height of 28 by 28. And therefore, we can go ahead and concatenate them all together. There's also one more thing which is a three by three fourths of a three-by-three pool is not that the pole shifts is overlapping and the overlap is chosen so that the feature at the output of a three-by-three pause until 28. 28. Then the depth of the three-by-three pool will be 256 because it's three-by-three pool will be applied to every single every single matrix in this 3D text or every 28 by 28 matrix. Any questions on the sizing of any of these? Great, yeah, so Jenna's question is when we concatenate with the candidate or the third dimension, That's exactly right. So we just take all of these features or which we have 128 from the one-by-one convolutions, one entity from the three-by-three of sector and we just stack them. And so one really involved with the textbook. So this will be 28 by 28 by 672. Any questions there? Does anyone see something concerning there? Perfect. Yeah. So the student says, we just keep doing this. It's going to grow and grow. You'll notice that the output of this concatenation is never going, that the depth is never going to be smaller than the input. Because this three-by-three pool will always have a depth of that matches the input. And so if an addition to this, I'm concatenating other features. This number after the concatenation is always going to be bigger than 256. Or the emphasis mentioned. The question is, why are the dimensions of the three-by-three for the width and the height, the same as the input. Yeah, so this is traded at such a number which I can get off the top of my head such that the app is 25, 28. So it's not strike it at three, but it's treated as either one or two. Alright? So you're going to address this problem in just a few slides. But before that, I wanna do one more. I want to do one quick calculation because it's going to relate to how we fix this problem and how the author reduce the computational expense of people max. So the question is, how many operations are there in this inception layer? This will be how many operations are there in these combinations? We've done this calculation many times, and so I'm gonna do this quite quickly. But stop me if you have a question. So let's look at the one-by-one convolution. So for the one-by-one convolution, we're going to ignore biases. Number of operations for just the one-by-one convolutions is the number of output neurons I have, which is 28 by 28 by 12828 times 28 times 128. This is the number of outputs, output neurons. And then this is times the number of operations it takes to compute the value of one output neuron. So one output neuron is the result of a one-by-one compensation. And this one by one convolution has a depth of 256. And so the number of operations here will be one by, one by 256, the filter size. Any questions on that calculation? All right, so then let's do another one. For the three-by-three Kong. Number of operations will be the number of output neurons, 28 times, 28 times 192 times the number of operations to calculate one output neurons activation, that'll be three times, three times 256, the size of each of these three-by-three filters. So I go ahead and calculate that number for the comp layers, the one-by-one to see if I can multiply by five comparable things. Then the total number of operations in this particular inception module, we've drawn out, it's going to be 864. No, Ian, any questions there? Okay, so here comes one of the new innovations of glucose. So remember we have two goals. One goal is that we want to make sure that when we concatenate the outputs of these, that this number is to just continually drilling. Alright? And then the other goal is to reduce computational expense. So what we're going to do is we're going to insert additional complications. So what we're going to do is we're going to have a new block. I'm going to call this a block with 64 filters, each of them doing one-by-one convolutions. And if I put this block somewhere in this computing chain, right? Then what we're going to do is we're going to effectively decrease the number of computations. And it will also be a way for us to decrease the number of features if we so desired. So I'm gonna give you two options. Just looking at the three-by-three convolutions, I'm going to tell you that we have the option of placing these 64 one-by-one convolutions after the three by three ones. Or I could put this filter in purple right before the three by three ones. I might have misspoke oranges. I put the 64 one-by-one convolutions after the three-by-three filters. And then purple is I put it before. Okay? So let's just worry right now about the question of which one is better from a computational expense perspective. So I want you to take 30 s to a minute to think about. Is orange or purple better in terms of reducing the number of operations in this backward. And feel free to talk to your neighbor. Alright, excellent, good discussion. This is not an easy question to answer in 1 min, so don't worry if you're right or wrong, but I'm curious who thinks it's better to put these one-by-one convolutions before the three-by-three? How about after? Okay. I would say more answered that. It's better to put it before and that is correct in this case. So let's see why this is. So what we wanna do is we want to calculate the number of operations. In the case that we put these 64 one-by-one convolutions before versus after and see which number is smaller. So we put it before. Then. The output of the 64 one-by-one convolutions will be 28 by 28 by 64 because we only have 64 filters. And then it'll be times the number of operations that it takes to calculate one output. And that's the size of this filter, which is one-by-one by the depth of the input, which is typically six times one by one by 256. And add to that, we then add the number of operations for this 192, three-by-three convolutions. And so that will be the output of the output size of this filter, which would be 28 by 28, by 192 times number of operations per activation, which will be three by three times the filter times the input depth, which is 64. So it's gonna be three times, three times 64. Any questions? Alright, let's do the after one step we put it in after the number of operations is going to be the output size, 28 by 28, we do the three-by-three convolutions first, so the output is 192. And then each of these is three times, three times 256 operations per neuron. And then we have the one-by-one convolutions. So the output will be 28 by 28 by the depths of those one by one convolution is 64 times one times, one times the depth of the entire layer, which will be when added to it, because that's the output of this three-by-three convolutions. Want me to review or go over either. Alright, cool. So if you look at these two numbers, Let's look at this 128 by 28 by one. I did save that here, and that's also here. We have a three by three by 64, gets a three by three by 256. And so we can see that if I just compare these two numbers, this one is smaller by a factor of four. Then if we look at these two numbers, 28 by 28 by 64 times one by one by 256 here, and one by one by 192 here. This number is bigger than one, I need to, but it's not for x bigger. Alright? So some of these numbers is smaller than the sum of these numbers. And therefore putting these 64 one-by-one convolutions before the three by threes results in fewer number of operations and then less compensation. So Daniel's question is, in this case the output filter max are still 28 by 28 by one by six sounding too. And remember the, the key problem here is that the three-by-three pool, it's always going to have an unmatched. In addition to putting these 64 one-by-one convolutions in front of the three-by-three or five-by-five. I'll put us the one-by-one convolutions to after the polls reduce the staff from typically 60, 64. Possible, in this case, the concatenation, it will still be larger than the conflict, but now it is possible to choose the number of doctors so that the concatenation is smaller than in this case if we were to calculate all of these. And I saw right before lecture that I have a typo here. This is 28 by 21, 28. If you were to do these operations. This case, again, the concatenated outputs on larger depth. But I could have reduced the number of filters to make that number smaller if I wanted. All right, I want to pause and ask if there are any questions. Great. The question is, where do these numbers but 192.96. Those are arbitrary hyperparameters that we get this. So we can make this smaller. Alright, so we have said that putting these 64 one-by-one convolutions before the three-by-three or five-by-five should decrease the number of operations. If you go ahead and do this calculation, which I've read snappier, then the number of operations in this inception module is now 271 million. And that's about a four times reduction from before we had the one-by-one convolutions where it was 870 million or something like that. So these one-by-one convolutions reduce the amount of computation by a factor of three to four. Alright, question for you all. Isn't there a concern that we are going to lose the information through these one-by-one convolutions. Basically, I see a bunch of students thought and gas jet is thought to be a difficult question. We definitely lose information because by doing these one-by-one convolutions, we're taking information that was across 256, 28 by 28 matrices that we're compressing it down to 64, right? So there is a very real concern that whenever we do these one-by-one convolutions, we won't be losing information. That may be now this three-by-three convolutions couldn't pick out because that information is gone after these one-by-one convolutions. So that is a concern. But interiorly. And again, this is one of those things where I say the empirical results to make the argument. The performance of these networks is still, it doesn't degrade. And it doesn't degrade. Meaning that this tensor, which was 28, 28 by 256, can really be dimensionality reduced. It can be squeezed down into 28 by 28 by 64 tensor and still contain the relevant information that will be necessary for performing classification. Alright? So, yes, information is lost when I put in these one-by-one convolutions, but empirically still do well with the additional benefit that would reduce the amount of computation. Yes. So the question is, why don't we just do three-by-three convolutions and have 64 and then e.g. right. Yeah. So there is one additional benefit of having this one by one convolution here, which is that we get an additional layer of nonlinearity will be given additional layer in a neural network. So this actually counts as a two layer inception module. And because there's another linear followed by ReLu transformation that also empirically helps to increase performance. The question is, how do we change the size of the feature map? Because we always have a one-by-one convolution. So in the Google net architecture, what they'll do is they'll have these 28 by 28 by hover feature depth layers at the output of the instruction module. And then they'll reduce the width and the height by using that spores. The spores will decimate by a factor of two every single time they happened. Is it weird to think of the inception module as a mini auto encoder. In each layer. I see where you're getting with that Daniel, that. So I wouldn't say it's on an autoencoder because it's not trying to reproduce the layer input, but it is a dimensionality reduction, which we also know is that picture of an autoencoder. Intuition of reducing information, squeezing it into the 64 is correct. Alright, so that is the inception module. And this is a figure from the Google Map paper showing through the entire architecture where you can see I've had to shrink it to fit it in this slide. But basically each of these things where you see like these rows of convolutions and the max pool. These are all your different inception layers. And many of them are something that are very deep neural network. Each instruction layer. This is the overall architecture in terms of the order of operations. So the column max pool, comp max pool, and then after that they have stacks up in sections, followed by next calls. Every single one of these inception layers. To write the depth of two comes from having these additional one-by-one convolutions. And so when you sum up then all of these inception layers and convolutional layers, you get that this is overall either 16 or in 19 bear neural network. Let me go to my fireflies, I tell you the right number. Oh sorry, 22 layer neural network for people. Then there's one more feature about That's interesting, which is, if you'll recall, when we talked about VGG at 100 million of VGG nets parameters, we're in the fully connected layers. At the output. Gets rid of those fully-connected layers entirely. So it has columns and max pools. And then you'll see at the end, all they have is they have a linear layer. This is necessary to get from your features to your 1000s softmax scores for each of the 1,000 classes and ImageNet. So that's just unnecessary linear layers to transform it to softmax scores. But aside from that, there are no other additional fully connected neural network layers here, but that's also probably kept the number of significant B since there's no FCS kick. Yeah, great. So just saw that there are three softmaxes here. Yeah. Yeah. That's a really astute observation. We're going to talk about that on the next slide. Yeah. Any other questions on the architecture here? Yeah. The question is in the max pool is bear, you mean within the interception module? Is there padding? Almost surely, yes. So I don't I don't know the stride and the cutting off the top of my head. So I'm going to ask rocks at their tongue what it says, calculate or look up in the paper what the straight into the setting of the players are. So if the infection Any other questions? Yeah. Yeah. The question is, what's the difference between average pooling and max pooling on in terms of like what might give better performance. I don't have a good answer for you. I'm pretty sure that in this case they probably just tested both in one byte for better performance. And the question is, why is Trump got only applied to one layer? As opposed to all layers usually applied on fully connected layers. And so that's why it's only applied on this last linear layer at the antipsychotics like a fully connected thing without the, without the activation. Other questions? We're going to answer Jake's question next switches. What are these additional softmax classifiers here? So we're going to discuss a problem with Google Maps that lead to them having auxiliary softmax classifiers. And that will ultimately be solved by our next and last Architecture top advisor. So this is the architecture of BubbleNet where I've reduced the inception of layers to these yellow blocks. And this two x means that there are two of the assumption layers, vaccines, there are five. Alright? Then I just also want to point out again that I'll extend them. Vgg net had many more estimators here, which is why they have so many more parameters and go that only has linear layer to the soft-max. That's why it has so many fewer parameters in this architecture. This architecture, this architecture has 22 layers. And what that means is that at the output, I'm going to calculate a loss. I'm going to call this loss L two for now. And how to update the parameters of my convolutional filters and session layers which include convolutions. I need to backpropagate through 22 layers. Right? I mean, we know that there's this fundamental problem of deep learning. Where would I do repeated iterations of my chain rule across 22 layers, grab my gradients, might vanish or explode. And therefore the gradients when I get my stayed at the activations in layer of wine were called H1. If I were to compute d, L2, the loss DH one, or this one could have exploded or vanished, right? And therefore, my weight updates and these earlier layers might not be good updates. And if the early layers aren't identifying the features, I'm never going to get good classification. Alright? So in some, because there are so many layers and backpropagated gradients. When I back propagate all the way to the start, can be very important gradients. And therefore, there will be no learning that happens in this architecture. Google Maps solve this by adding two auxiliary classifiers. So what they do, what they did is they took the layer 11 activity. Let's just pull that activity H 11. They took the layer 17 activity, we'll call that age 17. And then took the HL7 activity. And they put that through a couple of fully connected layers and then to a softmax. So this would give a loss L one. This loss, remember we called L2, I'm going to call this loss L. What this means is that when I back propagate, I don't just have a backpropagation through L2, right? But I'm now also going to backpropagate from L1. And when I do that, I'm gonna have to calculate a gradient at this point. We'll call this gradient D01. D. Alright? But now the gradient from L1 only has to pop back, propagate through 11 more layers. Alright, so what's 22? And therefore, the gradients from this pathway will lead to reasonable signals or reasonable gradients, sorry, to update the weights of these convolutional filters. So that by layer h 11, these features are good enough to perform your ImageNet task, right? Same thing for a 17. So you can think of this as reducing the effective depth of the number because it's saying by later HE, but then you have to have good enough features to do my task. Also by their age 17, you have to have good enough to just do my task. And then of course, throughout the entire network should be able to solve any questions on that. The question is, doesn't this add a lot of parameters? It does especially because there are these Fc next. However, when they then do inference, so when you want to actually make a classification, you don't need these auxiliary classifier. So this is only in the training process. But then our last, our final architecture or the final architecture they submitted for the competition is just the sector path. Great question is, how are the losses for L1 and L2 and L3 calculated? All of these are cross entropy loss. So these will be the same cross-entropy loss for by 1,000 classes of ImageNet. Just Try to decode it from just trying to classify from h 11 versus age 17 versus the entire network through losses. Are you aware that the question is, do we put equal weights to all the walk or do we wait L2 more than L1 and L3, I believe in the paper. They waited them equally with 0.3? Yes. Yes. That fingers from seven. Decent enough. It's a tomboy. Question is, if you have a good loss over there, 17, why do I need the additional five raters? So ignore this point for now. Because it says that it had a minor effect. But it could be that your loss after these five layers is less than the loss from the upper layer 17 activity. So that is the congregations in these layers can still be extracting out better features to make L2 and L3. You are using. Explicit function. Says In general, L2 should always be less than L3 because L2 gets additional parameters to reduce the loss. And that intuition is correct, although we're going to see that practically it doesn't always happen later on. So I can see that two events ingredients. But I would say, that's a great question. So the question is, this student says, I can see how this would tackle vanishing gradients, but has tackled exploding gradients. That's a really great point because if I backpropagated here, I say D L2, d h 11 by say also backpropagated through this path to get a deal three, d h 11, right? The total gradient at age 11 will be the sum of these three paths. If we have exploding gradients, then this thing is going to dominate, right? I'm just going to be a really big gradients that won't help learning. So really this type of approach helps with the vanishing gradients because in the case that they banished, this goes to zero, but we thought had good gradient signal from L1 and L3. So then the question becomes, well then how do we deal with exploding gradients? So dealing with exploding, I'm usually not as bad because we can use the gradient to things that we talked about in the prior lecture to always make sure that our critics have no norm bigger than whatever we said. That hyperparameter to. Your question is, could you actually use this as an ensemble to increase performance? You could. But then remember the ensembling works best when the predictions are independent. And in this case, because the predictions are coming from literally like the same pattern of network, there'll be more correlated. So ensembling is less likely to be beneficial. And given that they didn't ensemble trees, I'm not pretty sure it didn't help much. If at all. Other cups, keys. Yeah. Yeah. That's a good point. Yeah. So the student is saying that it's trying to reduce the computational expense by not using fully-connected layers, but they do have them in these auxiliary networks and so on. I want this increased training time and the answer is yes. Well, I've included them because I'm presuming that without these additional effects innovators, they didn't need this for computational capacity to do better. Their final numbers, they don't report these layers, but you're right that they are there in China and they will increase competition with other questions. Alright, so that is pupil that later work showed that these auxiliary classifiers minor effect and you only needed one of them. I'm presuming that means that initialization, alright? And then like I mentioned, these auxiliary classifiers are discarded and France, so when they actually did ImageNet competition, they only use the central path. Other details that was trained with stochastic gradient descent with momentum. The momentum hyperparameter was there a 0.9? They decrease the learning rate by annealing it by 4% every eight passes through the training data. They used 144 prompts per image, as well as other data augmentation operations. And then when they actually Competed. They trained setting to go next and averaged the results of this happening, bottlenecks or ensemble the results of the seven Google Maps. They did not use GPUs to train this network. And I have four or less do for fewer parameters as we know, then Alex and epigenetic because they don't have this fully connected layers and they want Initiative in 2014. Right there. That's my last slide on B woman. So any questions about people in general before we go to our last CNN architecture? Right? Is that a great? Yeah, The question is, this bottleneck outperformed VGG net with far fewer parameters? And is that a theme of trying to find architectures that are more parameter efficient and get higher performance. There are definitely groups working in that area. It turns out that after the next architecture we're talking about guys, that's there. We're than people who are combining the idea of resonance with Inception and those were running ImageNet competition is for years, but really the more modern day. The key architectures, even in computer vision, you use transformers and those are, those are pretty parameter unhappy. Other questions? Yeah, Tom, I ask these people ins"
"I put the input to the system being the direct delta. Then the output of the system is my impulse response. Alright. And so this function here, this is the impulse response. So very straightforward, intuitively again, what's the app on my system when I put an impulse, that's the impulse response. But we'll see that with it, we can do very powerful things. So last lecture I had mentioned this subtlety about the difference in the T's on the left and the right hand side of the equation. I mentioned this because I made this mistake when I was an undergrating this class. And so I just wanted to explain this one more time since just to clarify it. And so if I have my impulse response, H of T, which is the output of my system H when the input is an impulse delta of T, the T on the left hand side for H of T and on the right hand side, this T here are not the same. Right. So what's the difference? So let's take a system where I have an impulse delta of T, that's my input. And it's going through a system H. And now this system H in particular is an integrator. And so the system H is going to integrate this input. We derived either last lecture to the lecture before that the integral of the impulse is the step function. So the impulse response of the integrator system is a step function. So the step function is the output when my input is a delta. Right. So the T on the left hand side, which I'll do here in green, the T on the left hand side refers to the value of the impulse response at a specific value of time."
"I remember we did this question, we've given this question in prior years also. The TAs may go through it tomorrow. I'm happy to discuss 2A right now. That'd be great. Yeah, because I'm kind of looking at it and I'm not really sure how to even approach this. I hear you. So, let me go ahead and first copy over the Fourier transform properties because I've mentioned I sometimes remember them myself or I remember them incorrectly. So, for a transform properties. There we go. And again the derivation of these."
"I started off so far. So, I said if so in the slide, it was assuming FT was real. So, I said, FT is imaginary, then it's kind of like you just multiply in a J. So, the. Yep. Yeah. Yeah, that's right. Yeah. And then what Tyler said is integrate over a period. And then Tyler multiplied some real signal. Since f of t is purely imaginary, let's just call the other signal g of t. So here f of t would equal j times g of t, where g of t is real. And then we would have our And that's J sine two pi k over big T zero T, just like in the lecture notes. TT. So, now, we want to get the, just like an electric us we want to get the real part of Tk is going to be a 1 over T0 integral over a period. Whereas J times G of T times minus J sine is going to be purely real because the J's here are going to cancel."
"I wanna set the values of these so that they well describe the data. So for these plan ups, I want the mean to be in the vicinity of the points, essentially it's centroid. And here the covariance ellipsoid should be skewed so that it has a positive covariance between x1 and x2, because we see this general trend that when x1 is higher, x2 is higher. But then maybe P of x given c1, its centroid is here in the midst of the red x's, and its covariance ellipsoids are like circles, because there doesn't seem to be too much correlation between x1 and x2."
"I'll modify things, but you should be able to unmute yourself and talk. What we'll do is we'll still use the raise hand system here, but I won't need to unmute you. And if you want to interject at some point in time, as long as it doesn't become disruptive later on, then that's all good with me. And so I'm happy to have this very open class where you can ask your questions, where we can think of that things together. And I think that that's fitting for such an honor seminar. This class is one unit. And the way I gauge what the work in this class should be for something that's one unit, which is the following. ECE-102 has four units and about, or has seven problem sets. So what I've done for this class is I've designed this class of just have one project, which is given at the end of class. And it's roughly equivalent to doing two problem sets in ECE-102."
"I'll recap for now. Alright, so then, if we need to learn P of X given CK, then we need to choose what a distribution for P of X given CK is. And so then towards the latter part of last lecture, we decided that we were going to model P of X given CK with a multivariate normal distribution or a multivariate Gaussian distribution. In a multivariate Gaussian distribution, there are two parameters that set, that specify everything about the distribution."
"I'm a question. So this is like the first finding today on common filter and then starting dimensionality reduction. What are we going to be doing during V10? I assume we won't be spending like two and a half lectures on dimensionality reduction. Yeah, that's right. So next week because of the holiday, we only have one vector after this. And so during that lecture, we will finish off PCA. And that'll be likely all will cover for dimensionality reduction in detail. But we may talk about some ideas related to how it's attached to extent PCA to probabilistic PCA as well as vector analysis. I totally forgot about the holiday. Yeah. Yeah. And in a more typical offering of this class, we do get beyond when we get the vector analysis, we also talk about the expectation maximization algorithm, which some of you may be interested in learning. And even though we likely won't get to those this year, we have uploaded those materials into the lectures tab of CCLE. And so if you're interested in looking over that material, that material is available on CCLE. And and TA tonight would be happy to answer any questions about it. All right. Any other questions? Okay. Let's recap where we were. So last week, we did the derivation of the optimal parameters in a maximum likelihood sense for a linear dynamical system. Right. So we had our state update dynamics process and our observation process."
"I'm a question. So this is like the first finding today on common filter and then starting dimensionality reduction. What are we going to be doing during V10? I assume we won't be spending like two and a half lectures on dimensionality reduction. Yeah, that's right. So next week because of the holiday, we only have one vector after this. And so during that lecture, we will finish off PCA. And that'll be likely all will cover for dimensionality reduction in detail. But we may talk about some ideas related to how it's attached to extent PCA to probabilistic PCA as well as vector analysis. I totally forgot about the holiday. Yeah. Yeah. And in a more typical offering of this class, we do get beyond when we get the vector analysis, we also talk about the expectation maximization algorithm, which some of you may be interested in learning. And even though we likely won't get to those this year, we have uploaded those materials into the lectures tab of CCLE. And so if you're interested in looking over that material, that material is available on CCLE. And and TA tonight would be happy to answer any questions about it. All right. Any other questions? Okay. Let's recap where we were. So last week, we did the derivation of the optimal parameters in a maximum likelihood sense for a linear dynamical system. Right. So we had our state update dynamics process and our observation process."
"I'm happy to take questions in any order. Hi. I like to ask a question about the question 4 of homework 6. Okay. The comment figure 1. So I understand that on the journal lecture, would you write these formulas to calculate AC and I guess 7Q? But I'm confused about what does it mean to compute the values we calculated into a 5 by 5 matrix for example for 8? Yeah. And then let me just get my pad also. All right. So for part a, we have this vector where we have the x and y positions at time k and then the x and y velocities at time k and then 1. And then what we wanted to do was to fit the a matrix like you mentioned using the equation in class which is essentially the least squares bit."
"I'm happy to take questions in any order. Hi. I like to ask a question about the question 4 of homework 6. Okay. The comment figure 1. So I understand that on the journal lecture, would you write these formulas to calculate AC and I guess 7Q? But I'm confused about what does it mean to compute the values we calculated into a 5 by 5 matrix for example for 8? Yeah. And then let me just get my pad also. All right. So for part a, we have this vector where we have the x and y positions at time k and then the x and y velocities at time k and then 1. And then what we wanted to do was to fit the a matrix like you mentioned using the equation in class which is essentially the least squares bit."
"I'm just thinking sure that. So the book itself that we wrote the, uh, the width equals 12 thing, neat. Yeah. Yeah. Okay. Yeah. And so if you, if you, if you don't set the x positions to be 20, 40, 60, 80, but you said, 3, 1, 2, 3, 4, 5, then the, um, width equals 12 thing won't make sense. So make sure that you set the x positions, anyone watching to 20, 40, 60, 80, et cetera. Okay. Awesome. Thank you. I'm probably three now, but, uh, yeah, Zach, if you have any questions, go for it. All right. Sounds good."
"I'm not sure if you can answer that. I'm not sure if you can answer that. I'm not sure if you can answer that. I'm not sure if you can answer that. I'm not sure if you can answer that. I'm not sure if you can answer that. I'm not sure if you can answer that. I'm not sure if you can answer that. I'm not sure if you can answer that. questions regarding last homework? Yes. Also a question from lecture. All right, let me pull up the last homework. I ended up submitting last night but I had some conceptual things. Yes. All right, I have the homework up. Great. So this is in reference to the multi-ion species question."
"I'm wondering if I can ask a question about the latest homework? Yes, let me pull it up. All right, I had the homework up. OK, so I'm working on the last problem about the precision matrix. And so far for the covariance, I have a 4 by 4 that just has to add no terms. But I'm a little bit confused because in part C, I see that we're able to invert it. But my dad knows just turned out to be the variance of each input, so I feel like I'm not completely comprehending the problems. I see. Sorry, let me just really quickly read this. Yes, so I'm sorry. Edwin, you were saying that your precision matrix to the inverse of the covariance, you had just the variance terms on the diagonal? Yes, correct. OK, yeah, so there is something, I think there was a, that's not the question answer, so I think that B was probably incorrect."
"If I have a delay the impulse now by a time TOW, then my impulse response is going to be the exact same shape, but now delayed by a time TOW. Okay. And so this is a nice property that happens if we have a time invariant system. Of course, we know delay in the input means corresponding delay in the output and give me one second just to make the T is host. One second. Alright, so that's for a time invariant system. Now we're going to start to see how we can use the impulse response to show this fact that we had on this slide, which is as long, even if I don't know what the system is, as long as I know the impulse response, I can calculate my output for any input. And so before we do that, I just want to give one more terminology thing, which is something that is called extended linearity. This is a very simple idea. It earlier we had our concept of linearity, which is that if I apply a system, H to inputs X one and X to with X one X to being added, then if a system has linearity, sorry, A X one plus B X to if the system has linearity, this is equal to a times H of X one plus B times H of X to. And so I call this linearity, some people call this extended linearity, so this is for terminology. Extended linearity extends this definition of linearity to end input instead of just two. And so for extended linearity, if you have H applied to the sum of, sorry, this is correct. If you have, each applied to the sum of an, an XN, that's going to be equal to the sum of an, right, so this is this is saying that, H applied to A one X one plus A two X to all the way up to A and XN, this is equal to A one H of X one plus A two H of X to all the way up to A and H of X and. You can see definition of linearity applied to and inputs. And then the reason that we talk about this is that there's also continuous version analog, which is instead of having a sum here, we could have an integral, right. And so right here, we're writing linearity in the script form here. It isn't continuous form, which is to say if I take a system H and I apply it to a sum, now represented here as an integral of constants A, tau times. Some X of T minus tau, that's the same thing as taking the sum of my constants A times the outputs of the system applied to X, which is Y."
"If I want to calculate this integral, right, all this integral then is would be mu of t. And we're going to need to calculate this because we have, we know how to calculate this probability from our definition of the Poisson process, because you'll recall in our definition we said that the number of spikes in a window is going to be Poisson distributed, where the mean of the Poisson variable is going to have a mean, which is the integral of the rate function between t and t plus s, that's just this quantity here, so it's going to be the mean mu of t plus s minus mu of t, raised to the number of spikes that we see in the window. And so the number of spikes that we see in the window is zero, so this gets raised to the zeroth power, times e to the minus the mean, so that would be mu of t plus s minus mu of t. And then all of this is divided by the number of spikes we see, which is zero factorial. All right, all I've done is I've written my Poisson distribution here."
"If knowing one of them does not give you any additional information about the other one, but you didn't already know, right. So when we're saying that the Poisson process from zero to N R is independent of the Poisson process between S and T plus S, what we're saying is that for example, knowing the number of spikes that happen in this window tells me nothing about the number of spikes that are going to happen in my window between S and T plus S. And then we mentioned that we would give the intuition over this property and you're not going to be responsible for the rigorous proof, although we have it on the next page and I'll go through it at a high level. But to show that starting from NS and looking forward to show that we have a Poisson process, right, all we have to do is show that our definition of the Poisson process holds and our definition of the Poisson process is that it's going to be a process where events happen at IID into arrival times or IID, ISIs, each of the ISIs being drawn from an exponential lambda distribution. A question from Andrew."
"If this is correct, then the peak of the cosine should occur at time t equals 1. However, if I plug in time t equals 1 here, so at time t equals 1, when the cosine should have its peak, this function is equal to 4 cosine of pi minus 1. All right. But we know that cosine has its peak when the argument to cosine is 0. Cosine has its peak at time t at when the input is 0, but pi minus 1 is not equal to 0. So this here must be incorrect. All right. So I want so much to write or someone can someone raise their hand and tell me why this is incorrect? Because I did what we taught last vector, right? I shifted the time over by 1. So why didn't this work? Salvador. Hi. So it wouldn't work because pi minus 1 is not 0 or pi, which is at its peak, which is 1. Cosine of 0 is 1. Cosine of pi is 1. It would be somewhere between 0 and 1. Cosine of pi minus 1. Yeah, that's correct. Yeah. So then that's a good explanation for why this is not the correct answer. I'm going to now go to Rampton and I'm going to rephrase my question. So in terms that we know that this is an incorrect answer, why was it incorrect to just subtract one here? And so I'm going to go having ask Rampton. Okay. So because you have to subtract one from the parameter T, meaning that you have to replace T with T minus 1. So that 2 pi times 1 half will be multiplied by T minus 1 and you have to distribute that pi into negative 1 as well."
"If we were to involve X of T and H of T, right? So we flip and drag this h of t here, right, this h of t is going to when we do a flip and drag is going to lead to a smoothing out of the abrupt corners of these racks. the function or in the frequency domain deviating from a straight line. Any questions there? All right, so then phase distortion is a bit more tricky to conceptualize. And so recall that our frequency response, h of j omega, can be written as the magnitude of H of j omega times the phase, e to the j phase of H of j omega. And so when there's phase distortion, what this is telling you is that the phase at different frequencies is going to be different. Right. And so, if I have. And I derive this for a decomposition, right, and the for a decomposition tells me that this is going to be comprised of science and cosine so maybe this is a low frequency cosine."
"In essence, this is something that I might think of as like a design choice. The reason that I would use x2, x3, x4, x5 is if I was under the rationale that I should use the most recent data. So if I need to calculate Y2 minus Y1, I've had kinematic data up to time bin 2. And I could say that this is the derivative that corresponds to the data time bin 2 therefore. So like this is the derivative at time 2. So there I'm defining that x prime of t, x prime of t would equal x of t minus x of t minus 1 over delta t. Where you don't need the delta t because the delta t is just a constant. So that's what I would do. But in truth, you could do any of these as long as you give a rationale for what you choose. I should say for this project, you don't have to give a rationale. But in other settings where you make a choice like this is good to make sure that you have a reasonable or a reasonable rationale for your design choice. Great. Any other project questions? Yeah, I think I think I did number three like this. Do we also have to do like the Y2 minus Y1 values for the Y test values? Yes, you do. Yeah, so this is related to a question. I think Brianna asked last week, which is when you make a change to your Y data in training, you want to make sure that that change is replicated everywhere and test as well. But because you want your test data to have the same statistics and distribution as your training data sense, you're expecting that your test data will look like what you change your decoder to do. So when you do these differences or when you do any smoothing, you want to make sure that you also do it to the Y data. Okay, so I think I think I did it right. Question mark. And the like I grafted and the resulting graph was like just a bunch of dots. Yeah, so I think that we said. Yeah, so I think I remember this from last year also. So I put on this question because I was at a conference one year and a researcher at another university told me that they had good results when they when they use the derivative to decode. But I believe that if you just use the derivative to decode, it performs poorly, although someone else can write in chat if you've done this to see if that's actually the case."
"In practice, is there a certain amount of complex exponentials that would be, I guess, quote, unquote, good enough to approximate a certain signal? Because we could never get an infinite amount which would perfectly approximate or perfectly simulate what a signal is. But would there be like a kind of like a certain amount that would approximate it close enough? Yeah, that's a great question. I am not aware, although it may exist and I just may not know it. So if the TAs know, please chime in. I am not aware of a general formula that tells you if you use K complex exponentials, how little your error is going to be. Like me is going to depend on several properties of the signals. For example, discontinuities make it harder to approximate. And so in general, I believe that the answer is a signal dependent. And so I am not aware of any general equation that will tell you how many times you need."
"In this case, we have two classes when the monkey plan to reach to a right retarded and when the monkey plan to reach to a left retarded. And those are red circles and blue, sorry, red X's and blue circles. And last lecture, we talked about how we were going to model this by having the distribution of the data coming from each class, being this multivariate Gaussian distribution with a mean and a covariance matrix. And then we said now from training data, which are examples of the monkey planning reaches to targets where the neural data during the reaches X i and the target identity is ti. We're going to try to learn the parameters of pi pi is the proportion of plan left targets. The means of these distributions and the covariance, right. And these are the parameters of my model and I get to choose them to make the data as likely as possible. Right. So last lecture, we derived the data log likelihood. That's this expression here. And we had this log of a normal distribution that was this expression over here. And we said we were going to now differentiate the log likelihood with respect to pi mu zero mu one and sigma to find the optimal parameters. And last lecture, we had done the log likelihood, the pie and got that pie, which is the number is the proportion of the probability of there being a left trial was just the proportion of left trials capital and one over the total trials capital and. Right. And the tackle and zero is the number of trials that were planted, but right target. And then we then differentiated log likelihood with respect to new one. And we found that new one was equal to one over the number of trials where the monkey planned left times the sum of all the neural activity when the monkey planted a left. And so this ends up being the sample mean of the neural data when the monkey plans to the left. And so that's the centroid of this cluster over here. All right. And we said that that made intuitive sense and there was a maligous answer for mu zero."
"Is it because V and A are independent and we've already shown that so it was a model? That could be the case. Yes, it is. Thank you. Okay. Yeah. So in part D, we showed that A and D are independent. And so actually this upper one would also simplify to P of D given A. So you can make an argument there. Yeah. So that's the case, P of D and P of D given A are the same. And so it doesn't define this way. Okay. Okay. So then that's how I can go to P. Okay."
"Is not overwhelming. All right, so. I'm going to just. Overview where we were, which is something that you use for the project, which is in the project. I'm going to give you data from a monkey and. The monkey has simultaneously recorded kinematics and neural activity. And in the 189 project, we actually get out this kinematics and that neural activity. So the kinematics were the R dot cursor pause. All right. And then. The neural data is the R dot spike rasters. And so. We also talked about how we've been the data so that we get the. The velocity kinematics every 25 milliseconds. And we similarly get the neural data every 25 milliseconds by counting the number of spikes that occur in a thing."
"Is that essentially reading a neural signal? And then corresponding that to what a person, what movement a person would be trying to make and I would give off that signal? Is that okay? Okay, cool. Yes, I'm sorry, what is your name? I try to remember names for this class, and this is smaller than I can manage it. I'm Blake. Okay, thanks Blake. Actually, I realize that it might be more difficult because I don't think I see videos. So I didn't think about this beforehand. You don't have to turn on your videos if you don't want to, but if people want to turn the videos, that could help me to remember your faces. All right, yeah. So Blake, your question is for this particular problem of brain-mishing interfaces, yeah, the movement corresponds to the intended movement that someone might want to make."
"It goes from the top of the curve to the lowest point. That's correct. We haven't covered this up to the hyperpolarization of the action potential. We have a particular example of the resting potential 50 plus to EK plus. For two B, when they say that the action potential has a 110 millivolts peak to peak, that should be related to part A. You were able to calculate EK plus and ENA plus and you know that the voltages can't go beyond those bounds. If you've calculated in part A, that the range from EK plus to ENA plus is less than 110, then you know that there's a contradiction there. I couldn't understand the relationship between the two parts. I had another question about myelination. Maybe this is not as related to the homework, but what is the difference in terms of specifically demyelination for Parkinson's disease and multiple sclerosis? For Parkinson's disease, so for multiple sclerosis, I believe that, sorry, let me just look at the Parkinson's demyelination. I don't know the answer off the top of my head, but I wonder if for MS, it's for primarily motor neurons. For Parkinson's, Parkinson's is implicated in some of those deep brain areas. I wonder if it has to do with neurons. I think it's the locality of the neurons. Yeah, it's actually specifically, I think this is a stancho Niagara."
"It is. We should remove that in future homework. Yeah, we're just asking what the purpose of these nodes are and how do they function. Great, thank you. Great, thanks, David. I also had a question on 5A. So for the area for, so both the resistance and the capacitance use an area, and I want to clarify that in this scenario that they're different, right? That is correct. Yeah."
"It says that it wants to plot the expected firing profile determined by the union equation, as well as this histogram on the same plot. Yeah. But for the histogram, since there's 50 bends of 20 milliseconds each. There's like the X that length of the axis 50. But if I'm plotting the expected firing profile over a second. And the like times and milliseconds on the axis of a thousand. I'm unsure how to. Yeah. So you will want to sample the, you'll want to sample equation one and two also every. Every 20 milliseconds. Okay. And so let me share my screen. That's good. Cool. So even though you have 50 bins and they go from zero to 1000."
"It takes on the value one. And then when T is positive. Time is growing. We have a decaying exponential. And this is an even signal. We're taking the absolute value of T. So when time decreases, we also have a decaying exponential like this. And so this is our F of T equals E to the minus a absolute value of T. And last lecture, we had derived the Fourier transform of F of minus a T times U of T. F of minus a T times U of T is the signal that is zero. Until time T equals zero and then it rises up and then is E to the minus a T. So it's this signal. So if I want to find the Fourier transform of each of the minus a absolute value of T. Then what I can do is since I already know each of the minus a T times U of T, the Fourier transform of this signal which we derived last lecture, I can write my F of T as being equal to E to the minus a T U of T. And then what I could do is I could add to this signal."
"It'll be on Monday, May 3rd, 2021. Earlier today, I sent out an announcement with details about the midterm, including how we're going to distribute it, how we expect you to upload it by 4pm, as well as if you're in a different time zone or you received permission from me earlier, how we'll schedule your other exam time. All right. The midterm is going to cover material up to and including croissant processes and so we're going to finish croissant processes at the first part of lecture today, and that'll be the end of material for the midterm. We put all of our past exams on CCLE and so you're welcome to look at those midterms, of course, to prepare for this midterm. With the following note which is that the first midterm that we gave in 2017, which I think Tom Wael said that was, was extremely long, way too long for a midterm."
It's a special case of the Laplace transform when s is just equal to j omega. So when sigma equals the row. All right. And we talked about then when are these two things the same or when can I just take the Laplace transform and replace all the s's with j omega's so that I get out of Fourier transform. So we started this example at the end of last class where we decided to take the Fourier transform of e to the minus a t times u of t. Its Fourier transform is 1 over a plus j omega. And when we took the Laplace transform we got the Laplace transform was 1 over a plus s. All right. And so we see that the Fourier transform and the Laplace transform are indeed of the same form except in the Fourier transform the s is replaced by j omega. All right. But we notice something interesting which is that in the course of doing this integral to get to 1 over a plus s.
"It's just a view of it. One second later, but it ends up being. It ends up being this is ends up being the picture of the steady state, because of this opposing na plus k plus pump which we'll talk about next lecture, but essentially, this pump is going to let me get my annotator. And there's going to be one more thing here, called the pump. And the pump is going to push out any plus. And it's going to push in K minus, sorry K plus. And so for that reason, the concentration gradients are going to stay the same such that this arrow, always stays the size. So this is a picture of the steady state, but it's only the steady state when you also consider this, this NA plus K plus pump. So the chemical driving force on B and C are for the last channel. Like should be in the same lens right for the figure. The blue, the blue arrow. It seems like the part C is longer. I don't know. Maybe it's just a mistake. Oh, I see what you're saying. Yeah. Like this arrow looks a bit shorter than this arrow, right? Yeah. I think you're right. It is shorter. It should not be shorter. It should be the same length, right?"
"It's like they require more force to get through. Yeah. So the channels are primarily K plus, and a few and a plus channels but primarily K plus. The thing is the the new equilibrium if you didn't have this sodium potassium pump would be likely a higher voltage. I'm not sure what the new equilibrium would be without the na plus k plus pump, but it would be a different concentrations and if it were at a higher voltage then, you know, if the nervous system didn't have this na plus k plus pump. I imagine there is an answer. Since, again, yeah, these blue arrows will be changing to reach the actual equilibrium, this blue arrow, either this blue or this orange arrow has to oppose the other one. Yeah. Or like the some of them eventually right. like there would be a constantly changing concentration."
"It's like, without it, it would take a while like without it, it's just natural processes. So it would be it's more efficient for the neuron to like, get there. So they can fire for the get ready to fire for the next round. Yeah, and then it'll interact with other things like if the if the at the equilibrium, if there is an equilibrium without the NA plus k plus pump, and it causes the equilibrium voltage to be for example higher than, then I guess like the the voltage gated ion channels that have evolved in a different way. Because, yeah, so so at least this is a system that we have now. how this, if this would reach a different equilibrium without this pump. Right, okay. And then to clarify about the pump too, the pump is constantly sort of working throughout the whole action potential, its effect is not really observable."
"It's not that it's very hard to precisely stimulate one cell and not the other. Sorry, I'm just noticing that my keynote is lagging by quite a bit. So let's say that I want to stimulate this cell over here. If I were to send electrical stimulation on this electrode right here, you can think of this as we're hitting a very finely tuned circuit with a hammer. When I send an electrical current into this circuit, that current, Audron Red, is going to propagate out in all directions. And it's very hard to just stimulate this one neuron that we want here without stimulating all the other neurons. And so actually a challenge of this system is how do I, I know what I want this neuron activity to be like, let's say the top neuron here. How do I design stimulation patterns to stimulate just this neuron and not the others. And so an example of an active research area in here is to design stimulation patterns across your different electrodes. And that for example, maybe this third electrode sends out this electrical signal that emanates out in this foggy metric circle. Maybe the electrode next to it sends another signal electrical signal that also emanates out. As you can see here, the red and the purple electromagnetic waves are going to start to interfere with each other. And you could have constructive interference and destructive interference. And so you might think to try to pattern this stimulation on these electrodes to just target particular neurons that you care about in the vicinity. It's a difficult problem, but that's an example of how people are trying to solve this question."
"It's time consuming to make the transition from that lab to Python. So please factor that into your work. I think it's worth it because Python is the de facto standard for machine learning and signal processing today. I also mentioned this last time this class according to student feedback is a lot of work. And so I want to state this upfront so that you can plan accordingly. We do try to have this class be very fun, but we cover a bunch of topics and test it understanding. So we have seven homework assignments and we're told that those assignments are somewhat time consuming. All right. Like the mentioned piazza should be the primary names of asking and getting questions answered. And we just talked about this on the last slide also so I won't review that again. Any questions here? All right. So with that. Sorry. It looks like I have that was not last notes. I have a few more slides. For those coming with a background in machine learning. I want to say that up front since we start off doing neuroscience and we do derive maximum likelihood solutions for classifiers and regression in this class, which is something that may have seen before."
"It's time consuming to make the transition from that lab to Python. So please factor that into your work. I think it's worth it because Python is the de facto standard for machine learning and signal processing today. I also mentioned this last time this class according to student feedback is a lot of work. And so I want to state this upfront so that you can plan accordingly. We do try to have this class be very fun, but we cover a bunch of topics and test it understanding. So we have seven homework assignments and we're told that those assignments are somewhat time consuming. All right. Like the mentioned piazza should be the primary names of asking and getting questions answered. And we just talked about this on the last slide also so I won't review that again. Any questions here? All right. So with that. Sorry. It looks like I have that was not last notes. I have a few more slides. For those coming with a background in machine learning. I want to say that up front since we start off doing neuroscience and we do derive maximum likelihood solutions for classifiers and regression in this class, which is something that may have seen before."
"It's time consuming to make the transition from that lab to Python. So please factor that into your work. I think it's worth it because Python is the de facto standard for machine learning and signal processing today. I also mentioned this last time this class according to student feedback is a lot of work. And so I want to state this upfront so that you can plan accordingly. We do try to have this class be very fun, but we cover a bunch of topics and test it understanding. So we have seven homework assignments and we're told that those assignments are somewhat time consuming. All right. Like the mentioned piazza should be the primary names of asking and getting questions answered. And we just talked about this on the last slide also so I won't review that again. Any questions here? All right. So with that. Sorry. It looks like I have that was not last notes. I have a few more slides. For those coming with a background in machine learning. I want to say that up front since we start off doing neuroscience and we do derive maximum likelihood solutions for classifiers and regression in this class, which is something that may have seen before."
"It's. Trusted at in time. And so if I want to reconstruct this blue signal, I would use co-signs that have. A relatively low frequency, right? They have a very large period. So for the blue signal, I might expect the spectrum to look like. This because all I need is low frequencies. All right. And then the the amputees are just following this one over a scaling here. Okay. Any questions there? All right. So if you for if you don't recall this proof, we we derived this last lecture. So feel free to look at the last video to recall this. We're going to do a bunch more proof today. And so the next operate. So today we're going to go through all of these properties."
"Let me go ahead and walk us through this time. So here, we need a relation for xi and xj. And so if I were to write this expression out, let's just focus on the boundary equation. What I'm saying is that xt, given xt minus 1, is normally distributed with mean xt minus 1 and variance sigma squared. In this case, the number xt minus 1 is an observed quantity. It has no randomness. And so the way that I can simplify this equation, xt given xt minus 1, being normally distributed with mean xt minus 1 and variance sigma squared, that's the same thing as saying that x of t is equal to x of t minus 1 plus a random variable, I'll call it w, where w is normally distributed with mean 0 and variance sigma squared. Now why can I say this? It's because of following, if I take the expected value of xt given xt minus 1, that's going to be the expected value of xt minus 1, xt minus 1 is an observed quantity. It doesn't have any randomness, so it's just equal to xt minus 1 plus the expected value of w, but the expected value of w is 0."
"Let me just. Part one of the project is to build a decoder with with a low pass filter, which will do in the slides. Part two is to use an intermediate or high pass filter. So it should not be too difficult to generalize the code that we're going to look at today to do part two. And then part three is more open ended. So we give. We give the idea to say what about other features of the neural data like the derivative. And so, and so we'll. We leave that a bit open ended to you all. Okay, so if you were to load, if you were to open the assignment code plus part one, what we have here is a bunch of map code, which walks you through the data set. So we're going to go through that all together now. I can't see the chat and I can't see raised hands. And so if you have a question, just unmute yourself and please, please feel free to just ask it. So what I'm going to do is I'm going to go ahead and load this data. So I'm running this first cell. And what you should see is that it drops. A data variable called big R into our workspace. Right. And so what big R is, is it is an array of structures. And so if I just type R. What you're going to see here is that R is a one by five hundred and six structure ray with these fields. Right. So you may not be familiar with trucks or raising or you supply via familiar with the raising that what this means is that R is something an array that has five hundred six components. So I can reference R of one. This is the first component are I can reference R of two. This would be the second element of R. Now every single element of our itself is a structure. And what a structure is in MATLAB is it's. You can essentially think of it as a data structure that hold some custom data that we wanted to write down. Right. And so first there are five hundred and six of these structures because again R has five hundred and six elements."
"Let me pull it up. All right, I have it up. So I was going through problem 4C, and I had set up the integral in the same way that you did in office hours, but I was looking through Tanmoy's notes after, and I don't think I get the same response. So like, I understand how he went through it, because he did the, he used the equation of the expected, expected value with the summation, and then turns it into an integral but when I try to solve for that with the bounds of being one over lambda to infinity. I get to over lambda is the correct answer. Oh, okay, then I guess I'm misinterpreting. Yeah, I can. I haven't seen Tom boy slides. With, did you say that that was for 40."
"Let me run this code again. I can see the traffic. All right. So do you all see a plot now that has appeared? Yeah. Yes. So this again is iterating through all 506 trials and it's plotting the target location of that the monkey was reaching to. I can see that there are only nine unique targets. The target is going to be either zero zero and X position. 84.85 comma 84.85. This would be the upper target, which is zero and X position and 120 millimeters and Y position. And so there are only eight unique targets at the monkey reaches to at the periphery and then center target. All right. So those are the targets the monkey reaches to. We also have fields like if I were to do our. We must look at the 20 trial again."
"Let me, let me tell you a simpler way to set it up. So, essentially, what you can do is you can consider this as essentially a coin flip. So, you flip a coin and either the spike that comes will have an ISI less than the mean ISI. And so, what you want to know, so let's say that tails is tails is when you see an ISI less than the mean ISI. So you want to know how many tails are going to flip before you get ahead. Does that make sense. Okay, yes. And we know the probability of tails and heads because we calculated it in earlier part, I believe. Probability of T greater than one over lambda, we calculated to be one over E. Yes."
"Like I think about or like when you raise your right arm is something that haven't automatically and that's the goal of boy, you won't bring machine interfaces that your limb would be controlled in a similar way to how you would have controls your native arm. So your robotic limb would go up when you want to move, when you want to move your native arm up. And that's called a biomatic brain machine interface. A biomatic because it's exploring the same types of, uh, motor commands that would have originally moved your arm. And so they can fully these things should be relatively naturalistic. That's the goal at least. Okay. Any questions here? All right. So this project is one of is also was also later funded through funds made available by President Barack Obama's Brain Initiative, which has been really critical for research in this area."
"Like when I was first seeing this, I thought it would be more natural to have like the VNB associated with the positive side. No particular reason, just convention in the field. So whoever first measured these, put their positive electrode inside the cell. And so since then we've all been defining it going from inside to, excuse me, inside to outside. If we were to just reverse everything, everything would be consistent, but all the signs would just be negated."
"Normally when you hear sound, we talk about this in the first lecture. What happens is there's a pressure wave that's being sent to the air and that pressure wave is going to vibrate the liquid in this cochlear duct. And that vibrating liquid is going to move these hair cells left and right. And so the hair cells will be activated. And that will cause an electrical signal to be sent to your auditory cortex. So then you hear a sound of that frequency. And the really interesting thing about the cochlea is that it has so-called tonotopic mapping. Which means that if you activate hair cells here, it's the perception of a 20,000 hertz signal. And as you go down the cochlea, these hair cells activating give off the perception of a 3,000 hertz signal. Hair cells over here give off the perception of a 200 hertz signal. Hair cells over here give off the perception of a 600 hertz signal. And so as long as you can get the spectrum of the signal, as long as you know what its frequency components are, then you can replicate the sound by stimulating the hair cells in a particular area. So you can imagine if you're listening to someone speaking, you can have a device that records the audio. And then after it records the audio, it does a Fourier transform on the audio. And that Fourier transform is this operation you will all know very well through class. And it takes you from the time domain to the frequency domain where I know as a function of frequency, how much power is in a signal. And so this signal would have very high power at low frequencies, the lower power at higher frequencies. And so if I could take the Fourier transform and know what these powers are, then I could say, OK, let's say that this was centered at 300 hertz, then I could stimulate here with a large power. And that if this was, let's say 1000 hertz and I could stimulate over here with a bunch of power. And so that's how cochlear implants work. Any questions there?"
"Not unlike how for a transistor, they all have the same components, but you can, and you can, sorry, much like for a transistor, you can have different whips and lengths, but they all have the same components and then computational complexity and capacity come from how you hook up these neurons together. And we were beginning to talk about how the neuron is a fundamentally, fundamentally electrical system. And so we talked about how if you have a neuron and here we're showing here inside the neuron and here outside the neuron and you were to stick two electrodes in one of positive electrode and the other are ground. So I put in two electrodes, then if I could measure the voltage from inside to outside the cell, which I call the membrane potential, VM, that this is a voltage that will give you a reading of about negative 65 millivolts. All right. And we talked about how this voltage is present because we have both positive ions like sodium ions and potassium ions, as well as negative ions like chloride ions that are in the extracellular and intracellular fluid."
"Of course, we're also dealing with the pandemic and with online teaching. And we understand how that could lead to homeworks taking a longer amount of time. Given the poll that was on Piazza, the TAs and I discussed how we responded this. And we propose to do the following, although I'll be happy to take any other feedback. First, although we've released homework number three already, what we're going to do is effectively shorten it to be 80% of the original length by giving you full points to these three questions on the homework. So these three questions comprise 20% of the homework points. And you don't have to do them to receive full credit. And so if you left these questions blank, you'll get the full 20% points on those questions. That said, we still encourage you to understand how these questions are solved and to be able to replicate their arguments because they're very game for the exams. The TAs and I will also look to shorten future homeworks. And so we'll try to write for example, less sub parts for some of the questions. And then the PAs also communicated that feedback received in discussion is that MATLAB is very time consuming. And so we aim to, although we're still starting out the details, we aim to distribute a MATLAB dedicated discussion video each week as well to help with the MATLAB part of the homework. All right. So I wanted to take a pause here and ask if there are any questions about any class related logistics."
"of T squared, F T of T, DT. Whatever is in this expected value bracket is the term that multiplies the PDF in this integral. And we won't do the calculation, but if you do the integration by parts, which again is in the posted lecture notes, you'll get that this is equal to one over lambda squared. You can see that this answer ought to make intuitive sense to what is what this is telling you is that as lambda gets bigger, the variance of my spike times is going to get smaller and smaller. Right. You can see that in the blue, in the blue lambda is small we have a slow decaying PDF like shown here in green, and the spike times can take on values across a much larger range and therefore have greater variance."
"Oh D, right? Yeah, it's d also. So in d we give you the magnitude and the phase, right? So if you were to go out ahead and write this out, magnitude and phase, right, in d we have a number where x of j omega is going to be equal to the j pi over 2. And we know that e to the j pi over 2 is just equal to j. And so this signal is just j times a rect that has a width of 2. So it's t divided by 2. Sorry professor, is that the magnitude times e to the j pi over 2, is that just a definition for that? It would be then equal to, so the magnitude of x of j omega is a rect, that should be omega over 2, rect of omega over 2, and then the phase would be e to the j, and then the phase is pi over 2 for all omega, so it's just pi over 2, and that's equal to j times rect of omega over 2, and rect we know is an even signal. I mean, you can just look at it, it's an even signal. I didn't need to say that. And so we know that j times rect is also an even signal."
"Oh, I need to share the audio. Let me just share my screen one more time. But with the audio shared. audio share. So this is a lab tour from from my PhD advisors lab at Stanford, and it starts off by showing the inside of the lab space. And so, let me sorry let me make sure I did pause the recording. Okay. Yeah, so, I'm here, what you see is a monkey named George, and George is currently sitting in an experimental rig where we're recording from neural data. And he's doing a reaching task. And so, you'll see that he has essentially an implant here. This is dental acrylic that sits on top of his skull."
"Oh, I see. So you multiply then. Yeah, okay. Yeah. So for 2a1, we should get the answers a, d, and e. Let me make sure that's correct. Yep. That's correct for 2A1. Sorry, one more time. So from that multiplication, J times rect omega over 2, how do you know that that's even other than just looking at the graphs? Yeah, J times rect omega over 2 is a signal that starts at minus. Oh, okay. Sorry, I think it's omega over 4, sorry. Because the width is 4, sorry."
"Oh, okay. And then what you'll see is that all this is is a Poisson distribution, summed over all potential values of k, and so this whole sum will equal one and then the sum will go away. This is equal to one. Oh because it's exponential? Yes, plus on and you're summing over all potential values and we know that a probability distribution when you sum over all potential values has to equal one. Okay. Okay, thank you. That helps a lot. Cool. Yeah."
"Okay, and so we found that the Fourier transform of one is two pi delta omega, which is just saying that the Fourier transform of one is a signal that only has a non zero value at omega equals zero. And so this solution here corresponds to, or makes sense intuitively. Okay, any questions here? All right. And then we also asked, well, if the Fourier transform of one is delta omega, there's a similar analog for when we have delta, what happens if our Fourier transform is delta omega minus omega not right. So the sum of some signal is delta omega minus omega not and we calculate the inverse Fourier transform, we do this approach because then this gives us a delta in the, in the integral from which we can use the properties of the delta to compute its inverse Fourier transform, we get another Fourier transform pair, which is that the Fourier transform of a complex exponential is two pi times a delta omega minus omega zero. And so this answer should also make sense intuitively because if I want to construct a complex exponential with frequency omega zero, right? All I should need in my frequency domain, in my Fourier domain are sines and cosines that have a frequency omega zero."
"Okay, any other questions. Right, and then are we submitting it to grade scope? Yes, the exams will be submitted to grade scope. All right. And so what I was just saying about if we, the scale may be relaxed, but it will not be made more stringent. And then I'm writing this because I sometimes receive these requests at the end of class. I will not make a change to your final grade unless I made a calculation error. And so please do not send any requests of this nature and I will not reply to emails making such a request. And then you have the option of taking this class. If you're an undergraduate student pass or no pass or if you're a graduate student satisfactory or unsatisfactory. And I just wanted to copy down the UCLA registrar rules for what is assigned a grade P and what's assigned a grade S. Any questions. All right. So, as we mentioned before, the homework has a significant weight in this class. I personally believe that a lot of learning happens on the homework. And we will strive to give homeworks where there is a lot of learning that takes place. So, the homeworks, we try to make thorough and instructive on the flip side. They end up being sometimes a lot of work and time consuming. And so I want to give you a fair warning about that to please start the homeworks early. The upside to this is that we design the exams so that they are at a difficulty no more than the homework."
"Okay, any questions from last lecture just to follow up on anything about any properties any Fourier transform where when we just apply the definition naively. It doesn't work out. Right. So, these will be the generalized Fourier transforms that we discussed today. We did talk about a few of them so a few of them, we can do some straightforward computation. If we use the Dirac delta and we apply the algebraic properties we know of the Dirac delta, we found that the derivative of the Dirac delta is 1. And we said that this makes sense intuitively because the Dirac delta is a function that is 0 and then changes infinitely fast to be of an infinite amplitude. All right. And so, to create that infinitely fast transition we're going to need sinusoids with frequencies that approach infinity."
"Okay, I see. Thank you. Great. Let's go into. Let's go for the next questions in the order of hands race so I'm not sure. Okay, I think it's, I think it is in order so Chase is next. Hi, I just wanted to like, talk about five and see if my approaches correct. Sure, yeah. Which one of number five, or which. would just be multiplying them because they're independent, and then it would be one minus the CDF of both the first neuron and the second neuron."
"Okay, I'll record the second half of this. Makes sense that once you cross connection, okay, I'll record the second part of this. Thanks Jeremy. Alright. So to turn this into a summation, I will transpose this quantity with itself. I'm going to share a bit in writing and just copy and paste. This thing I wrote, copy and paste. So if I take this thing transpose, but the cell is going to implement this summation. Alright? We're going to check the dimensionality. So I'm gonna call this vector of the water wants to buy anyways. I'm going to call this quantity a big matrix, Y. And Y we can see is a big m-dimensional vector. We'll call this thing here big X. Someone told me what the dimensionality of the axis. Once students have any other takers, heard, someone say n by two. Yes. So remember that because I'm one step here, that each x i is a two by one vector. When I transpose it, it becomes a one-by-two. And I start to begin again. So this is in our big N by two. And then I theta isn't art here. So this big X times beta will indeed gives you just an m-dimensional vector back. Each element of an n-dimensional vector is one of these terms. And so if i.it with itself that implements this entire nation, any questions on that vectorization? Alright, so this will be like playing around with something in a vector or a matrix for that removes the formative and it'll automatically scale computation. You'll do this in homework number two. Alright, so this is going to be equal to one-half. And I'm going to write this as big y minus x Theta transpose y minus x Theta. Lastly, I could just do oil to write out all these terms. So this is equal to one-half, will have a Y, Y, Y transpose Y minus Y transpose X Theta minus Theta transpose X transpose y minus Theta transpose X transpose X Theta. Doing like foil for these two expressions. Sorry. Thank you. Questions here. And just say, thank you for the questions ahead of time. I definitely will make mistakes while I'm lecturing up here. So if you ever see some mistakes that I make, feel free to just call it an ***. Alright, so what I'm going to do is I'm going to take this expression. Remember that this is 0. And you can see here I combined these two inner terms into just one to y transpose X Theta, right? The reason that I can do that is again, because these expressions are scalars and do the transpose of each other so I can combine them. The same exact logic that took us from here. Alright, so this thing is equal to ls data. Tomboy is referring to this expression here. Yeah, tomboy is pointing out one way to write this notationally is as follows. The two norm of y minus x Theta squared. And Tom, I also raised something which is, this is called the squares. And probably many of you have seen this example so far. And I know that this will give us the least square solution. Thank you, a tomboy. Alright, so we're going to differentiate this guy with respect to Theta. So I'm going to compute DL. D Theta. And to do this, you're going to use the matrix and vector derivatives that we just derived, actually just a vector of derivatives. So if I have d theta transpose theta d theta, we know from the prior slide that this is equal to a plus a transpose theta. Alright? And then I'm going to write this in terms. If we have a w equals z transpose Beta, then dw d theta is going to equal z. I'm going to differentiate with respect to Theta. I'm going to have my one-half to turn come out. Y transpose Y has no data in it, so gradient with respect to theta is equal to zero. All right, For this next term, I have a two, y transpose X times Theta. And y transpose we know is going to be a one by big N vector. X is going to be an n by two matrix. And so overall, y transpose x, which I'm going to call z transpose, is going to be a one by two. I'm going to call this thing z transpose. So this is gonna be a two times z transpose Theta. Gradient of z transpose Theta with respect to theta is just z. So the gradient of this term with respect to Theta, it's going to just be z and z is x transpose y. Me questions there. Okay, and then the last question I had is this Beta transpose X transpose X Theta X transpose X. I'm going to call a matrix a. And I know the derivative, the gradient of theta transpose theta with respect to Theta is gonna be a plus a transpose Theta. So this is going to equal X transpose X plus X transpose X times questions, question, what is the gradient of w with respect to z? Oh, so the question is, why is this gradient to z and z transpose? Because we're using denominator over. Yeah, so here theta would be a vector that is in R2. So this should also be a vector in R2. It could be Z transpose if you're using numerical that. But in the past people used denominator, they will also still BC. So for that, just play around with and then use the definition of the gradient. Yeah, it's, it's something that we actually derived an order. It doesn't conference but because we derived theta transpose x instead of x transpose beta, but using the same exact logic you'll, you'll get because in denominator. Alright, so I'm going to continue simplifying. This is equal to minus X transpose y because the 2's cancel out. Then here I have Q X transpose X is, but then that two will cancel out with this one-half b plus X transpose X times beta. Then to get the optimal Theta, I, set this bin to be equal to zero. So now if I set this thing equal to zero, I'm going to give me the equation, X transpose Y equals X transpose X Theta. As long as X transpose X is invertible. This gives me my solution, theta equals X transpose X inverse times X transpose y. Any questions? Alright, so if you've had linear algebra and machine learning before, right? This is something that you'd like to you already know. We went through this example. So again highlight portions of machine learning problem, which is that we need to first define a model. In this case, linear model means to find an extra or loss function that tells us how good or bad. That's my Theta. And then I'm going to need to know how to differentiate L of theta with respect to Theta. So that's something that we've done here. And in this case, we can solve directly for theta. The more general case where there are many minima and maxima, you will never be able to solve for theta because you won't be able to write down a closed form analytical solution. Hover, will use gradient descent when we aren't those cases. Questions. Next question is, is it because of denominator layout that X transpose X is a matrix instead of a scaler. Note in both numerator and denominator, we have x transpose x will be a matrix. They'll just be the transpose of the chapter. Did that answer your question? Yeah. The question is, what is X transpose X a dot product with itself? It is, but actually call is an n by two matrix. So it's a two by n times n by two. I think it's a two-by-two. Oh great. Roxanne is asking here. We stepped it up in terms of rows, can be stacked them in terms of the columns. For x. Change data for the other side. Yeah. As long as everything works dimensionally and you haven't orders partner, you just stack them however you want. Question is, can I remind you what x is an n by two matrix? Yes, because remember this X hat is r square footage x and a value one. So it's a two by one vector. And I stacking up and have them. So each of these is a two dimensional row vector and I'm Stephanie. The question is, can I stay wide? X transpose X is invertible. X transpose X may not always be invertible, but you will see that x, right? In this example is X transpose X could be a two-by-two matrix, right? So it will only be non-invertible if the rank is foreigners thereof. But when we do x transpose x, we have big N examples. And as long as big N is larger than two, usually we only multiply them together. It would be high probability. There'll be likely that it'll be full. If you have many more dimensions of this too became ten and you only had two good examples like big N equals two, then it wouldn't be invertible. Other questions, actually, a transpose X, transpose X. And the reason that is is because if a, I'm just gonna write this as b, c, right? If I do a transpose Ax equal to c transpose B transpose. So if I transpose this quantity, I take this thing and transpose it and put it in front. That gives me the first X transpose. And I think take this x transpose, a transpose if I just put it to the bathroom, That's just one Other questions. Alright? So this solution is called the squares. It appears in a variety of linear application, but we're also going to see in just a few slides have non-linear polynomial models. Okay? So this is code and kept up the code from what I prefer the password was in, I believe 20 2017. So some of the syntax is a bit outdated, like Python, you can use an app symbol for doing matrix vector. Matrix multiplies as opposed to dark tones. But in this code, which you'll do something similar in your homework, why is gonna be a big n-dimensional array of rents? And x is going to be a big N-dimensional array of square footage is. And then this code is going to implement the least squares solution that we saw on that we just derived in part with the model isn't that model will be this green line. Alright? Alright. See, you might look at this thing. Can't we do better, right? We talked about before how they could make e.g. polynomial model that maybe does a better job of going through these data points. Alright, so here's a question for you all to get on, and I'll ask them to answer the question. The question is, how does our current least squares formula allow for learning non-linear polynomial models? Like, let's say I wanted to be an nth order polynomial instead of just the law. Alright? So I'm telling you that what we derived already can do polynomial. And I want something to think about how that's possible for their current machinery. I see some hands going up already on here, w1, like 20 s to think about this and then I'll call someone to answer it. Received one. All right. Someone raise your habits. How many? Polynomial regression? Yeah. Perfect. Yeah, so this student says, what I can do is I can make a new attack feature right? Before, it was just x and one. But now I can make this be, let's say I wanted a third order polynomial fit, right? I can make it one, x squared and x cubed. If I do that, then let me write it more generally here. What I can do is I can define X hat to be one, x squared all the way to XN. If I define theta to be B, then what multiplies x one is a one multiplies x squared is A2. So it'd be, be A1, A2, all the way down to AN. Then I've written this wide as a theta transpose x, where I get to choose the values of B, A1A2 am, that are the coefficients of my polynomial powers of x. Any questions here? So this is one way in which you can make a nonlinear model, which is that you choose what the nonlinear features are there ever choosing that are nominator features are squared all the way up to x n. And these are their coefficients. Alright? And using the exact same these squares formulation we derived, we derived how to minimize that loss function, the squared loss respect to theta of this form. You can now fit polynomials. So all of these data points. Oh, thank you. Yeah, the student raises that I missed my factor. Other questions. The question is just num pi used denominator or numerator. So NumPy is a collection of like a Foxconn's operations. But you can keep track of whether like how you define the gradient with num pi citation. It's up to you. And so you define the derivative of a scalar with respect the conductivity, conductivity denominator reaction. And the rest would even later. So don't fight doesn't have a preference if it's just a way for you to do these compensations. The question is, what is linear regression? So this solution here that we derived for the data is the least squares solution. But then a linear regression or linear model, or an affine model would be y equals b plus a one, x one. So the linear part refers to the model. This gear is not a linear model because it has non-linear functions of x. The question is Theta transpose x that we derived? We can generalize the higher parts of that. Yes, and that's what we're doing here. Oh, the question is, what if x were apertures? Does this still work out? The answer is yes. Oh, yeah. Error on despite here, there should not be a one here. Alright, so use this to polynomial functions of x to your data. And I have now a question on the slide. A higher degree polynomial will always get the provided data as well as the lower order polynomial. What I'm saying is that if you were to measure the error from these blue points to your model, prediction, error will always be higher. If I make my polynomial higher order or not better, it will always be worse than it will always perform as well as someone told me why this is true. Perfect gas or the student says, in the worst case, you could always set the coefficients of the higher-order polynomial term Cicero and model lower one. And that's exactly right. So if I had the model, let's say I had a third order polynomial is the plus A1, B1, A1 plus A2 x squared plus a3 x cubed. This is a third order polynomial. It can always do, as well as model B plus A1, A2, and A3 to be equal to zero. All right, so when I add more polynomial terms, As long as A2 and A3 could be set to a value, a non-zero value, that makes the error smaller and therefore it will make the loss. Any questions there? Yeah, Tom, we asked you can set a you can make the loss of the road by fitting a polynomial exactly to these data points. That's correct. If you make n large enough, eventually you're going to get a polynomial. With that, we'll go through every single data point. Alright? So it is possible to get the losses the road by making a really, really high order polynomial. Which leads us to our next question. Why don't you want to do this before? That's the question. The question is, is there a ever only one solution to the least squares problem when the loss function is convex in the parameters as it is in this case. Yes, there's one solution which is the global minimum. The question is, can I clarify the notation for this class? When would I want to write y hat and x hat? There will be a general notation for the uses of hats here. Because I wanted to say that this vector of features is related to x. I want to keep the variable x, but I just chose to make it different than x. So there won't be anything consistent with these attacks. I could have called the other questions. Okay. So if you were to look at this data and I want some to answer this without using the word overfitting. Why is it not? Gets to make the polynomial n arbitrarily high? The provided data very well, but it may not fit the underlying distribution well. That is correct. Another way to say that is there could be data points from the underlying distribution that we haven't yet seen. It's just one data point here that wasn't in the training data. So the high order polynomial fit this data really well. But if we were to predict which model actually has the lowest average for this data point over here, it would be the linear model. So in general, when we build machine learning models, right, we don't want them to only work on the data that we provided it. We wanted to generalize well to data that it hasn't seen. So this leads us to the concepts of overfitting and underfitting, which I understand it again, obviously for most of you. But to finish it up on the same page, we want models that are general, that will generalize well. This leads us to first, so first I'm going to talk about training and testing data, but then in a few slides for them to just another notion called validation data. The idea of generalization can be made more formal by introducing two datasets, the training data and the testing data. The training data is data like these blue points here. There are used to fit the parameters of your models, but theta's, the a and the b. After that, testing data is excluded into training and testing data are like these red axes that weren't used to learn the model. Through which we can compute the error of our model by measuring what the law says with respect to these red x's. Again, there's also validation data and we'll talk about that in just a few slides. But at the highest level, we define this notion called overfitting. Overfitting is when a model has very low training error, which means that it models the blue points really well. But it has high testing error, which means it does not model the red axis, right? So that models are training data very well, but it doesn't generalize if you are in that scenario than your model has. One thing which we'll see later on is that more data helps to avoid overfitting. You think about polynomial that can go through every single data point. But when you have a ton of data points, that polynomial will no longer go through every single data point. In fact, if we look at the models for the polynomial is going from n equals one to n equals five, almost close to linear, right? So even for the polynomial with n equals five, even though it has x squared, x cubed, x to the third coefficients are closer to zero and the actual fitted model is linear. So more data helps to avoid over fitting. There is also somewhat more estimation underlying distribution. Distribution. So let me repeat what I said that she wanted to clarify. Another way to say this, which is having more data helps you to learn the underlying distribution better. Model the underlying distribution is better at a training and the testing data have the same distribution, then you should also generalize better to testing. Great, yeah, so Andy's question is, when I say more data, what is that relative to? Because presumably you increase the order of the polynomial, we're going to be even more me, that's good. In general, when we have many parameters as we will have neural networks, you will need commensurately more data to avoid overfitting. Right? So in addition to over-fitting, there is something called underfitting. Underfitting is the idea that we cannot think of model really simple. So there could be e.g. data that comes through a distribution that is polynomial. So this is data that comes from a distribution that is up to third order polynomial. Here's the Python code for that. If I were to assume my model was y equals a x plus b, so that I can draw a line through it. No matter even if I find the optimal values of a and B, I won't do a good job at predicting the, from the y-values, the outputs of these blue points, because this model is overly too simplistic and can never capture some of these interesting nonlinearities in the data. So going through the same method that we derived, you can fit these polynomial models, this data, and this n equals one model won't capture the curvature and this distribution and it's in these points. And so a model is called underfitting if it has both loads of both high training error and high test error. Since on those definitions of over 100. Alright, so then that leads us to one more thing which is called cross-validation. So in our model, you'll have noticed that there's something that we had to choose beforehand, which was the order of the polynomial, right? I had to set what little n was equal to, say n equals four. And then after I chose little n equals four, I did my gradient of the loss with respect to theta to calculate the F1, A1, A2, A3, A4, right? So the B, A1, A2, A3, A4 are my parameters. But this thing that I chose beforehand, little n, is called a hyperparameter, is something that I choose before I do the optimization. And this hyperparameter is quite important because it defines the model. How do we choose our hyperparameters for? The question? Yeah, great. So the question is, how do you decide that? What is meant by the training error as high as the validation testing error is high or low. So we'll talk about this more when we get to gradient descent, but it's usually a relative thing. So when we look at our loss function, and this is for training generally as a function of theta, but as a function of the amount of time that we trained for, the loss function will decrease and the slope decreases will be in front and we'll talk about that. But then for testing or validation, let me write validation because in general we won't be in this situation, although actually not having introduced validation here, let me call this test. The test error will also decrease, but then there'll be a point where it begins to increase again. So it's a relative comparison. In this regime here we would say that we are overfitting now because we haven't gotten your training error but higher, higher. Alright? So again, we were saying that a model will have hyperparameters. Why? The order of the polynomial, okay, I have to choose the forefront to the optimization. And we may be wondering how do I choose the optimal polynomial order. To do that, we also introduced this notion of validation data. And validation data is data that is used to optimize the hyperparameters of your model. So it allows you to choose what is the best little. Alright? How do we do that? What we're going to do is we're going to divide our training set, or sorry, we're going to divide our data into three sets. The first is the testing data. The testing data you should think of as with pelvic samples, Christina and set aside. And you basically get to look at this testing data once after you've set all of your hyperparameters and then the parameters and the testing data is just satisfied. You get to query it wants to ultimately scored or model. Okay, so that's what the test data is. Partitioned away from your original data set aside and use wants to just get a score on your model at the very end. Okay? Any questions there? Okay, So that's testing data, training data and validation comes from the remaining data that you have leftover. And what you will do then is you'll withhold some data and validation data someday. That's training data. Training data used to train your model. And then validation data is used to score your models generalization on data. This might be confusing, so we're going to write this out. So there's a process called k-fold cross-validation. This k-fold cross-validation is going to assume that you already have a separate testing set. And what we're going to do is we're going to take the remaining data that we have, separate it into a training and validation data set. Let's say that for training dataset contains and examples. Let's say that is 800 examples. Okay? So we have big N equals 800. Let's say we're going to do four fold cross-validation, which we often call CBD. Okay? Then what we're gonna do is report to split the data into k equals four equals sets, each with 200 examples. So we're going to have four folds, each worth 200 examples. Of this force will be used as training data. And then one of those poles is going to be used score model, which is called validation data. Alright? And then you can swap between which are the training data, which are the validation data. And what that looks like is the following. So I've drawn two examples here. The first example is very rare, so I'm always showing this to you as something that you can do. So in the case where your model has no hyperparameters, you can split your data, your original data into train and folds in green, testing fold in red. And because you have no hyperparameters, you can just train on these four folds of the data and then test on the fifth fold. And testing on this pitfall will give you a score of how good your model is. Then after that, you can make fold one, the red testing data and make folds two to five degree trended data. And you can do this five times together. An estimate of what your average test error. This is very rare. You will never see it in the class. So I've removed it off this slide. And this is what k-fold cross validation will look like. So let's say that our original data has 1,000 samples. Alright? What I will first do is I'll make it successful. This test fold will have 200. Examples in the remaining training data will have 800 examples. Now this tough fold is put aside. I'm never going to touch it until I'm done with all of my machine learning optimization. And then I get to query the test fold, wants to score my model. My remaining 800 examples. I want to make a bottle as good as possible, which means I didn't choose the best value of this hyperparameter little pen. And then you'd also do my optimization of the coefficients, a one, A2, A3, et cetera. So what I do is with my 800 examples, I'm gonna do, in this case, four fold cross-validation, which means I'm going to split this into four folds, each with 200 examples. Yeah, tomboy saying, do you stumble the data before you assign the folds disease, you could idealize shuffle your data before you assigned to e.g. if you just kept in order and this was like SeekBar time unfolds. One might contain a lot of cars and horses, but the cats and dogs mail will be important when you want somebody to sample all the different classes in each of these faults. So what we do with these oracles is this is used to learn theta. Then this is used to assess how well does it generalize to unseen data. So this is our validation fold. Right? So I would set M equals one. Then learn AND from my green training data. And then I would assess its performance on the withheld data. And my validation. Set n equals to training again, assess performance on the validation fold. I was at n equals three and then assess performance again on the validation. Right? And so you can repeat this procedure first, then choose what the best setting up little n is. That would be the model that, and to achieve the lowest validation error, let's say it was the n equals three bottle, little n equals three. So the third order polynomial, I got the best validation error, right? Little n equals three. Train a model on all of my training data. And then at the end I can query it on the test together by one measure of what the losses on this test. All right, Let's talk with them and then let's generalize it to do it. But then I didn't make us. Great. Yeah. Alright. Yes, That's perfect. Yeah, So tomboy is mentioning something that I think back then, which is choosing the best value of n when we do four fold cross-validation. But we're also going to do is we're going to swap which ones are the validation in which went to the training set. So in my second go round, I'll make fold one. The validation and then falls to four will be training. And then I could also do one where fold 13.4 are training but for two or validation. So by swapping out each of these, I will calculate it for validation errors for every single hyperparameter, and that'll give you a better estimate of what the actual question. Yeah, so the question is for doing optimization of the hyperparameters on the validation datasets. That's correct. And then after you have the optimal hyperparameters, you could set them and then just train across all of your green data. Could just get the best out of all of your training data to build your ultimate model. The question is, how's a number of both optimize? A very common thing is to do five-fold cross-validation, where you've got 80% shining in 20% test. There's also something called beat one out cross validation, where your validation is one example. And if the rest of the training data, and they'll depend on your datasets. I said, How do you choose that? There's a book. That's how much does it's a whole field called next, which is devoted to this hyperparameter optimization. All right, so I want to say one more thing before we end. Which is, students will often ask, why is this testable left out? And what do I have a different validation dataset so that your model is not Crawford demise or fine-tuned to do well on the test. It could be that if you were to optimize the hyper parameter middle n by repeatedly looking at your passport, you might get a value of n equals three. But then you would have chosen the hyperparameter based off of predicting this particular dataset. And that might not be the model that generalizes the best out there interviewing. And so the test fold again, just think that you should clearly once so that you don't learn the particularities of it. They will come back. Yes. Recording."
"Okay, I'm gonna clear these drawings. Yeah, sure. Thank you. That was all I had. Okay, great."
"Okay, let me make Tom White co-host really quickly. And, circle on. I wanted to add that we will be posting the discussion problems for this week along with the recorded videos on the solution so that it helps the student with solving homework six. Great, thanks, Tomwe. So if I'm clear, based off of, in response to Daniel's question, there will be a discussion video this week, as well as posted discussion questions and solutions for students."
"Okay, so we could calculate the probability of a plan to the right, the probability of a plan to the left, and the probability of a plan upwards and get numbers. And we could choose that the monkey reached to the, the monkey plan to reach the target with the highest probability. Right, and then last lecture, we had done the math to show how we can calculate P of CK given XJ from knowing this training distribution. Any questions there."
"Okay, thank you. Thank you very much, Brianna. Can the brain machine interface also affect like pain transmitted, like with fibromyalgia, there is pain that occurs, could that prevent? In theory, yes. So I'm not aware of any studies related to looking at pain pathways. And for example, preserving them, or writing an information to those pathways to prevent the perception of pain. But in theory, that's something that I would call a futuristic brain machine interface. In general, it's very hard to write information into the brain, or to exactly perturb the brain in the way you desire, not just because of the lack of tools, but also because I would say that we understand less than 0.001% of how the brain works. And I'm a computational neuroscientist, that's my field. We understand very little, but we still have made great strides. But then as to how to particularly perturb brain pathways to solve the perception of pain, I'm not aware, I don't know that area too well."
"Okay, thank you. Yeah, let me stop share. And then I saw David, yeah. I was hoping, and I don't need to bother other people, I was out of town, so I didn't see the tour of the lab, which I don't need to bother others. I can come back at another time when everybody's not asking questions. Yeah. Does anyone have any quick questions that we could go over right now?"
"Okay, yeah, I just wanted to clear up on that convention. Thanks a lot. Yeah, yeah, of course. Professor, can I ask about the previous figure, the figure on the page 34? Yes. For the, so the C is a steady state as compared to the change at the figure B, right? Sorry, I was scrolling. I missed the first part of your question. Can you repeat that? And the C is T equal one second, you mean that is a steady state. After the be right. Um, so, in this, in this drawing it is a steady state."
"Okay. All right, what is your question. So I'm a little confused about how to approach one, a, I think I watched Shoshana's office hours and he was explaining how it could be turned into like a binomial distribution. That's right. I'm, I'm not sure how to go about it. Like how to reach the, where the lambda s comes in, and why that's like separate from 1 minus p. Got it. So, in this question, let me just pull up a, let me pull up a blank window so I can Right on there."
"Okay. Any questions here? All right. So we talked about how these action potentials are, all are nothing events, and they look like these spikes over here. And in this sense, we can think of signaling in the nervous system as being digital, right? Either you have no spikes, or else you do have a spike, which you can think of as a one. And this digital signaling allows for high fidelity communication across long distances in your body. And so your brain has to tell your toes how to move, or else you have to be able to feel things like if you step on something pointy, you feel that pain. And that pain should be conveyed to your brain at high fidelity. And that happens because the neurons are going to fire these spike signals, and the spike signals are the digital signal. And as they get transmitted across the nerves in your body, they're going to decay like all signals do. They're going to decay as they travel. However, there's going to be these repeaters in your nervous system called nodes of Rambiae that when the signal is decaying too much, it's going to amplify the signal so that it regenerates the spike. All right. So if you've heard of a motor disorder called multiple gulerosis MS, actually the degeneracy of this disease is that the spikes can't get to the next repeater. And if the spikes die before the next repeater, before the next node of Rambiae, then the signal is lost. And so people who have multiple gulerosis have the degeneracy and not being able to regenerate or re-amplify these spike signals. Any questions? All right. So, I'm on this. Yeah. Okay. Thomas, I think you were having a sorry. Yeah, I think you're on me to be there. Yeah."
"Okay. For the second bullet point. Yeah. Thomas, I heard for the second bullet point. I heard for the second bullet point and then I got cut off. Okay. Is it working out? It's working out, I guess. Okay. Yeah. For the second bullet point, what is like highly stare type to mean in this context? Oh, it means the shape is highly stare type. And so the shape is going to look consistent across different neurons. Now, it's not 100% true that all neurons have the exact same shapes in terms of waveform shapes, but they look fairly similar. They will differ. And for example, they're with or actually with is a major one. But that's usually reserved for more advanced neuroscience classes. And so this stereotype refers to this shape. All right. Any other questions? All right. So, we mentioned then that since neurons communicate via these spikes and these spikes are spikes in voltage. What we can do is we can drop an electrode and we can try to record the voltages that these neurons emit. And we mentioned that when you drop an electrode into the brain, there's no guarantee that you get very close to the neuron. So usually what happens is that I have an electrode over here. This is going to be my positive terminal. And I'm going to have some ground electrode that's far away. And so really what I'm measuring, if we assume that the ground electrode stays at some static level, what I'll be measuring is the changes in voltage in this vocal proximity. And even though I'm not right on top of the neuron, if I'm close enough, I'm going to get some detection of a change of voltage whenever there's a spike. But instead of it being 100 millivolts, it's going to be, let's say, 100 microvolts. So, one thousand times less than amplitude."
"Okay. Hi, Amina. Hello, how are you? Sorry? I said, hello, how are you? Oh, I'm good. How are you doing? Thank you. I just had a couple questions on the homework. Great."
"Okay. However, we do have, this is for 2G, right? Yeah. And 2G. So I did show you row first and then I went to the joint probability over D. Yeah. And then I took out all the probability of C times probability of A given C from the ratio over D. But so then I'm left with, yeah, with what you wrote down in the first line. You're left with a, you're left with this. A is probability of B, yeah. Yeah. The probability of B given A comma D. Yeah. Yeah. So sorry, let me just look at the solution because that's the first step that you do also, but then they incorporate a probability of B given A. So let me just see how we get that."
"Okay. I just wanted to ask if you could please repeat that last portion of what you just said. Like we're calculating the probability of the cursor and like finding the mean and stuff. Exactly. So what we're doing here is for every time step k, so at a certain point in time, the time that I'm decoding right now, I'm going to calculate a distribution of what I think my velocities will be given all of my neural data. That's what this pink distribution is here. And so if my pink distribution looked like this, then I would guess that my velocity should be somewhere in the vicinity of 10 centimeters per second. And then I need to actually choose just one velocity to actually move the cursor on the string. And so the velocity that I'm going to choose is going to just be the mean of this distribution. So the decoded velocity will be the distribution mean. So I need to first calculate the distribution. And then after that, I need to take its mean and that gives me my decoded velocity. Not an other question. Yeah, coming. Yeah, so over here, they're minimizing the mean squared error and that gives the mean, right? But let's say you minimize the absolute error instead of the mean squared, then you will get the median of the distribution as the optimizer. So is there any intuitive reason behind this government filter using this mean squared error versus some other error? Yeah, it is straightforward for us to calculate the mean of, actually, it's also straightforward for us to calculate the median of the gas distribution. So actually, in this case, it would be fine to also use absolute error. And you would just need to decode using instead of the mean of the distribution, the median of the distribution."
"Okay. I just wanted to ask if you could please repeat that last portion of what you just said. Like we're calculating the probability of the cursor and like finding the mean and stuff. Exactly. So what we're doing here is for every time step k, so at a certain point in time, the time that I'm decoding right now, I'm going to calculate a distribution of what I think my velocities will be given all of my neural data. That's what this pink distribution is here. And so if my pink distribution looked like this, then I would guess that my velocity should be somewhere in the vicinity of 10 centimeters per second. And then I need to actually choose just one velocity to actually move the cursor on the string. And so the velocity that I'm going to choose is going to just be the mean of this distribution. So the decoded velocity will be the distribution mean. So I need to first calculate the distribution. And then after that, I need to take its mean and that gives me my decoded velocity. Not an other question. Yeah, coming. Yeah, so over here, they're minimizing the mean squared error and that gives the mean, right? But let's say you minimize the absolute error instead of the mean squared, then you will get the median of the distribution as the optimizer. So is there any intuitive reason behind this government filter using this mean squared error versus some other error? Yeah, it is straightforward for us to calculate the mean of, actually, it's also straightforward for us to calculate the median of the gas distribution. So actually, in this case, it would be fine to also use absolute error. And you would just need to decode using instead of the mean of the distribution, the median of the distribution."
"Okay. Right. Everyone happy to start office hours, as always, if you want to speak about something that you don't want recorded just let me know and I'm happy to turn off the recording. Okay, Professor. Oh, so I actually had confusion about the, the one that you make the discrete in the class but I think my confusion mainly come from when you said like, it's not because of the law, total probability that we expand the party into the sun. Actually I wrote something. Can I share the screen. Yes, let me, let me make sure it's enabled. You should be able to share now. Okay yeah. Oh sorry, wait a second I think there is a problem on my side. Oh did you guys, can you guys see my screen? I cannot see it. Okay. You may have to hit the share button after you select the window. I made that mistake. Okay. Like, this is what you wrote on the lecture note right."
"Okay. So lastly left off, we were on property two of the Poisson process. And this was the so-called restart property. And so the property formally said that N of T plus S minus N of S, right, which is the number of spikes that occur in a window between time S and time T plus S over here. The number of spikes in this window is a Poisson process or this end of T plus S minus N of S follows a Poisson process. The number of spikes in this window is going to be a Poisson random variable with lambda times the length of this window, which is little T, right. And it's going to be independent of N of R where R is before S. So the number of spikes that happen in this window and the Poisson process in this window is going to be entirely independent of what has happened all along in the past. We say independence, right. We should be thinking something is independent. For two things are independent."
"Okay. Thank you. Any other questions on exam policies? All right, great. So we also recognize that students may be in different time zones that make taking the exam during class time or during the scheduled final exam time difficult. So if you're in that situation where you need to take the exam during a different time, please email me by the end of this week so that we can prepare for this. I just saw in chat someone asked about a map lab. So there's not going to be any map lab syntax questions on the exam. You won't be asked to write card. All right, so grading in this class is done on an absolute scale. And so this is the scale for how we're going to assign grades. And so throughout the quarter, you'll be able to calculate your grade exactly given the breakdown. We had in the prior slide and this scale here. Now, when I was an undergrad, I knew when I went into a class and there's an absolute scale, I was a bit scared because what if the professor writes a very difficult exam and all the grades are really low. And so we work, we try to design the exam so that that isn't the case. But if we do end up giving an exam where the average grade is very low, causing the students causing many students to be in a lower grade range, then we reserve the right to relax the scale so that the scale would be the scale would be an easier scale would be relaxed, but it will not be made more stringent. All right, I'm seeing a bunch of questions on the exam. So I'll really quickly look at the chat Angela asked the exam written or typed out. So we are going to give you questions, which if you want to type them on my logic, you can, but typically most students just write their work and their answers on a sheet of paper and then also that PDF to grade scope. We will not be using respond this lockdown and we will not monitor you during the exams. And so we will distribute the exam for the in class exam, it'll be a one hour or 50 minute exam. And the TAs and I will also be in a zoom room in case you need to come and ask questions, but we won't ask you to turn on your cameras and we won't monitor you."
"Okay. That makes sense. Thank you. All right. The TAs chime in if you know, if you know more than I do here. All right. Okay. So that's the square wave. Well, what I want to now ask is a question, which is you'll notice that even when we had 100 complex exponential frequencies and my sign waves added together approximate the square wave pretty well, you'll notice that at these discontinuities, they're not perfect. And if you zoom in a bit, you can see that there's a bit of ring. It might be clearer when we have 10 frequencies, which is that at these discontinuities, it seems to overshoot and then ring a bit. And that ringing, if you were to zoom in on this plot, is still visible."
"Okay. Well, I mean, five A and five C. Five A, do we have to draw an RC model? Sorry, this is for question five A you said? Yes. For question five A, when you say draw an RC model, you mean do you have to draw a resistor and a capacitor circuit? I quite literally do have to draw one because I use like proportionality to reason through it. Yeah, you don't have to draw it. All you have to do is calculate rA times cm. And so I, since I will set the time constant, so you don't have to do any drawing of a model here. Okay, great. And then for 5C, is the y at the end, just a repeat of the first question asking what is the purpose of these nodes?"
"Okay. Yeah, so let's say that we want to calculate the covariance for xi and xj. So this would be the expected value of xi, xj minus the expected value of xi, expected value of xj. And then Edwin, sorry, was the other thing that you said with the graphical model? I just wrote down the equation. Yeah, so what were the equations that you had? The probability of x1 times the probability of x2 given x1 times the probability of x3 given x2 times the probability of x4 given x3. Great. So this is the joint density. All right. And then again, Edwin or anyone else, given this information, how does anyone want to take a step at how they might have calculated these quantities? Okay."
"Once I know the mu, which is the mean of the distribution, and the sigma, which is the covariance of the distribution, then I can write down the distribution exactly. And I can calculate the probability density function for any point x. All right. And then we went into some detail about what this covariance matrix means, this diagonal terms and this off-diagonal terms. And that was last picture. Any questions here? All right. So if I model p of x given ck via this multivariate normal distribution, then what the problem is going to boil down to is this multivariate normal distribution is totally specified by two numbers or two, one vector and one matrix, mu the mean and sigma the covariance matrix. And so for every single distribution in the training phase where I learn it, I'm going to want to set the value of mu and its covariance matrix, which remember we conceptualize as these ellipsoids."
"Or are people fine going with the lab partner. I also realized, actually the monkeys only shown in the beginning of the lab tour so I'm going to record the rest of the lab, I'm going to pause during the monkey part, and I'm going to record the rest of the lab tour so that anyone else who who is curious can see it then. So let me do that again. And then we'll go on to other questions. So, let me just pull up the slides. I almost have it, I think, this keynote presentation. Great, okay, I have it. All right, so I'm just going to pause the recording for the first part of the slides. And then I will resume it after the monkey part is over. Great. So this is the lab tour."
"Or do I have to use that for the chain rules? The thing is, you can't, sorry, you are generally correct that some over A of P of A given C will go to 1. Yeah. But in this case, it doesn't simplify nicely because P of A given C is still multiplying this expression here. So we will have a question. Yeah. I didn't know if I could separate that up or not. Yeah. So sorry. I'm writing it out, hopefully. So we should have a P of A given C, a P of D, and then a P of B given A and D. Okay. Yeah."
"Overall K plus is still leaving the cell. And overall, and a plus is entering the cell. So, if the pump wasn't there, then it would reach. Then the diffusion currents in the blue. This would be getting smaller and smaller. Okay. So, So if you were to pump like for any plus ions out and three k plus ions in, then it would reach in a slightly different steady state, and the VM would be different because essentially this thing is saying what the steady state concentration of K plus my sense. Only the, the, the sodium potassium pump is pushing na plus that. And so if you didn't have that you would have a ton of influx of na plus in, and this overall system would reach some different equilibrium would be probably through leak channels, right? Because the sodium potassium voltage channels seem to be a bit more finicky."
"Perfect. Yeah. Yeah. I have another question. All the other people ask and all circle back around. Sure. We have a good number of people here. So maybe we could do the race hand system. So if you have a question, you'll be the razor hand and then we'll just take them in order. So we can do that. Okay. So my question was also on 2F. I was wondering when we're simplifying probabilities. I got like the sum over D of P given A, D times P of B. And I was wondering."
"possible. The expected, you're calculating the expected number of spikes, given that the ISI is less than 1 over lambda. Yes. I believe that is, I believe that's an interesting way to go and I think that, are you able to calculate that expectation? So I didn't go through with it because I didn't even know if I was set up correctly, but that was like my English sense of it. Yeah, I think that at least getting that conditional distribution, that conditional distribution challenging, because you would have a P of NFS, given, T is less than something. And however you split it up, you would, you would have to compute either a probability of an NFS given T or probability of a T given NFS with very conditional probability."
"Questions from Edwin. Hello, I'm just curious whether we will still cover dimensionality reduction. Yes, I believe we will be able to, I think given the pace of the class, we should finish common filter sometime next week and then after that our last topic is the mentionality reduction. Thank you. Any other questions? All right. So we are continuing on continuous decoding for BMI's. And today we're going to talk about the common filter. And so our motivation at the end of last lecture for the common filter was the following. We could decode with the optimal linear estimator as well as the weiner filter and we saw the weiner filter did quite well. But neither of these decoders took into account anything special about the movements we make. Both the optimal linear estimator and the weiner filter were essentially these squares regressions of neural data to kinematics."
"Right, so in a homogeneous Poisson process we assume that the next spikes should happen with a high firing rate, i.e. they should happen in less time. Whereas if I knew that the first spike happened after a long amount of time in this firing rate curve, then we would expect T2 to be during a time when there's a smaller firing rate. And so we would expect the time to the next spike to be bigger. And so, we already have this intuition that the size should be dependent. But to calculate it, we needed to to show a rigorously we need to be able to calculate this probability. And so remember our approach was to calculate the distribution of the second ISI, it's time being greater than some time, s, given that the first ISI happened at time t or happened after a length of t. And if this probability depends on little t, then this distribution of t2 being greater than some time depends on the value that t1 took on. And therefore, they can't be independent because t2 depends on the particular value of t1. Right, so if t1 and t2 are independent, then this won't depend on T, but if it depends on T, then T2 and T1 are not independent."
"Right, so. So, for this question, it sounds like from what you said that Shashank had discussed how probability of M equals M given N equals N is binomial, right? Mm-hmm. Great. And at this point, we know what this probability is. That's the binomial distribution. Are there any other probabilities that we know here that we could use to calculate probability I'm not seeing it. Oh, that it's Poisson? Yeah, we know the probability of the n process. So we know that probability of n equals little n is lambda. Exactly, that's right. So this is Poisson lambda. And so that's where the lambda s is going to come in, in your derivation. So now the key is, if these are the two probabilities that you know, this binomial one and this Poisson one, how do we use it to calculate probability of big M equals little m?"
"Right, when we plot spectrum frequency is on the x axis and then on the y axis, you see how much of that frequency is used in the signal in this class. We will use the notation J omega. However, the J there is primarily symbolic. It's to tell us that this omega is a frequency. And we'll see why we use this notation more and get to a plus transform. But remember, ultimately our original signal F of T whoops. Original signal F of T is a function of time. And it tells us how our signal evolves over time. Our Fourier transform F of J omega is a function of frequency. And it tells us the frequency, how much of it tells us how much our signals composed of cosines and signs of frequencies over entire spectrum and entire span of frequencies. And we also derive last lecture, the inverse Fourier transform, which takes us from our F of J omega back to the time domain F of T. Right. And so last lecture, we did a few Fourier series of Fourier transform examples. We derived the Fourier transform, for example, of this signal each the negative ATU of T by just plugging it into this equation and doing the integral. And so this is an example how we actually compute that Fourier transform integral."
"Right? Any questions here? All right. And I saw a question that Chad, a great stature, because of decreased resistance, and Tom William said, reduce capacitance, also reduce resistance, or if you were able to reduce the axon resistance, you would also increase the action potential speed since it's inversely proportional to both of them. But for the particular mile-innation, it's because of the reduced capacitance, like Tom always said. All right. Any questions from any of this recap? Question from Shishank. Prof. Shishank, the technique that you have marked there, isn't it only supposed to be the technique of the mile-in sheet, and are in the charges supposed to be positive on both ends and negative on the interior of the cell wall?"
Right? Let me just go ahead and make my t is copus one second. So tonwards of cosine. Alright so we want to show whether a and radio is bivostable or not. Right? And so if a and radio is bivostable what that means is that if my x of t is bounded then y of t is going to be bounded. Alright? So to show it this is stable I'm going to start with the assumption that x of t is bounded. So I'm going to assume that x of t is bounded. So it's less than some constant and x. Alright? Now if I take this assumption and through math I can show that a y of t is also bounded by some constant. Then I've shown that a and radio is a bivostable system. Right? So let's go ahead and see if we can show this to be true. Right? So if x of t is less than mx then we'll follow mathematical argument. If I were to take the absolute value of both sides I would get that the absolute value of y of t is equal to the absolute value of x of t times cosine omega c times t. Alright? And this we know is equal to the absolute value of x of t times cosine of omega c t. It's absolute values. I can still have the absolute value. Right? Now remember for bivostability I want to show that the absolute value of y t is less than some constant. Alright? So can someone tell me what a next step would be if I want to show that y of t is less than some constant?
"Right? So therefore, A and B are not independent. Just like the last example, one of the conditions where we explored this a bit more, one of the conditions for this equal P of B is that P of C given A is equal to P of C. Right? In which case, this would equal P of B comma C and then we sum over C given P of B. But in general, that is not true. And C is going to depend on the value of P of C. Any questions here? All right. So then let's do the conditional independence one. So I'm going to copy this graph over also. I just saw in chat. Can I explain why this is not equal to P of B? Yeah."
"Right. And for each of these different reaches, the neural data, the neurons are going to fire in different ways. And so what we do is we perform a training set where we know the answers. We ask the monkey to make a plan to a reach. We record neural data. And we know the answer of what target they were planning to reach to. We know if it's a red X, a blue circle, or a green triangle. And then the goal of the training phase in machine learning is that then we want to take this data and learn a decision rule for how to classify new unseen data where we don't know the answer."
"Right. And so I'm going to tell you an answer, which is that even if I were to make this an infinite number of complex exponentials, we would still have this overshoot. It would not perfectly go away. And so this is odd because we did approve where we said that as long as CFK is equal to this expression, that F of t is equal to the sum of the ck times this complex exponentials. And in math equals has a very particular meaning. It means that this is going to the left and the right hand side are going to equal to the same value at every single point in time t. And yet we see that at particular points in time t, in particular these discontinuities, F of t and my square wave and the right hand side, my Fourier series expansion, they are not equal to each other. So this is a hard question, but I want you to think for maybe 20 to 30 seconds about why this might be the case. Why is it that the Fourier series expansion doesn't equal F of t at every time point t, even if we prove that to be the case?"
"Right. And so if big T equals one. Then my signal f of t is just the one signal. And we said that this makes sense because if we look at how we define this rect, the rect is a 1 between negative 0.5 to 0.5. And then it stays zero until negative t over 2 on the left hand side and then positive t over 2 on the right side t over two is plus 0.5. And so actually when t equals one, the signal is just this blue line here that stays at one. And so if I made the periodic extension of the signal, it would just remain at one. And so that's why the fourier series coefficient indeed comes out to one and, and that makes sense. asked to go back to this slide. Yeah, so I want to pause and see if there are any questions here again. All right. So now what we're going to do is we're going to set, we did this for big T equals one."
"Right. And so that's our XK and our YK. And then we talked about then. We talked about the, what we want to do is to derive a decoder, the decoder, what we want to do is we want to take new neural data YK and multiply that by a matrix L to get my kinematics, my velocities XK. Right. So I need to learn this matrix L. And we saw that if we set up this question. With this decoder, we got a linear equation X equals L Y. We're big X and big Y. Big X are my neural data's contaminated across all time. And big Y is my neural data can cadmated across all time. Which would be your neural data can cadmated across all time. And then X pin would be your neural data can cadmated across all time. So your X pin again, remember has a 2D velocities. And so we would have a VX and a VY. And you have this for the first time bin."
"Right. And that makes sense because if your neuron is firing less spikes per second, we would expect that the time in between spikes should extend. Any questions here or on any intuition? Alright, so the other quantity that would be of interest, that was the mean, would be the variance. And you should recall from your probability class that variance of a random variable is defined as follows. It's going to be the expected value of big T squared, which is called the second moment, minus the expected value of t minus expected value of t squared. And then this quantity here, if you wanted to compute it, it would be the integral from minus infinity to infinity of t squared f t of t dt."
"Right. And then the next slide, Cisco through the derivation of those four years series coefficients, which we've done before. then started to just try to get some intuition over what happens if big T goes to infinity. All right, so we started off by just saying, well what if big T was equal to one? So if big T equals one, then the Fourier series Ck is equal to just sinc of k. All right, and sinc of k looks like this function. All right, and so now let's calculate my Fourier series coefficients. Let's calculate Ck, so let's calculate C0, C1, C2, C3, etc. All right, so for C0, that would be k equals 0. So for k equals 0, this corresponds to the complex exponential e to the j times 0 times omega naught times t, which is just equal to 1. All right, and so at k equals 0, then c0, the first coefficient, is going to be equal to sinc of 0. And so sinc of 0 is just this value of the sinc function at t equals 0, or sorry, at omega equals 0. And so that's c0 equals 1. Right."
"Right. And we said that we would define for this Poisson process, we would define these inter spike intervals to be exponentially distributed with a rate lambda, and that all of these TIs were independently and identically distributed. Right. to define the exponential distribution. And so we set a random variable big T with rate lambda bigger than zero. And this rate lambda has units of spikes per second. This big T is exponentially distributed. If it's density function looks like the following. When T is less than zero, it's zero. And when T is greater than zero, it's this lambda E to the minus lambda T, right?"
"Right. I want to pause here and ask if there are any questions recapping this. All right. So then when we left off, we were now going to do maybe the most involved part, which is to differentiate with respect to the covariance matrix sigma. Right. And we mentioned as so these are just copy and pasted the log likelihood from prior slides. And then we mentioned that we are going to use these properties about the derivative of scalars with respect to matrices, as well as the cyclic property of the traces. And we can use these without proof. So we'll just assume that they're known for this class. Right. So we did any questions before we head on to do this. Question from Amina. I wanted to ask about the trace. Like I understand what it actually is, which is just the sum of the diagonal, but I can't. I don't really see why it's fundamentally important to know that. When you say fundamentally important to know the trace, do you mean like why do we need to use it as an operator in our derivation? Yeah. Yeah. So the nice thing about the trace is that it makes this product of matrices into a scalar. And in this class, we're never going to differentiate more than a scalar with respect to a matrix. It is true that you could, for example, differentiate a vector with respect to a matrix. And that becomes a 3d tensor or the derivative of a matrix with respect to a matrix that becomes a 4d tensor. You could try to apply those rules to solve these derivatives. But for us to make our life simple, we end up using this trace property. And we find that this will allow us to just work with matrices and vectors and simplify the algebra involved. Any other questions here? All right. So we're going to take the derivative of the log by the hood with respect to sigma. And so this first term, TI log pi has no sigma in it. So we can ignore it. And then we were differentiating then this term, TI log of the multivariate normal distribution. And so my derivative operators linear so it can go inside the sum. So we'll have a sum. And then I bring out the TI because it doesn't depend on sigma, my covariance matrix, big sigma."
"Right. If Omega not is big, which means that capital T is small, the time in between my sampling impulses is small, then the replicas are going to be far away from the center remember the replicas occur every omega not right. However, if omega not a small shown here in purple, meaning that the time in between my samples is relatively large, then the replicas are going to be close to the original signal, whereas in the case of when Omega not was big and we had a replica far away. I could recover my original signal which is just this triangle over here by applying a low pass filter in this range. Okay. And so we're going to be the maximum frequency of the signal. And so, the frequency at which the signal goes to zero and stays at zero forever. We're going to call that be. That's going to be a bandwidth and frequency but in the omega domain, it would be two pi times. times B. Grace. I wanted to clarify some notation."
"Right. So for my compress signal with a greater than one. If we compress a signal in time, what we see is the Fourier transform the spectrum gets expanded because a is greater than one. So this omega is going to multiply is going to be divided by a greater than one. So it's going to expand the spectrum. Where is if I expand the signal in time, this blue signal, my Fourier transform is going to be compressed. Right. Can someone remind me the intuition of why this makes sense. Why is this an expected result? Can someone raise their hand and respond? Kai. Because for compressed signal, your all the frequencies are going up. So you're going to have more higher frequency content. And in the frequency domain, that's further away from the origin. Yep."
"Right. So I want to. Sorry. I'm not here for this. Sorry, was that Caleb? I'm not sure if you have like a procedure for who goes first or something. Usually we do a hand raised procedure, but there are. There are so few people here that you could just scale for it. Yeah, thank you. I was really stuck on the Poisson distribution part of the homework. And I've been looking at some of the discussion notes regarding solving the optimization parameters. And I guess I just had a few questions about how our variables are being formatted. For instance, in Shoshank's discussion. And I will for thought our homework, we have these why I terms, which are elements of a vector why. And then in discussion, it seems like each why I term is also a vector from a size D."
"Right. So is this basically for all aspects of bioinformatics and biology where we always take VN as the inside of our cell, like by convention? I don't know about fields beyond neuroscience and how they, what conventions they define. I would guess that they use the same conventions as in neuroscience, which is the field that uses the most electrical engineering in terms of cells."
"Right. So let's just take one of them as an example. So let's say that C2 is equal to zero. Right. And it's going to be true for every single even harmonic. Every single C sub even number. Let's go ahead and see how it can be the case that C2 can equal zero. And then we're going to see a general property for which Eric's explanation is to more general explanation. But this would be how I start off the question as a I'm not sure. Or I'm thinking I'm not sure what the property is like let's say that I didn't know this property. How would I arrive at it? Right. So C2 is equal to zero. We also know that C2 is equal to. The integral from over a period. So from zero to big T. So we have a C2 of our signal, which we call X of T. And then each of the J. And then we have a K omega not little T. And here since we have C2, then K is equal to two. So we have a J times two times omega not times little T. And then we have a C2 of T equal to zero. So if this is equal to zero, then it means that the integral over a period is going to cancel out to zero."
"Right. So then, last. Oh, and here's an example which we'll talk about later on, which we'll talk about later on, but just shows that if you only have samples like the red points for the samples shown in the red points, we can show here that it could correspond to cosines at different frequencies and so here's an example where, without any other information these red samples here are ambiguous because it could be this cosine at 0.75 hertz, or they could be this, they can correspond to the samples of this cosine here at 1.25 hertz. So we'll talk about that later today this is an effect called aliasing. All right, so last time we took a very roundabout path to essentially build up the intuition for how to think about sampling. And we introduced the signal called the impulse train or the delta train, where this big T tells us that we have a bunch of deltas that are separated in time by a big T. So these are a bunch of deltas that are all big T's spaced apart."
"Right. So with that we're going to finish Poisson processes during the first part of today's lecture and then after that we're going to start to get into decoding neural data and so we'll get into discrete classification today and motivate that and talk about the particular decoding task we're going to be doing. So these are recap slides from last lecture. We're at the end of Poisson processes and last lecture we talked about inhomogeneous Poisson processes where the key difference is now that firing rate of the Poisson process lambda r can change over time, right. And we define the Poisson process, the inhomogeneous Poisson process with the following properties that there are zero spikes at time zero, that the number of spikes that happen in a window between time S and time T plus S is given by the integral of the rate function under the curve, and that becomes the mean of my Poisson distribution."
"So a few things before we begin. First is a reminder, homework number three is due tonight. And please be sure to upload the code with the assignment as well. We've been receiving a few e-mails about late. We've been receiving a few e-mails about ME days and I just want to remind you, oh, you're welcome to use your late days but got to the foreign cars and you have three free rate. Alright, so there's no penalty for guessing. Here. We're going to upload homework number four today. And so that's an assignment that the TAs asked me to tell you all to start early. Because in the assignments, we will also ask you to do some hyperparameter optimization to achieve at least 60% accuracy, punk boy, yeah, 16% accuracy. So that optimization will pick something. So please be sure to start early on homework number four. In homework number four is going to be due Friday. Not this Friday, but the Friday after that. He taught boy, that's 17. Regarding the midterm exam, all of the past exams that we have ever given for this course, our ability to grow and learn already under modules. And for this year, the exam is back in person in prior years, but the pandemic, it was remote. So for the in-person exam, the exam is closed book and closed notes, but we will allow you to bring forward. She cheats shipping one standard 8.5 by 11 inch paper. You can write on both sides. So you'd have to pay tools sides, and you are allowed to put whatever you wanted to see shapes. So I have friends who would, you know, they would write cheat sheets and they would also like to put my lecture slides on there. And then after they made a cheat sheet, they would scan it at like 60% size and put it into a corner. And then this led them to tissues that are extremely small materials. You can pick whatever you want to print out, retain any questions about any of that until after. So this question is, what will the midterm cover up to? It'll cover up two covers up to material a week from nasa covers up to, and including Wednesday's lecture on February 15th. And therefore the midterm, you'll have a week after that to study further material quite. The question is, will the midterm review session have a Zoom? Often there'll be a Zoom Room and the TAs will record on Zoom how clever they won't be monitoring the chat so they won't answer your question builder. And then the TAs will put effect session. The question is, you need to write matrix could put the brakes on a PC. Now, there is a gradient that would come out of the matrix cookbook. We'll provide that gradient. Other questions. Yes, it's almost that'd be what remained. I'll provide simple, in fact, the gradient of its two norm or the ones that we derived in class by the backprop gradients. Stop pumping through a matrix vector multiply or a matrix matrix multiply. You see if I've got under teaches other questions on admin. The question is, will the exam cover homeworks for Biden? Will cover homework four. We're going to finish the material for homework number four today. Homework number five is on convolutional neural networks, and we have pushed the due date of homework number five past the midterm. So the lecture material up to an including next Wednesday, we'll have some convolutional neural networks, but it won't have all of it. That's pretty hard to do homework, number five, so it'll just be whatever we get up to understand and swipe this one. Are there any questions? Sorry, can you repeat? Your question is, what do you need to be prepared to use knowledge from the homework to solve questions on the midterm? Yes. Will there be any way to tell you? Yes. We may ask you to write code on the exam. It will be handwritten. Yeah. I got it. So Tom ways terrifies. We're not going to be testing on it'll just be on more likely concepts around how you'd like to add something. Other questions. Are you allowed calculators? You're allowed to bring calculators, but we typically design the exam so you never have to use a calculator. Other questions. Alright, I just have one last bullet point, which is a word of thanks. So we all know last factor didn't start off in ordinary way. And while I was up there, I can confess to you that my mind was totally flooded and I wasn't sure exactly what was going on and how to respond. And therefore, I wasn't even process what was said in the audience. And when I went back to look at the video, your support for me. And that meant a lot to me because I think that honestly that's probably what's most effective at helping the pranksters to eventually leave. So I just want to say, Well, where did they so all of you for your support. And it's just really appreciate that. And I think that that was what's effective to meet you all. Alright, we're gonna get back into material. So last lecture, we were continuing to talk about things that are tricks, they should be regularizations, I think helped to increase neural network performance. We talked about this concept of ensembling, which at a high level is to say, instead of training one model to do a task, bot, training them, or some number and average the results together. And the basic intuition is, for a given example, if those ten models make independent errors, if one model gets it wrong, then it's wrong. But if you have ten models and only two or three of them and get them wrong, wrong, but the other seven get it right and you average the results are probably gonna get it right. So as long as the models are making independent errors, if you ensemble them together, you will get better performance. And we should last lecture some tables that talked about it showed that compared tree. So this actually motivates a technique that is very frequently used in deep learning to regularizing network called troponin. So why dropped out? Well, we also talked last time how we could build them ensemble by essentially training ten separate neural networks. But train a neural network to be extraordinarily expensive so you don't want to devote the computational time discontent networks. Dropout is something that will basically allow us to get some of that regularization effects of ensembling. And while we won't cover it as much in this lecture, Ian Goodfellow in the deep learning textbook. Significant space to, to talk to you about how dropped out is approximating, ensembling through bagging. Let's talk about them with a diesel to drop that arm. So what happens in dropout is it starts off with a hyperparameter p. Alright. Let me first give the starting, let's say that we have 100 artificial neurons. What we do in dropped out is you randomly a given neuron and we set it equal to zero, meaning that we chop it out of the network. So if I had 100 neurons are, what I would do is I would draw 100 Bernoulli random variables with probability p. So this probability is essentially going to tell me how severe by drop-down. He told me the probability that I keep a neuron. So what I would do is I would draw 100 of these random variables. And so because Louis, they're going to be zero or one, it's going to be one with probability p, and it's going to be zero. With probability one minus p. I'm going to take, what I'm going to do is I'm going to take this mass m. I'm simply going to multiply the activations H of my 100 neurons by this mask pattern. So that goes around the activity where we drew a zero for that Bernoulli random variable. All right. Typical values of p are 0.8 for input units of the network. And then at t equals 0.5 might be a bit severe. I think in the homework we give you, P equals 0.6 for the hidden units would be that you would keep 60% of your units. And then on a given iteration of training, throw away or set to 0% confirm. The picture that you have in mind is that this is your neural network, shown here. This neural network has 13 artificial units. And what we would do then is for each of these artificial units, we would make a mask. The mask is also 13 dimensional. The mass is going to contain zeros and ones. And so I multiply this mask by my artificial units. And the ones that multiply is zero or effectively dropped out of the network. And so for a particular mask, this might be the network on some training iteration I is instantiated since all of these other units hoping trump card. Questions on the mechanism of what's happening here. The question is, does a massive change with every iteration? Yes. Over the course of training? You'll notice. So first off, if we have capital N units, you'll notice that there are two to the n possible configurations. Since every single unit can be in one of two states, there are dropped down. And so we're going to talk a bit more about why dropout it is a good idea. But at least the first idea that you can see is that if we're changing this mask, every iteration of training, what we want is that this neural network robust and works well in many different configurations to work out well when despite units or dropped out, it has to work out well if you go another subset of units or talk down on the bench for a given me. Any division can be wonderful. That's a good question. So what do we do in homework for author? Always putting out, I'm using the word iteration here and that could be vague because it took me back. They're doing a different mask on every mini-batch or you're doing a different task on everything. I'm going to guess that in homework number four, we tell you to do at every thought, but I'm not sure. Yeah. Yeah. So it's almost surely will say it will typically use the same probability for each layer. So usually you would want the probability to be quite high for the input units, meaning the first layer, because these are the ones that are capturing information about your images. So usually we might the first P for the first side, The Chief of the first layer, something like 0.8. And then these ones, you can be potentially more aggressive. People do hyperparameter. Values between 0.6 is 0.9 are typical. In the teal shirt. First, we define talk in general or the whole network with one exception. You might define it for the implementer separately, but generally it's to the floor. Yeah. Yeah, missing in this example, p is the probability that we keep a neuron. So here, P is the probability we keep a neuron. And we're gonna go with that convention for homework number four. But then the student mentioned that in the applied torques dropout layer can use a public IP dropping neurons. So instead of passing and p, you would ask them one minus p. So, but for the implementation, but we're going to do with p be the probability of getting it wrong. Does this also work? We basically have different mass for every single one. That's right. Yeah. It's asking if instead of doing dropout on the pocket into it on a batch, does that lead to better performance? I'm not sure if the answer doesn't, TAs or anyone else. It's something that we can talk about homework number according to my lab experiments, but I don't know if there's a consensus. One can be approximated by making the line. I see lots of people. Yeah, that's a rocket ship. And Tom y are both essentially stating that it may be too aggressive to change the dropout batch. Let's take these three participant aboard. Great. The question is dropping values, especially normalization. You've done it before or after batch normalization should be done after that. So the question is, when we multiply the activations by this mask, is it before or after? Oh, yeah, just send me the testing phase. You will get to that. You do something else in the testing phase. So we'll have that repeat just like wonderful. The question is, how does this affect the backpropagation? So when you do the backprop, you will need to backprop through multiplication by t. That talking through a simple application should be something you are all comfortable with. So you're not going to talk about the backprop lecture, but you'll implemented on homework number four. Alright, so that's trumped up. This is what dropout, so ******* good. So here I have my affine layer, wx plus b. It goes through Rava and that gives me they didn't do this and they are one. Each one. What I do is I just draw this mask of the same size as H1, where it's going to be one or zero with, it's gonna be one with probability p and zero with probability one minus. Alright, and then after that I just multiply each one by M1, and that will give me the drunk units in there. Alright, so this is a super simple operation to implementing code. It's just an additional two lines of code for each layer. Alright? So that's how the forward pass changes when we implement Dropout in one message. Remember that for each net, right? Yes. Oh, yeah, so and this other student questions, tomboy, staying here, I defined one mask for the entire network and then multiply this mass by all the units. And here I am doing it per layer. They're equivalent as long as the parameter p is the same. But this is how we wrote it out for layer in this example. Alright, so that's struggling code. Now we're gonna get back to this question, which is, What do we do during testing time, right? Because and training time we just kept generating all these random. Nasa gave us different configuration to the network. So what configuration do we use intestine. We're gonna give an intuitive argument for what they should be. And then the argument is that let's say that we had four units, H1, H2, H3, H4. And these are their weights and the rest are just gonna be W1, W2, W3, W4. And this leads to HR. So let's say that for this example, we're going to have a dropout parameter, p equals 0.5, right? P equals 0.5. Then for these four units, Let's say I do training iteration, one. Iteration what I dropped that mask on these four units. So the mask will be for the first four units, H1, H2, H3, H4, and it's gonna be 1010, right? Sorry. This means that HCl is going to equal ReLu of W1 x1 plus W3 X3, because those are the only units true mammal. It's the same for every example that alright, and then let's see, I'm training iteration to redraw a new mass m. And now this mask here is 0101. And so for training iteration to the network was using the second and the fourth unit. So the ReLu of W2 H2 plus W4 H4. Alright? Now let's say that in the test phase, we decided, let's not use any configure any particular configuration. Let's just use all the units. So let's say that in the test phase, or at least to start, we're going to abbreviate this. The test phase. Let's say that we just did h equals ReLu of W1, W2, H2 plus W3 X3. And then WE, for each four. If I were to do this, how would the statistics of H out in the test phase compared to the statistics of HR in different periods. Right? The skill will be different because in the training phase, we throw away half arguments. And so on average, this h out and the training phase will be two times less than if I were to use all of these units together. Alright? So what we then do is for the test phase, equalize the scale of h out. We're going to take all the units together and simply multiply it by the value p. And therefore, the scale of HM, the test phase where I use all of my units will be similar to the scale of HR in my training days. Another way of doing this is that over many iterations or training, the contribution of wi, HI. So the contribution of unit one wasn't always just like HI. W1, x1 only existed in the training iterations. So the contribution that WISMDI to each out was P times wi times HI. Alright. So this rule of scaling these values by P captures the fact that these units only contributed P, percentage, or proportion of the test phase. What we do is we don't use a particular configuration. We use all of the units, but then we scale down how much they contribute to factor p. Any questions? Yes, here is the p is the probability of other questions. Alright? So what that then means is that at least in this current iteration is written. Sorry. There's one more thing to say here, which is this row here of scaling the piece. You can think of as scaling these weights, W1, W2, W3, W4 by the value p. This is something called the wage scale inference rule. And although we haven't very intuitive argument for why it's reasonable, like we just discussed. This is from the good Philip book. There's not get any theoretical argument for the accuracy of this approximate inference rule in deep non-linear neural networks. But this is another one of those things. It seems to be the captain, but we don't have a theoretical reasoning. And then instead of scaling the weights in this class, we're going to scale the activation. That's typically how that dropout rates and productive. So instead of multiplying this P within the ReLu with these grapes, what we actually do in this class is we can feed the baby. When we put the p outside, we multiply the activations h out by a homeless person is it will be after the batch norm? That's correct. The question is, why is it putting it? Why is it that putting it outside the ray moves makes a difference? Because if this number was greater than zero, then the p would come through anyways. Yeah, that's, that's correct for the rating occupation or other activation. Alright. Okay, so because in the test phase, we take RHEL, we multiply it by P. This is what the testing phase looks like. So we have the testing phase where we're going to use all of our units. Do I find later than our ReLu? And then we multiply by p. We do the same thing for layer two, and then we have to type it here. This is W2. So that's this inference rule. Or from now, there's something that we don't like about this, which is we generally don't want to change the text of our neural network class. It would be great if we could just add dropped out. The training and not have to have like a different test versus test underscore dropped out that has this P here. So the way that we can get around this is we use something called inverted dropout, where we kick this p factor that would normally be in the test phase. And we move that key factor into the training phase by dividing each one by p. So p equals 0.5. If I take each one and I divide it by P, Then when p equals 0.5, that would be like multiplying these things by two, right? So now if I multiply these things by two, the scale-up H AB is the same as this h Tau. So inverted dropout says to get rid of the P and the test phase so that I don't have to modify the testing code. I'm going to in the forecast, simply divide my mask or my activation to call it b by P. Then my testing base, I don't have to have an HP and the test case. Any questions there? All right. So that is trumped up. The question is, is dropped out to the inputs the same as profit? It could be, but if you crop an image or do you like black and, or abuse or allow parts of the image? Then for fully connected network, it could be, I think that's an equivalence, but like for a convolutional neural network, that will be too many units in the next layer having a spatial zeros, which wouldn't be the case if you just draw on a random mask so that there are differences? I would talk ever at Foundation. Do you mean does our population change when we incorporated dropout layer? So because basically non computational mask, right, we're going to have an H1 and now it's going to multiply a mass m. And that's gonna give me, I'll call it H1, capital superscript D for dropout. So there's an additional multiplication operation here. And you're going to have to backpropagate through this multiplication operations. Question is yes, that's correct. Other questions? Questions. And so that's the verdict drop that. And this is how you're going to implement drop that on the homework. Alright? So you might have been the question, how is this a good idea? It's really funny because I'm Jeff and his group are the ones who discovered dropped out. And they were presented once in a talk because they said we did this pain. They have been by accident. I did this thing called dropout and they found that lead to increased to performance and 2%. And the audience are like, That doesn't sound right, but it's often a simple enough that the audience, some people tell anecdotes of how they were coding it on the fly. By the end of the talk, they saw a 2% increase also in their data. It's a very simple and quick way to get performance. But the fact that they tested it brings up this question that we all have, which is dropping out. So again, I'll refer you to the Goodfellow textbook for more details. But Goodfellow talks about how approximates bagging, where when we have dropped out, overdoing it. And every single key part, we are using a different network configuration. Alright? Our total task space, use all of the units. It's kinda like adding all those numbers together. And so, because you have many different configurations that must be good at predicting the output and can predict the output when you combine them all together and you're testing phase, you have an effect of ensemble as a result of dropout. Other reasons why he dropped out might be a good idea. You can think of dropout as regularizing each hidden unit to work well in many different contexts. The context being different masks. So a unit has to work well. If it's an active in two consecutive epochs, that would be two consecutive, two different architectures, effective architectures. I'm asking that you didn't have to worry about in both of those contexts. That's what number two. Number three is trumped up may cause units to encode redundant features. So maybe if you want to detect the cat, you need to get features like that. It has 20 years and the network has to learn that there's only one artificial neuron remembers that learned it might not be robust if the activations on that neuron are in any way altered as a result of some other factor in the input. And so when you have dropped out, many neurons have to encode these features, especially if their vital. And so these two redundancy in the network. One more thing to note is that when you run dropped out, typically you have to make them larger. So if you have a network with 100 neurons and it does well with, without Trump got to have good improvement with dropout. And we'd have to increase that to 100 units to something darker. Which also makes intuitive sense because when we dropped out or we're going to be throwing away a bunch of pounds. Any questions on any of these intuitions? Copying off some units. Dropout rates are set to zero. Tom was asking when you do drop that drops out. We all the ways that companies that needed or sexes there are. Tom was asking, does that have an effect of something similar to that L1 regularization? Which bases are always read. The answer is no because if it was always trumped up in gas, but because on some interval it dropped up, those will lead to great effect change but switched to be nonzero. We apply. I really only some delays. You can trump. I've just talked about layers if you want. You certainly see in the homework, we will put Trump on every layer. There are some layers, but he wanted to, particularly for companies and big fire information, regularized some boring display the chalk out. The question is, how would you determine which layer you should not do the drop that arm. Yeah, that's not something easy to know, a priori, which is why people just generally quite tired. Hi, The question is, are redundant features a good or a bad day? Redundant features can have a quote unquote con in that invaded more than expended to do the same computation. But if you can tolerate that, but it's generally a good thing because then you aren't reliant on just a few numbers. It's kind of like the idea about sampling, if you have any that perks that will look for this future appointment. Yeah, so even if one neuron, as is often the others, are there. Is it generally the case that we had better testing versus training accuracy? Let me do drop that. You mean like an absolute value of testing higher than the training node. Generally, the testing error will still be, the testing accuracy will generally be worse than the training still. But it'll reduce the gap. The training error will, the training accuracy will go down and testing accuracy both. So that's the regularizing. The question is is the drop-off I'm asked if he didn't each batch or is it saved over the entire talk? Actually, I'm going to just ask the TAs to look at homework number four. We were saying you bought because we think it might be a bit excessive for each batch. I haven't I haven't written dropout code in awhile. So the TAs will give us a firm answer in a minute. But the question is, if we do it for a whole, you talk with at the poem doesn't kind of definition for it. There'll always be epoch, which is that you pump is just one facet of your chain. I see. Right. Okay. So this to me is asking a question which is beyond the scope of this class. But if you're a deep RL context when you're turning from a replay buffer and you're not going to go over all the samples from the replay buffer, or you're going to sample for every iteration. I don't know what the best heuristic is, but you may want to keep a mask or a few, a few videos to saplings from the takeoff it, that would be my guess. Music, any last questions here on top up? Alright, so that concludes this lecture on importance of various regularizations, augmentations and industrialization is that improve the performance of neural networks. So remember initializations thought here is unfortunate. Regularizations. We have things like dropout, batch norm, and then did augmentations. We've talked about how we can just take different costs of images. And that's a way to get a 2% of Africa is considerable. Okay? So the TAs are correcting that. Crafting what I've been saying incorrectly, this vector, which is that the masks are drawn for bash, not premium fonts. So they are changed every single batch. All right? Okay, so in addition to these tricks for improving the performance of neural networks, you also know that there's one more component of our machine learning problem, which is the optimizer, and that's the cast of gradient descent. And are there particular ways we can improve this for deep learning? I'm going to say one thing at the top also, which is that even though all of these optimizers were developed to deep neural networks, they're also available outside of deep neural networks have just general machine learning algorithms. So that's gonna be the topic of the next set of slides. This is gonna be optimization for neural networks. And we're going to talk about ways that we modify stochastic gradient descent with momentum adapted, brilliant and adaptive moments. And this is something that probably a lot of you have heard of called the Adam optimizer. And so we'll talk about exactly what. We're also going to add these at a high level, introduce you to the concept of what's in order. Gradient descent methods are. Although they are rarely seen in neural networks for reasons that we're going to talk about later on. But if you would like to learn more about these than the optimization classes to 36, BMC will cover them. The associated reading with this topic are components of chapter eight. So what we know, going back a few weeks, we now know how to design neural network architectures for loss functions and the activation functions to choose. We know what the hyperparameters and cost or loss functions for these numbers should be right, for classification would be, we should be using the cross-entropy loss, which is the negative log likelihood of the softmax classifier. We know how to calculate gradients of this loss with respect to all the parameters in the neural network. That was the backpropagation of lecture. And then we talked about how to initialize the weights and regularized and networks and ways to improve the tray. That was the beginning of this lecture. And just like we were saying, now, we know how to optimize these networks with stochastic gradient descent. But can sarcastic gradient descent the improved and because the sector, the answer is yes. So let's get started. So this is the destination will be using for this factor, the cost function. This is just the loss, is what I'd usually written as L, but we will use the variable j as well in this vector. And then our parameters are theta. We know that then stochastic gradient descent or Mini-batch gradient descent, I'm using them interchangeably, is that we update theta by stepping in minus epsilon times the gradient direction. This is a slide just from a prior lecture, which is just recapping the differences in batch, mini-batch and Catholic algorithms. But remember in practice we always use the dash. We call that mini-batch gradient descent, stochastic gradient descent. Alright? So this again is what we know. We'll just do this because when we introduced the other ones, we're going to follow the same structure. So let's say that we have an initial parameter setting theta. We have the minibatch with ME examples, which means that we have, this is C4, ten images, x and m labels. Why do gradient descent? We compute the gradient of the loss function with respect to those parameters. Remember that's the softmax loss and grads function. You didn't hover number two as well as the backpropagation homework number three. And then after we get this gradient g, we update theta by just typing in minus epsilon or gradient direction. We know that the way this works is that we have some function. This function is like softmax, loss and grad, which you implemented in the code that I'm going to show here. These axes are the weights. Softmax, loss and grad is going to return to you the loss function and the gradients and then update the width. I'm just going to subtract off epsilon times, right? We're going to return to this function called Beall's function that we talked about in which the capsule gradient descent lecture. And then we're going to optimize it or we're going to update it with our new optimizers. So here's just a reminder. This is the video of what stochastic gradient descent looks like on this fault surface. So starting from initialization, this bottom right-hand corner, stochastic gradient descent will follow the gradient which is perpendicular to the contour lines and go to the optimum. And here is a video of that and Cassian gradient descent, but starting from another location. And you'll see that it's moving slowly. Let's talk about the first thing that we're going to do, which is momentum. So we talked about how the cost of gradient descent, we have to set the learning rate epsilon. And sometimes when epsilon is a bit on the larger side, we'll run into problems like this green descent curve where we get into a valley, we start to zigzag back and forth, right? So this zigzagging back and forth along this valley that researchers to think of an idea called momentum. And momentum works in the following way. Let's say that we weren't exactly. So we start out at a location Studying of our legs and our first gradient points in this direction, I'll call that G1. Then we get another gradient and that points in this direction, G2. Do a third gradient, G3 like this. And then maybe a G54 like this, right? So our average class grade is to get the overall direction that we're going in and cancel out these zigzags. Alright? How does it do that? It sets a new variable d. This is our momentum. It's initialized to zero. And then we have a hyperparameter called outlet. And it also is essentially going to be how much of a running average to do. This will become more clear to write it out. So let's go ahead and write out this momentum update step. So first we compute the gradient of g, and then what we do is we update this momentum vector. So let's start off at the initialization. We have that's at the very first time step to the substructure will be initialized to zero. Alright? At the second time step, we compute a gradient. And that gradient will be equal to g1. Alright? So that's my gradient at the first time step by dy is going to be alpha times the prior B, which is zero minus epsilon times by gradient, which is G1. So b1 is just going to equal minus epsilon G1. Then after that, we're going to calculate a V2. V2 is going to equal alpha and alpha. Let's say it's gonna be like 0.9 e.g. it's going to be equal to alpha d1 minus epsilon times the gradient at time step two, which is G2. And if I go ahead and expand what D1 is here, I'm gonna get that this is equal to minus alpha, epsilon one minus epsilon g2. L"
"So actually for this one, we could actually solve it using the other thing we know, which is a law of total probability. So we know that this is equal to the sum over all possible values of little n. And this probability here is just the numerator, this expression over here. So if you plug in this expression into here, then you can start to do the simplifying algebra that should lead you to the correct answer for this. So it's going to stay in terms of n though, right? Well, this expression here is going to have an n in it, but then when you sum across all potential values of n, then it should disappear."
"So basically we have coin flips that occur with probability, the tails occurs with probability 1 minus 1 over e and the heads occurs with probability 1 over e. And so if you keep flipping this coin you want to know the expected number of coin flips before you get to the heads. And so this is something that is modeled by what is called the geometric distribution. And so for the geometric distribution, let me just pull up the page here. So in a metric distribution. We are computing the number of coin flips before we get one success. And so, in this case, one minus p. We would have k minus one of them for the success to be on the kid coin flip so the probability that takes cave coin flip to get the success would be this probability. Okay, so resolving for K, which is one over the probability of a success. So it's one over one over e, which is d. Okay, I didn't know where that where his distribution was coming from, but now I realize this geometric."
"So be checking Piazza because we're going to be posting exam updates there. And we also have a Zoom room where you can ask us questions during this exam time. Right? The final exam is going to cover up to and including everything from last lecture. And so today's lecture, we're going to cover PCA, but there will be no PCA tested on this exam. All right, so everything up to and including last lecture, everything up to and including homework six. That's what is fair again for the final exam. We have prior years as final exams, uploaded to CCLE in the past exams folder. But we want to note that last year we gave the exam in class. And so the final exam last year was an hour or 50 minutes. And it was shorter than a normal final exam, which is three hours. So if you're looking at last year's exam, please keep that in mind that it was for a shorter time slot. And then lastly, just like for the midterm, if you need an alternative exam time, we're happy to accommodate. But please read the instructions in the CCLE announcement and then send me the appropriate email to reschedule for the exam. All right, any questions on anything related to the final exam or any other course logistics for that matter."
"So be checking Piazza because we're going to be posting exam updates there. And we also have a Zoom room where you can ask us questions during this exam time. Right? The final exam is going to cover up to and including everything from last lecture. And so today's lecture, we're going to cover PCA, but there will be no PCA tested on this exam. All right, so everything up to and including last lecture, everything up to and including homework six. That's what is fair again for the final exam. We have prior years as final exams, uploaded to CCLE in the past exams folder. But we want to note that last year we gave the exam in class. And so the final exam last year was an hour or 50 minutes. And it was shorter than a normal final exam, which is three hours. So if you're looking at last year's exam, please keep that in mind that it was for a shorter time slot. And then lastly, just like for the midterm, if you need an alternative exam time, we're happy to accommodate. But please read the instructions in the CCLE announcement and then send me the appropriate email to reschedule for the exam. All right, any questions on anything related to the final exam or any other course logistics for that matter."
"So do you have any idea of like what's the scale of maybe how many nodes of around VA are on an axon and how much larger the myelination sections are? Yeah, so the myelons used to appear to be one to two millimeters. And then the nodes of around VA, the bare patches are about two microns on length. Oh, excellent. It's right on this slide. Thank you. Great. Yeah, thanks for the question, Jonathan. All right. And then any other questions? All right. So today, we are going to finish in the first half of lecture, the basic neuroscience, and then after that, we're going to move on to talk about experimental setups and get into firing rate statistics."
"So does anyone know what this device I'm showing there is? You can write in the chat if you know. Ryan says battery as several people say battery that exactly what this is. This is the Voltaic pile. It was developed by Alice Andrew Volta and essentially it's a stacking of several electric electrochemical cells and it allowed there to be a voltage now across two terminals, which could be used to power circuits. Later on, Gayord Oom came along and he derived this very important equation V equals I R, which I'll be it simple. If you've taken a circuits class, you know that from V equals I R, we get in for we derived equations and concepts like that in a North team equivalent circuits or care cost voltage and current laws. And then here I'm showing one of the very first transistors, which we know then allowed us to make active circuits and underlie many of the integrated circuits that are in our computer today as well as in analog circuits and devices that we use. And so with this and with electricity, we were able to start to communicate information in new ways. And example of this is. Alexander Graham Bell, one of his first ways in which he transuse voice into electrical signals was he would take. And then he would put a needle in water and that needle would vibrate according to the voice, the sounds that you made with your voice and when the needle vibrate in the water that would change the conductance of water and we know from V equals I R, if you change the conductance or the resistance of water that's going to change the current. And so that current could be transmitted as electrical signal and you can convey the signals from your voice to many people. Right. And so this was a voice to electrical. And so why am I talking about information and how we convey it will it turns out that signals and systems correspond to these two things. And so signals are things that represent information. And so in this class and the signal systems perspective will conceptualize these information or signals as our information and then how information then is changed in process is what we refer to as systems. And as electrical engineers or engineers in general, we tend to think of signals and systems in electromagnetic or computer terms."
"So each of these elements, each of these are corresponds to one trial that the monkey did. And so we're giving you a data set where there are five hundred and six trial. And then for every single trial, we write down information. So if I want to access the 10th trial, I would do R of 10. And so what are 10 would show me is information about the trial. And so it would include information like where what was the target that the monkey reached to on this trial. And so if I look at if I asked you what target is the monkey reached to on the 20th trial, you would do our 20 and you would look at the target field here. So again, our 20 is a struct and then our 20 dot target is going to be a 3D vector where the 3D vector tells you the X, the Y and the Z position of the target. And so the position is always going to be minus 70. So we really just care about X and Y. We just care about 2D. And so in this first cell right here, what I ask you to do is I ask you or what we do in the code. I don't ask you to do this. We say what are the targets of the monkey reaches to. And so what I've done here is I'm going to iterate over every single trial. Now R is just going to return 506 a number of trials. So for a for loop over trial, 1 to 506. I'm going to plot the X position of the target and the Y position of the target with a marker. So if I run this cell. I'm running right now and it's normally not the slow, but again, my computer is doing too many things right now. Do people see a figure has appeared? Or do you only see the mat lab? You only see the mat lab. Okay, let me show my entire desktop. I'm running a share screen only shares the current screen that you're on. Yep. Right. So I'm sharing my desktop now."
"So for 2F, I mean, you said that you had factorized how the key of C is, and you had a key of C, right? And then the summation of A for probability of A given C just goes to 1, right? That's correct. You had something like this. Yeah. Okay, and you had some over D. Yeah. So P of D, probability of A given A, D. Yeah. Great. All right. Oh, yeah. Okay. Then if you had the set, then, then, then, then you are good. Sorry. I think I misheard you earlier. So then that would just go to A given C just goes to 1, and then, probably, of D goes to 1 also, right?"
"So for the capacitance, the area should be 2 pi r times the length. For the resistance, the area should be a pi r squared. You think you'd draw that out? Yeah, sure. Let me grab my iPad really quickly so that I can share the screen. So, I'm just going to join the Zoom room. Let me share my screen. Let's do one more thing, which is stop the audio of my iPad. All right. Alright, so for 5A, we want to compute an R axial times C membrane."
"So for the even least space samples of, by even, I mean like on even numbers, we can represent it as a sum of delta of t minus 2k and then k is the whole range of integers. And we can represent an odd number as 2k minus 1 or 2k plus 1 or any 2k plus minus an odd number. I just used 2k minus 1, I think. So in that case, we would, I used, I wrote delta of t minus and then in parentheses, 2k minus 1 plus tau. Great. Great. Yeah, so that's, that's correct. Thank you, Eric, for that. So this is correct and then we have the plus tau because all of these blue impulses are delayed by tau. So we need a minus tau in here. And so that's what we get from this expression that Eric wrote for us."
"So for, uh, so problem two, like towards the end. So for the spy times, so I know I for the last part, when we plot the CV, so the average asset in the CV of the ISS. So for understanding is for each of the eight, which part of the sec? I was at part G. Part G. Okay. Oh, okay. So for each of the, uh, reaches, so see how there is 100 trials in each of the trials, we have the spy times and the original spy times are like the capital TN where they show, like it's increasing versus if we do like a difference array, et cetera, we can find the individual spy times. So when we calculate like the average by a side of a given reach, I guess my intuition is, do we calculate first the mean of a given trial? And then from there, do it over the different trials or to be like, add everything up and then divide about a total number of, I guess, ISIS."
"So he'll. Maybe if you ask me how you want your own, could you like specifically design it in a way where say you want the red one there to simulate only the left green one and not the one on the right. Then you can have the purple one signal be in such a form that it acts as a low pass filter for the part of the red wave that you want to chop off and not the red right one to not see. You can just filter it out using a special touch to pass on the purple one so that the right one only sees the purple signal and the left one sees only the red signal. Could you is that possible to do or. It's not currently possible because it's not straightforward to implement a filter from just an electrode that can stimulate. So that's why instead of filtering the ideas here are constructive and destructive interference because really all we can control are signals that we send out. And we could we could try to position and set the parameters of the signal so that they will destructively interfere some cells, but then sending out a signal and asking it to do a filtering operation. That is non trivial. I'm not sure how that would happen. So that's a rational implant is an example of how we can write in to the central nervous system. You might also have a cochlear implant. So for someone who is deaf. They can't hear outside noises. The thing is that actually a cochlear implant is not. Also operates on on a principle that is not too difficult to understand here. I'm showing you the cochlea. The cochlear got. And what happens is that along the cochlea you have a bunch of these hair cells."
"So his new values would be pairs Y2 minus Y1, Y3 minus Y2, Y4 minus Y3 and then Y5 minus Y4. And now we have five X values, but just four Y values. And so the question is then what alterations are what we make to the X's? I'm curious, has anyone run into this and have an idea or want to share what you did? And this is for the task three of me doing the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, one where the Zh That's a reasonable solution, which is Y2 minus Y1 uses data over a window. Let's say that the bin size was 10 milliseconds, so Y1 and Y2 use data over 0 to 10 milliseconds. And so maybe we use the x value at 5 milliseconds, which could be approximated by taking the average of x1, x2. Since the value at 5 milliseconds would be the average between x1 and x2, the average would be the linear midpoint between a line connecting x1 and x2. So that's one solution. The solution that I would do is I would just take x2, x3, x4, x5. And so I would chop off the first value. That is one other solution. You could use x1, x2, x3, x4."
"So I'm confused how like if there's like time required for like information to propagate. How is there a baseline neuroactivity? Or is there baseline neuroactivity like before anything happens? That's correct. Yeah. So I didn't explain these. I mean explain those really quickly. Thanks for pointing those at Andrew. So the baseline neuroactivity corresponds to the activity that's just in motor cortex when you're holding the center target. But there is no target that has been shown that you should plan to reach to get. And so this is just activity when you're holding a center target and you don't know what you're going to do next. And blue here. Yeah. And then in blue here will be the activity where a target comes up on the screen. And there's going to be some way to see maybe about 100 milliseconds for your eyes to proceed this target for that information to go through the visual cortex. And then eventually make its way to the motor cortex. I was just confused because they thought like baseline activity meant like activity. That was already part of like thinking about the task. Yeah. It isn't. Yeah. It's only just the activity before any target is shown. Thanks for that clarification. Any other questions here? So does it do we use the baseline activity anyways in the record decoder? For the discrete decoders that will have you using class, you will not use the baseline activity for the decoder. Since it doesn't correspond to any particular reach direction."
So I'm going to end the meeting now.
"So I'm going to move this to the next slide so that we can just say this slide for number two. So is A independent of B intuitively now? All right, and so if we want to show that P, that A and B are independent, then we need to see if P of A comma B does this equal P of A times P of B. Remember that since these are not independent, you can also just come up with a counter example and show in that counter example that A, that A and B are dependent. So let's go ahead and just try to answer with a general proof. So we have P of A comma B and we want to see if it's equal to P of A times P of B. So maybe think about it for 20 to 30 seconds as to what the first step here would be. Can someone write me in the chat or I'll space your hand for what the first step you might do here is? Total probability, that's correct. So we're going to remember we want to show if this is true for this particular graph, right?"
"So if S was here, then the number of spikes would be one. If S is here, the number of spikes would be three. All right. And then last lecture, we spent the majority of lecture proving the main property of the Poisson process, which is that the number of spikes at time S. And so this means the number of spikes going from zero to all the way up until time S in that window. Big N of S is going to be a Poisson distributed random variable with mean lambda times S. Again, S is the enough of time that we're looking at for spikes. All right. And what that means is that I know the exact probability to distribution over the number of spikes I'm going to observe in my window from zero to time S. And so I know the exact probability of observing zero spikes, one spike, two spike, et cetera. And that's computed from this expression right here. Right. Any questions here?"
"So if you write that in like the trigonometric form, you'll get it to always equal negative 1. And that's how you get the negative x of t. Okay, great. So one thing along the line, you said you subbed in T plus big T over 2. And what was the rationale for that? What do you like do you mean like the motivation for plugging in? Yeah. I would I just proved this property. I didn't actually got it. Yeah, that's that's that's good. Thanks. Thanks for that Eric. Yeah, so let me tell you how. This is how I thought through this question, which is I want I know that and it's long these drawings exactly of what Eric said. And so we'll do what Eric said. So, but just a slight with minor differences. So we know that these even Fourier coefficients are equal to zero."
"So in 50 milliseconds, I'm going to count how many spikes happened for neuron 1, 3 spikes happened for neuron 2, 3 spikes happened for neuron 3, 0 spikes happened all the way down to 192. All right. And so this is going to be YK, my vector of neuron firing rates at time K. All right. And the goal then of this problem is when we want to build a brain machine interface, we want to calculate some function of Y of K that predicts my velocity X of K. All right. So this is saying, given that I observed some neural data in the future, I want to pass that through some function F that tells me what my velocities should be. And that then lets me do the following. Let's me record a paralyzed person's neural activity. Pass it through a function F to get the velocity of the cursor and then update the cursor's movements on screen. And so that was very yet. We were going to try to learn this function F. And so we talked about just how to do a simple 1D example."
"So it would be the x axis of our spectrum. Okay. I'm going to get the delta. Because if we wrote the delta train out as a summation, we'd have delta of omega minus k pi, right? So then would we say that omega on that exponential would be k pi if we were writing it in some machine notation? I knew our correct. Yeah. So yeah, so it makes for pointing that out. Daniel is giving you a further simplification, which is this impulse here is impulse train. The delta pi omega is you can view it as sampling this exponential. And so Daniel was saying that you could further simplify this as a pi. These impulses are delta omega minus k pi e to the minus j omega 1 plus tau. And because the omega occurs, the impulse occurs that, sorry, the impulse occurs that omega equals k pi."
"So it's going to be pi, and then delta pi omega. All right. So that's for the purple one, and then what happens for the blue one? I think Professor, we can just multiply the same pi delta pi omega with e to the negative j omega times 1 plus tau. Yep, that's correct. Thank you, Brompton. So that gives you then your answer. Your big f of j omega is just going to be the sum of these two. Any questions on 2b? Yeah, Professor, I had a question. What is that? I got confused about which omega that actually was on the e to the minus j omega? Yes. This omega is the frequency variable."
"So just like in the keyboard example, there's a click signal to select the target. In this example, there's also going to be a click signal or a binary signal, except it's not going to select the target, it's going to rotate the hand so that she can drink from the bottle. So let me go ahead and play this video. Sorry, I'm not sure. You can see she controls her water gun to bring the cup of coffee to her. And then she's able to drink some. All right, in the video, so at that point, I said, when it bit longer, you would see the completion of the trial. And she has this big smile across her face, because this was one of the first times that she was able to drink from a coffee cup without needing someone to directly hand it to her. Any questions here? All right, so in another example of the robotic arm. So the video that we just showed you is from a clinical trial called the Brain Gate Clinical Trial. It's part of Massachusetts General Hospital in Boston, as well as Brown University. And now they have sites also at Case Western and Stanford University. There's also another clinical trial at the University of Pittsburgh."
"So last lecture, we had derived the equation to go from mapping neural data to kinematics to velocities via least squares. There are a few more details in that lecture about training and testing data that we didn't get a chance to talk about yet. However, I thought it would be better to just go over the project data first and if you had time, we'll talk about those things later on. But it's not, we'll cover them next week, but you don't need to know them to do the product. So the goal of today is to really just get you all the, I guess you set up so that you can do the project. So let me just move around windows. Okay. So on CCLE, we put up the 189 project PDF and there's a lot of description in there. We're going to walk through code today that essentially goes through that description. And so please read the description again, even though hopefully I will be clear after we go through some of the code. We gave you a data set, which is this J are 2015 12 of our truncated to dot max file. And so this is going to contain the data recorded from a monkey while the monkey was making reaches and we were recording neural data so that we could get training data to map neural data to reaches. We also uploaded a helper function, then, which is a supporting function that that will use to make our lives simpler. And then the critical thing that should be on CCLE is assignment code plus part one dot M, which walks you through building a decoder. And walk through that today, as well as it does part one of the project. All right. And so, let me just look at part one of the project. Sorry, I should have the PDF open."
"So last time we asked about, I asked about equilibrium versus steady state, and so like you mentioned that equilibrium is a situation where it's like a single ion species that can come to a situation where no ion is really moving. There's no net movement, there's no movement, but like a steady state it's like no net movement. Am I interpreting that correctly? The key difference to the steady state is that in steady state and equilibrium, there is both no net current, but in steady state, you need to put energy into the system. gradients so that you can maintain these unequal drift and diffusion currents for each ion species so that the ion species concentration, the diffusion currents don't reach some steady state value. So still the steady state and equilibrium eventually do come to like a no net movement. Yeah, both of them result in no net current. No net current. Not necessarily. I see now. So you're saying that, okay, that makes sense because with your steady state, you can still have a chemical gradient as we see in the Constant voltage."
"So let's say I dragged it by a time T equals 0.5. Oh, sorry. I wasn't keeping track of my x-axis. This would be 0.25. And earlier when I said earlier when I said that the orange wrecked was over at time one I met time T equals 0.5 because I wasn't paying attention to these axes. Apologies for that. So if I drag it halfway over, which is at T equals 0.25, then when I multiply these two rectangles together, I'm going to get a wreck that has with 0.25 and height of one. Alright, and so at time 0.25, the overlap of these two rectangles when I integrate it will also be 0.25. And so at time T equals 0.25. The area under the curve is 0.25. So as I continue to drag this rectangle to the right as it overlaps this black rectangle, they're going to have more and more overlap leading to a higher and higher integral until at time T equals 0.5. That's this time over here. T equals 0.5. They're fully overlapping. And so when I multiply them and integrate the area under the curve is 0.5. And so this pink point is T equals 0.5. And we talked about last lecture as I drag this rectangle in the trend at which increases is going to be allowing. Alright, and then as it drags out, we're going to have a line decreasing. And then after time T equals one, which is when the rectangle. When the impulse response rectangle is at time. There T equals one. Then they no longer overlap and the integrals is there. So with this same procedure, you should be able to calculate or you may be already calculated the convolution of these three as well. I want to pause here and ask if there are any questions on any of the answers."
"So let's say instead of just 144, 140 Hertz wave, you're listening to now orchestra music. That's a changing stimulus over time and that's going to be represented in this neuron. Right. So the second thing temporal characteristics of the stimulus, that's just again, relating how the stimulus affects the neurons firing rate. Right. Now what do we mean by intrinsic neural dynamics? What this means is that this neuron isn't in isolation. Rather it's participating in a neural circuit connected to other neurons. And so all these other neurons I'm going to draw also a circles and these all have connections to each other. All right. And then maybe also it turns out that the year cochlea has projections on to some other neurons as well. All right."
"So let's say that little T is equal to one. Right. So what the T on the left hand side is. This T over here is telling me that I want to go to time T equals one and measure the value of the impulse response. Okay. So this would be the impulse response H at time one. Okay. On the other hand, the T on the right hand side. So the T here is something that varies across all time. Meaning if I want to integrate my impulse, I'll have to know the value of the impulse from negative infinity up to time T. And so in general, the T on the right hand side is going to be a T that isn't just the finite time at one point one time point, but I'll need to know multiple values of what this function X of T or delta T was at multiple times. I need to know that to integrate it. I need to know if value from negative infinity to T. And so the output, the impulse response at a specific time T on the left hand side in here in green is going to depend on the input at several times T on the right. And so in particular, it's going to depend on the value of the input from negative infinity all the way up to time T for this particular integrator system. And so that's why you can't just plug in the same value for T here. Okay, any questions there. All right. So with that, this side again is just explaining what we said on the prior set. I'm putting it here for completion. We have a question from one. This is just more of a clarification. Like it makes sense now the different T's and stuff, but the left hand T is that the same T as the upper bound of the integral that we have inside of the H function right now."
"So now the whole game of training is, how do I choose mu and sigma, so that they really well describe the distribution of the data for each of these planned reaches. Okay. Any questions there. last lecture, which was introducing the maximum likelihood framework. So I mentioned that before we derive the maximum likelihood solution for this classification model that we're working with. So you may be confused about exactly what are the parameters, what are the goals of this modeling, etc. We have a question from the swan. Yeah, I just want to make sure. So, you're drawing a leaf solid on the 2d plane that doesn't mean that data point will be like a leaf site. Right. So just a conceptualize of the correlation matrix."
"So now when I record from this one neuron here and I see that it's neural responses, spike chain changes, I need to understand if the change in this neuron or if this neuron spike train is reflecting just the external stimulus or if it's also reflecting internal dynamics meaning that all these other neurons are also inputs to this neuron and could also cause it to fire. All right. And in practice, of course, it's going to be some mixture of the two. All right. And so that complicates relating how we can map the external stimulus to this single neurons neural response. Right. Okay. Any questions there? All right. So that's the first challenge."
"So on Piazza, people have said that this odd harmonics property means that negative X of T is equal to X of T plus T over 2. Big T over 2 or big T is the period. And so that's pretty much the critical insight that you need to be able to solve the rest of this problem. And so can anyone has anyone derived this and or have an approach, I guess, as to how to go about showing this? Oh, yeah, I did. Wait, I did derive it with T minus T over 2, which I think is the same thing. Yes, it's the same thing. Okay. Yeah, it shouldn't matter then. Oh, what I did was just. So we have the Fourier series representation of a signal is like the sum and then times the coefficient and then the complex and exponential. So I since it's odd harmonics, I replaced all the case with like inside the summation with 2k plus 1. And then I just subbed in T plus T over 2 into the thing, the fourth A series representation. Since it's like addition, you get on two complex exponential terms. So one of them has 2k plus 1 and this is multiplied by T over 2 and also omega non. So T over 2 times omega non is should be pi and then this is pi times the odd number."
"So the expected value of xt given t minus 1 is equal to xt minus 1, and so the mean here is xt minus 1, and so that's all good. Now I want to calculate the variance of xt given xt minus 1. And in this equation, this would be the variance of xt minus 1 plus the variance of w. But in this case, xt minus 1 is an observed quantity, it's deterministic, so it doesn't have any variance, it's just a single number. So it's variance is equal to 0, and therefore the only variance in this expression comes from w, which has variance sigma squared. And so this variance is equal to sigma squared, right? And so this equation xt equals xt minus 1 plus w is a restatement of this fact of this distribution over here. Okay. Does anyone want me to go over any part of that again? I lost the motivation behind the addition of w, can you please go back to that? Oh, you mean why we put things in this notation?"
"So the facts of this Gaussian random vector are marginalization, linear transformations and conditioning. When you do these operations, the output is still Gaussian, right? Were there any questions from any of the preliminaries? So Jonathan, I have a question. Yeah, Tom White. So for fact number three, it will also hold if we like if we have a like say an dimensional joint leg Gaussian and we condition any subset on another subset, it will still be joint leg Gaussian, correct? That's correct. Yeah. And x1 and x2 here are vectors. And so let's say that the overall x was 10 dimensional. x1 could be 6 dimensional and x2 could be 4 dimensional or x2 could be 2 dimensional. Essentially, when you condition some of these random variables on the other ones, the result is always Gaussian. All right. Thank you. Thanks for clarifying that, Tom White. All right. So, okay, let's get to our proof then. So last time we had written out our dynamical system with the following assumptions, the noise terms WK minus 1 and QK, they are Gaussian noise and they're independent, which means that at every single time step, I'm drawing new noise, okay? And then last lecture, we had asked this question, if x0 is Gaussian and we have the state update equation. So x0 being Gaussian means that the first initial state is Gaussian distributed and downstream states, states later on in time will also be Gaussian distributed because they're linear transformations of a Gaussian random vector. So if we have xk equals axk minus 1 plus WK minus 1, I asked what is the distribution of xk given xk minus 1? All right. And we deduced last lecture that it would be axk minus 1 and as the mean and the covariance would be W. All right. We saw that because if xk minus 1 is given to us, right? A is deterministic, xk minus 1 then becomes deterministic because we observed it. And then WK minus 1 is zero mean noise. So the overall mean of this expression is going to be a vector, a times xk minus 1 that we observe plus some noise with zero mean. And so the mean is just going to be the deterministic part axk minus 1. All right. And then axk minus 1 is observed, it's deterministic. So it doesn't have any randomness associated about it. And so the only randomness in xk given xk minus 1 would come from this noise term WK minus 1. And WK minus 1 has covariance matrix big W. All right. So that leads us to conclude that xk given xk minus 1 has this normal distribution. All right. Any questions there? All right."
"So the idea is to like ensure privacy through the neural network computation task. One of the, I mean, most of the classes that really help me to understand neural networks are neural networks 247 by Professor Kaub. Then there is a great course on adversarial robustness of machine learning models from the computer science department. And obviously to learn more about the biological systems they try and emulate neural signal processing is a really good, give you a really good understanding of how neural networks work. So yeah, that's about me. Great, thank you, Shashank. All right. With that, I want to just ask if there are any course logistic questions. All right, then we'll get back into material. So last lecture we were talking about neurons and how neurons come in all different shapes and sizes, but they have the same fundamental operating characteristics."
"So the key thing is that tn can be less than or equal to s so we have to consider all possible values of big tn and and changing this tm less than or equal to s into the integral over tm equals t is the is the key insight there. Oh, I see. Yeah, but also I searched on the, I don't know, the Wikipedia, it said like the, there is a continued cases for the, I don't know, the law of probability. It's kind of similar to what in the end you take to the limit and that's something like that. So, yeah. Yeah. I guess I searched in the 10 minute break between the lecture and the office hours. Yeah, so in this case, the the multiplication Can you see my mouse on your screen. Sorry, see what can you see my mouse on your screen. I saw like you draw some like lion dash lion circle before. Okay, when I scroll the screen is gone. Got it. So, um, so I'm going to just annotate it. So this is still the law of total probability, because we are going from two variables, down to one. So, this is a lot of total probability, that's all good, because at this point we're kind of splitting hairs rigorously, it's not. Yeah. Okay, I see. But the intuition over of summing over all possible events that that is applying here."
"So the process is going to be motivated based off of material in 102. So we're going to do some filtering of signals, of neural signals, and then decode them, but it's going to be largely sent away. So if you're not taking this class concurrently with 102, that's also fine. Although, if you're not taking it concurrently with 102, ideally, you would have had 102 before. Otherwise, the project, the project would be very difficult. And sorry, just give me a second to see if I can change my view to be the video so that I can see everyone. Let me see if I can just make this to gallery view so that, looks like I'm inside gallery. Okay, and sorry, could we just ask the question there? Oh, Jerry. Okay. Thanks, Jerry. And then I see Bradley has a question, Bradley, could you please unmute yourself and ask?"
"So then we need to, this tells us that capital X of j omega is even. So then what we need to do is we need to look at that figure one. Look at, let me, I'm just gonna pull off this page so that I can look at it. Okay so I'm looking at figure one right now and what we want to determine is from figure one which of these Fourier transforms are even. All right so I'm gonna have you all look at figure one and then I'm gonna ask you all which of these Fourier transforms are even? Yeah, Sal."
"So this is correct. I'm going to use the short hand notation that we have from class that we could write that this here is a delta with a subscript 2 of t. And then this one, I can write as a delta. And so the subscript is still 2 because all of these blue ones are evenly spaced by 2, right? And that's what Eric had with this 2k minus 1. So we'll have a 2. And then now all of the delta's are shifted by, so if I just had a delta of 2t, the blue impulses would be on 0, 2, 4, etc. But they need to be at 1 plus tau. So it would be a 1 plus tau here, right? So if you write either Eric's representation or, or this simplified notation representation, those are both correct. And so this would give us that f of t is equal to delta 2t plus delta 2t minus 1 plus tau."
"So we apply the fourier transform to cosine one half comes out because of homogeneity. And, well, a bunch of things missing here. Okay. Then we have the fourier transform of. We have superposition, so that the fourier transform applies to each of these complex exponential. And then the twos cancel here to give pi of delta omega minus omega naught plus delta omega plus omega naught. And so just like the Fourier series of cosine, where two were values, where Fourier coefficients c1 and c-1 that happened at the frequency omega naught. The Fourier transform of cosine is going to be two delta functions that occur at the frequencies omega naught and minus omega naught. Any questions here? All right, so that's the Fourier transform of cosine. And then the Fourier transform of sign is exactly analogous. We will, it would be the exact same map is on the prior page."
"So we had this vector x k, we're k denotes time. And x k is going to be a 2d vector that contains the x velocity at time k and the y velocity at time k. And we're going to have x k, x k plus 1. We're going to have this for all time. And simultaneously to this, we're going to have neural data, which are spikes from 192 neurons. And we talked about how this data will become formatted as a spike raster. So again, next week, when we go through the project data together in class, we'll actually see this spike raster matrix where every single road corresponds to a neuron. So this is the spikes of neuron 1, the spikes of neuron 2, the spikes of neuron 3. And every column corresponds to a millisecond in the trial. So this is the first millisecond, the second millisecond, the third millisecond, etc. And what we do to count the neural data is recall we want to get a firing of rate. And so what we do is we count the number of spikes that happen in some window. So last lecture, we said, let's call it 50 milliseconds."
"So we have one last property then the phase. So, does anyone want to tell us how you did the phase. Can I ask a clarifying question on the phase property? Yep. On the handout, the property is written differently than what we had said in class. So I'm wondering which one we should use. Oh, the one in class. If there was a mistake on the handout, then I probably typed it wrong on the handout. So the one in classes the correct one and I apologize for the difference. Let me bring that up really quickly. Properties. Here we go. use, was it this minus sign over here, Melissa? I guess there may have been."
"So we recommend you can early start on that. And then last, we received some emails about read grade requests. And so if you have questions about or concerns, or you believe that a question that you submitted on the homework was misgraded. You should submit those read grade requests via grade scope. So just with the grade scope portal, you can submit pre grade requests. And this will wrap the request to the greater who graded your question to take another look. Any questions on any logistics? All right, so let's get back into material. So last lecture, we ended by showing this delayed reach task and a classification brain computer interface or neural communication prosthesis that we built from it. And so you recall the task was at the monkey touches and holds a center target. A target then appears in one of eight locations or seven, sorry, one of seven locations. In this case, the downward location. And then the monkey isn't instructed to reach yet. But what the monkey does during this period is he plans to make a reach towards the target. And then a go queue is delivered, which corresponds to this peripheral target becoming large."
"So we said, okay, what happens when we only allow one complex exponential frequency? What that means is that we're doing this sum here, except we're doing K equals minus one to K equals one. Right. And so what I would have here is a K equals zero term. Right. The zero term would just be a sink of zero is one and an e to the zero is one. So the C zero term would equal one half and that's this this blue line over here. So C zero equals one half is this blue line. And then I'm going to have a complex exponential with C one times an e to the J omega not T. And so these are complex exponentials with frequency omega not. And here those complex exponentials actually give me this red sinusoid. Right. And so we see if we add our blue line and our red sinusoid together, we approximate the square wave, but not perfectly."
"So we'll dive back into material. So in the second half of last lecture, we had started to cover our last topic in this class, which is the Laplace transform. And the motivation for the Laplace transform is that a lot of signals that we want to analyze may be growing over time and may be power signals. And we can't take the Fourier transform of a power signal. It's up for a few, which we did a generalized Fourier transform up. But in general, we can't take these Fourier transforms. And so while the Fourier transform reconstructed signals with bases e to the j omega t, for the Laplace transform, we get an additional sigma term e to the sigma plus j omega t that allows these cosines and signs to have a growing amplitude and therefore allow us to model power signals. All right. So the key thing is that there is also this e to the sigma part, which is a real exponential in time and models this growth."
"So we'll just do one condition. So one condition for this equal P of B would be that we know that P of B can be written as the sum over C of P of B comma C. Right? And this equals the sum over C of P of C times P of B given C. And so if this summation looked like this, then the sum would equal P of B. Now this summation has a P of C given A. And to say that P of C given A equals P of C means to say that A and C are independent. All right? But A is a parent of C. And so here A and C are not independent. And so in general, P of C given A does not simplify the P of C. And in general, therefore, this is not equal to P of B. All right. So we're going to do the conditional independence now."
"So we'll start off with a linear example. And so last lecture, we said that if we model the velocity as a linear function of the neural data, right? So in calculus, we had y equals mx plus b. Here it would be the velocity, kyw is equal to a times the y1k, which is the x axis value, plus some y intercept b. So b here is the y intercept and then a is the slope of this line. And what we want to do is we want to say in our model, if I model it to be this way, I get to choose a and b to make this fit as good as possible. So that when I later on observed neural data, I predict the y velocity well. And so we said, if I choose a and b to be, so if I choose b to be this value and a to be this slope, this red line is a pretty good. And so this is a very bad choice of a and b, whereas this green line here is a very bad choice of a and b. I shouldn't choose b thick and b big and a negative."
"So we're all done with the homework for this class. I have up here are back. So today we're going to finish covering recurrent neural networks, at least that's what a goal. And then the have made new slides on transformers. So it's like a little bit this quarter he would understand how checked GTT works. Alright, after transformers, we're going to cover variational autoencoders. And then after BAs will cover generative adversarial networks, Gans, and then finally we'll conclude with adversarial example. So we're gonna go through these architectures relatively quickly. But a philosophy moving forward is that even though they might be hard to understand the first time around, you already understand all the tools to know and even in cement all of these architectures. We know from prior lectures that for any deep learning model, what we have versus an architecture that is our connected network or CNN. We're going to talk about these orbits are all different architectures as well. We need to find the task difficult loss. We need a way to compute gradients with respect to that loss and that's backpropagation. Know we need a way to update the weights using these gradients. And that's the Catholic gradient descents with their favorite optimizer or momentum. Alright, so you understand those things for all of these architectures. The way that we're going to compute gradients is still with backpropagation. And the way that we're going to change the wage is still missing ingredient. You really understanding many of the ingredients for these different architectures. But then these architectures that will be applied in different settings like when the sequences are timed better. That's for RNNs and transformers. When we're doing unsupervised problems, like trying to find structure in data. We'll use VAEs and maybe we want to also generate data. We can use VAEs and gantry. Basically, what we're going to introduce with these different architectures is that each of these different topics, is there new architectures and how their needs for particular applications? All right, any questions on that? Okay. Friendly reminder that the project is the last part of this class and it's gonna be due Monday of finals week, which is March 20th. Please be referring to the colab notebooks back to Brandon and utilize prepared for you all that will help you to implement to recurrent neural networks even if you don't know how they worked out, but you still will get to you today. And then the TAs are also planning to go for some coding for the project for this week's discussion video. Last week, please submit any midterm regrade request has been tapped by the suspensory because we're not going to accept any exhibit. Any logistic questions. Alright, so revolt, get back into recurrent neural network for our last lecture on the criminal networks as a week ago. And refresh, remember that what we cared about and recurrent neural networks with this new setting, history matters. I care about how things evolved through time. So we said that the recurrent neural network would handle this by introducing a new variable called state. And that state has some value at time t. Alright? And what we can think of the state variable is a variable that contains all of the relevant information about your past historical inputs. Inputs are X1, X2, all the way up until time t, the state variable i sub t minus one would be a vector. That's a simply represents all the information I need from x one to x of t minus one to perform my task well, right, so it's a summary of the history. And then we're going to derive a recursion, or we're going to define a recursion for the state variables. So if I take, I take my state representing everything until I sub t minus one and I get my new input x of t. I could use that to update my state variable from S sub t minus one to S of T. And S of T contains all the relevant information about my templates from X1 to X2. And this will be implemented through a return neural networks. So this variable, concretely, of t minus one is going to be the hidden state h of t minus one of a recurrent neural network. Alright, any questions on the refresher, the motivation for RNNs? Okay, so then we ended last lecture going over how much the question is, will they not workbook output z of t and S of t? The answer is yes. So the network will, at every timestep compute S of T that the hidden state, which we're going to call this h of t, since that's what we've always called neural network hidden state. And then z of t will be the output of the network which the RNN will also. Okay. So we ended last lecture just going over some examples from this blog post by Andre capacity, the unreasonable effectiveness of RNNs. I wanted to just return to this example because this example is going to be super important to remember when you're talking about check GPT, Stanford generative pre-training transformer. And the pre-training part of captivity is exactly this task In the space of not characters but of words or tokens. In this task, what we wanna do is we want to generate text and generate texts by learning structure and the West Greenwich. So let's say that we just had four characters, which are H, E, L, 0, and we want to write hello, okay? So the inputs will be one-hot vectors, where if the first element is one, then h of the second element is going to be the third element. The fourth element. What happens is we inputting h into our network. Input into the network is going to update the state. And the state is going to be the hidden activations of this, sorry, he's gonna be the activations of this recurrent neural network. And then from the state, which here will be zero, we can put a softmax probability distribution over what the next characteristic, right? So currently is pulling up hello, then the softmax probability for each should be highest. Alright? You'll see that in this case it wasn't the softmax probability for 0 was the highest. We know that the target is E. And because we know that the correct target, and we know what an output, that would be a way for us to compute the cross entropy loss to update the parameters. Alright? So this is the same cross entropy loss that we would do for your homework. Let's say that he did have the highest public utility. What I would then do is I would sample from this distribution and hopefully pick up the next character is b. And I would take this character and I would pass it in as the second time step. Input into my R&R. And the RNN would hopefully output our correctly, that l would then go here. And I'll put another l that would come here. Now, I'm putting an output of one key thing to realize here is that In these two cases, the input character was exactly the same. Vector 0010, representing power. But the network was able to output something different in the first case, another L, and the other case 0. Because the hidden state was different. Because the state of the RNN was different. And so really, the state of the RNN again, succinctly captures my historical inputs, is what allows us to then generate texts because it's using information that is learned from historical characters. Any questions? Is there a reason? The question is, is there a reason why embark has three values? Just for ease of illustration, this will be the number of hidden units. Keep saying, Let me just say a number of artificial units in the RNN and it's usually on the order of hundreds of thousands. The question is, what will be the initial state? The initial state of the recurrent neural network can be learned through backpropagation. Any other questions? So all the blocks over here. Yeah, great. So tomboy saying all the blocks here, taking two inputs, which is the prior state and the current input. Is there something special that we have to mention here? Which is yes, there'll be an initial states with no input and that will be learned through almost I said that's not biasing the training process. Yes. So basically, the initial state can have quite a significant effect on the progression of the network. So you want this state to be judiciously chosen oral sex. Is there Any other questions? Okay, So then last lecture, we were showing these examples from Shakespeare. And so in this task, your goal is just to say, given my prior characters, predict the next character. So here if I stop Hello. Now if you say training on the works of Shakespeare, you train for many iterations. At first output shippers, but then learns to output things that aren't semantically meaningful. Like this is Jewish, but at least it follows capitalization, quotation basically grammar rules of the English language and capitalizes names. It puts a comma with an application. And again, it doesn't make sense, but it looks like. English language. And this is the final train numbered from this blog post where it learns the characters are fixed there and again, learns to output something that doesn't make sense semantically, but at least it looks like Shakespeare, right? You also talked about how you could find it too. I'll put Wikipedia articles and we'll learn how to do citations in Wikipedia. You can even give it low-tech code from an, from an algebraic geometry textbook. And it'll learn than to write LaTex code that looks like an algebraic geometry textbook. Were there any questions from these examples that we went through last lecture? Alright. So the recurrent neural network is able to learn long-term dependencies. What this means is it's able to learn things that occur over many timesteps, which is our goal with recurrent neural network. So if you look at one of the artificial neurons, blue to red corresponds to the value of the artificial neuron. And what we see is that whenever there's a quotation, this neuron has a low value. As soon as the quotation closes, the value turns white and red. And so you can see that this is essentially like a artificial neuron that learns that it stays silent until you close the quotation. So the base, the properties of how we use quotations. This is another example of when they use are an entity generate code. And so there was one neuron where they stand at the activation of this artificial neuron has to do with the amounts that you had indented. Alright, so this is a neuron that then learns to be sensitive to the depth of indentation. Your questions here. Alright. I'm going to show you a little preview of the PT network. Was it the general, which is the generative pretrain transformer? We're going to talk about that again at the end of this lecture slash next lecture. But things have come quite a bit since then. And so here's this really cool example. This is from 2018 with GPT-2, which is a precursor of TBT. And it's also trained to just generate instead of hex characters and generating next words. And so what they do is they seed this state of the network with some human texts that says in a shocking finding, scientists discovered a herd of unicorns to the theatre. Remote, previously unexplored valley in the Andes mountain. In the Andes Mountains, even more surprising to the researchers was the fact that the unicorns spoke perfect English. So something that is totally made up, never seen before. And here is GPT-2 completing that task within the same principles that we solve for x characters generation. And I'm not going to read through this right now, but I'll put this up during the five-minute break so you can read. But you'll see that they define a researcher, Dr. Dave Perez, University in the area. They named the unicorns. Unicorns. And then they even come up with a hypothesis for how these unicorns can speak English. And so I think they say the animals were believed to be a descendant, to be descendants of a lost race of people who lived there before the arrival of humans and those parts of South America. Alright, so actually it's generating something reasonable, but it has semantic meaning. I'll put that up during the five-minute break if you want to read a little bit further. Here's another example of a GPT-2, in this case, processing text about the 2008 Summer Olympics. And then being able to answer several questions about the passage. You read this and you wonder how you would do on the SATs. But that we're gonna get to the RNN, however texture, any questions on the examples? All right. I see. Tom boys question is, in this case, how does GET nodes and for bullet keep generating texts indefinitely. So when they train these things, there is both a start token as well as the end token. So it gets to this point and when it outputs again took investment. Other questions. Okay. Alright, so this is the vanilla recurrent neural network architecture. Vanilla. We've drawn it like this. This is how you should think of it, which is that there are a bunch of artificial neurons. And now instead of them before they are connected with the current connection. So there are loops within this network and their inputs that come into the network. Then been nephron kinda percolating on those inputs and an output variable z of t. So the equation of the, of the vanilla recurrent neural network is as follows. H of T is. It is the activity of these artificial neurons at time t. And it is equal to some activation function like ReLu. And they're going to be three matrices of interests. The first is W recurrent. And this is a matrix of connection weights that defines the values of these recurrent connections. And so this is going to be w recurrent times the hidden state at the last timestep, h of t minus one is also going to be an input matrix. And that tells me how my input is x of t map onto these artificial activities, HFT. So there's gonna be a plus wn times x of t. And then there's usually also a bias speeds. Any questions on that equation. And then after that there's a readout. So the rehab z of t is just going to be a function of my matrix W out and my artificial network activity. So z of t is going to equal, it's going to be a linear later, w times h of t plus some other buyers. So that is the equation or a vanilla recurrent neural network. Before we move on to then how we can train with this network. There are few things that I wanted to mention. The first is as soon as we can calculate outputs, we can start to define. So in the example that we showed before of generating the next character, right? You can think of these as, let me just pull it up. The Z of t is r like the softmax scores. So when you take softmax and z of t, it will generate a list item. This is the a t. You can think of. Z of t is the value of the output layer. And these are scores for each of the characters and it doesn't go through a softmax to turn it into a probability distribution. And then I could go into a cross-entropy loss. Alright? So one thing you'll notice is that there's a z of t and every single time step at time step 123.4. All right, so the way that we define the losses can be very analogous to what we've done so far. We'll take that z of t vector and turn it into softmax probabilities. In which case, you'd calculate the cross entropy loss at every single point in time. Any questions there? Alright, instead of cross-entropy loss, you could also imagine doing it, e.g. a squared loss, but we have overhears. Then there are also also had the liberty with an RNN to define exactly what time points matter to you. Alright? So even though we have a loss at every single time point t, we may not care about the loss and every single time point t. So let me give an example. Let's say that my goal is to input 50 characters into an RNN and then predict what the 51st character will be. I may only care about the accuracy of the 51st character and not the accuracies are the characters, one typically. So even though in this case I would have a loss for my first softmax at time step one, for my second soft much at time step two, all the way up until L 51, the cross-entropy loss for the 51st character I only in this context would care about Alpha-1. And so I can choose which losses at which time steps I care about backpropagating for. Any questions there. Makes sense to them. There's different hospital works. How do you fix the probability distribution and there can be any number. Yeah, great. Tom was asking a great question. So we've only shown right now next character prediction, which is really straightforward because there are only 26 characters. I've mentioned to you that checks you can t, isn't generating the next character, but it's generating the next word. And there are many more births than there are characters. So we're all happy on the stage to check digit is actually not generate the next word is generating the next quote, unquote token. Tokens can be worse, but more commonly the birds. So maybe if you've had the word minimise, this would be broken up into three tokens. Like there would be a token for men. There might be a token for n and then write your token for. Chaffey had, I believe, a vocabulary of 50,000 tokens on the order of 50,000 tokens. And by the way, the tokens include the individual characters a, b, c, d. Etc. So that through these 50,000 tokens, you can write any word in the English language, right? So I said before chat TBT is predicting the next word. Not totally accurate. It predicts the next token, but when you combine those two things being equal, birds and that vocabulary is about 50,000. So they're generating softmax probabilities over a 50,000 dimensional vector, right? Yeah, so the question is, during training, do I need to consider all of these bosses, L1, L2, all the way up to 51. So you can definitely compute them during training, but you don't have to use that. So what I'm saying is when you have a loss at the output, let's say l in green here is my total loss. I could have defined L to be L1 plus L2, or all the way up to 51. If I choose this to be my loss, then it will read my network to backpropagate so that all of these loss because our smallest possible. But I can also define in another setting that Alice just equal to L 51 and then back propagation but only change the weights. So that might fit the first correction is as good as possible. And it doesn't care about my first one to 50 predictions. Question is, can I give an example where that would be beneficial? Yeah, That's actually do use quite often for natural language processing. So even though we showed this setting where we're in this example, we do care about the loss at every single time step. Usually. We don't construct these in this way. We will construct them as you put an RNN, RNN, let's say 20 character inputs. And that allows the hidden state to update for 20 iterations. And then you only care about the classification at the last time step. And so that would be a problem of saying, I care about him putting 20th dark doesn't make it into 20 inverse correct distributions for the question is, when you do this, Do these intermediate character distributions of things before the 20th chapter ends up being correct? I'm actually not sure, but I would guess they're fairly accurate. Recall we should do exclusive, right? Exactly. Yes, It's homilies point here is related to Daniel's question, which is, even though we only care about the prediction of the 51st character, not the 50 before that. To get that 50, right, we would have had to do something reasonable. And that's why we would expect that the intermediate characters are also like. Alright. Okay, so that is the recurrent neural network loss using the loss functions that we've done before and character prediction Dolby cross-entropy loss. Alright, so let's take stock of what we know. We know the recurrent neural networks equations. And we can define a loss function. Alright, so immediately we know how they do it forward pass, right? We just compute this equation over and over again. And if we have a loss function, we know how to score how good our networks. We also know that when we have the gradients of the loss with respect to the parameters of the network. We know how to update those waves. And that's just using stochastic gradient descent, right? What's really then left for us to do is to answer this question of how do we compute the gradients of the loss with respect to weights. And in the case of fully connected networks and CNN is right for you that we did backpropagation. We've been at work. But this becomes a bit more challenging now because in the case of the recurrent neural network, there are loops. So the chain rule for derivatives that we just applied in each layer. In a feed-forward network becomes far more hairy when we consider a recurrent neural network. Alright? So how do we take care of this? What we do is we do the recurrent neural network, feed forward network in time. Okay, Let me unpack what I mean by that. So first off, I'm going to add one more thing to this drawing, which is that there's gonna be some initial state H zero, I guess into this network. Recurrent neural network in question was the following. H of t equals ReLu of w recurrent h of t minus one plus wn T. I'm going to drop off the biases. What this tells me is, if I start at time t equals one, then if I know the value of h zero and the X1, then I can calculate each one. So if I know the values of h to zero and x1, then through this equation I can get each one. And now I would know the value of X2. X1 and X2 would combine to give me a value H2. Alright, and I've drawn that right here. So we kept the cute H1 or H2. Know if h is zero and x one. To compute H2, I would just get to know each 1.2. Okay? So going along this axis here, we have time increasing. But now if I unroll this recurrent neural network equation in time, it becomes feed forward into time. And the number of layers of this network, however much time I'm using, my God. Any questions there? Yeah. So Tom Waits question is, can I say that one more time? How am I unraveling time? So what we're going to do is we're going to take this recurrence equation, right? Which in general will have groups because WE couples the various artificial units at time t minus one to time t, will draw that in the next slide. And I'll have another slide that says this more clearly. But if we look at this equation, we see that to compute the hidden state at time one, all I need to know is the initial hidden state h zero and my input X one. After I know each one, I can increment from t equals one to t equals two. So at time t equals two, I get a new input x2. And what this equation tells me is that I can compute H2 as long as I know what each one was. I got one already from each with their own X1 and X2. So from each one and x two, I can compute H2. Alright? So if I do this for every single time-step, then what I see is that when I write this equation, but I stand out time, right? This looks like a feed-forward network. There are no groups in this computation. Recall calculating the H1, H2, H3 is where all that I needed to compute the output of my network, because the output z of t is just a function of h of t. H of t. I can calculate z of t the output. And if I have z of t the output, then I can calculate my loss functions. I will unravel this further in the next slide. So let me just go to the next slide here. So let's take a concrete example where I'm going to have organelles. So here my hidden state h of t is going to be a 4D vector. And let's look at how this has recurrent by looking at the part of the equation ReLu of w recurrent times h of t minus one. If w recurrent or equal to this matrix. Then what this equation tells me is that I would add an h of t minus one. So I'm gonna write this as a 4D vector. Actually, let me, let me do the following. Let's, let's make this h of t plus one and make this thing h of t. So the four elements of this vector, I'm going to call h of t at index one, index two, index three, index four. Now what I know is if I go ahead and I take the ReLu, this W recurrent times h of t, That's going to give me h of t plus one. So this is going to give me h of t plus one. First element, h of t plus one, the second element. And so on. I'm going to call this vector h of t plus one. Way that we typically use is via a graph like the following. So if I look at how h of t plus one, the first unit. Is affected by all of the other units. We can see that h of t plus one in the first part official unit is gonna be one times h of t, the first unit. So there's a one here plus the value of the fourth unit at time t times 0.4. Okay? So these two values, I'm going to draw my graph via these lines. So the first unit has a connection to itself with value one. And the fourth unit affects the first unit with a weight of 0.4. So that's this connection right here. This thing here equals 0.4. Similarly, if I were to look at, Let's just look at the second unit, h of t plus one for the second unit is going to be 0.9 times the first unit. So there'll be a 0.9 over here. The third unit will have a weight of 0.6 to the second unit. The third unit here will affect the second unit with a weight of 0.6, and then the fourth unit will have a weight of 0.5. So that's this one right here. And if you do that for all of these and the networks, that's what this recurrent neural number. Any questions there? Okay? Now we're going to unroll this. And so what we're going to do is we're going to take this same network. And recall, we had this connection being a one and disconnection being 0.4. Alright? Now when I look at the values of my activations, so this will be in by h of t, which is a vector and R4, that's these four circles here of h of t minus one over here and h of t plus one over here. What we're saying is that h of t minus one affects the first unit with a weight of one. And therefore, this week here is a one. And then going from time t to t plus one is still affects it with a weight of warrants. So this is also equal to one. Then the fourth unit affects the first unit with a weight of zero point for this weight here is zero point for this weight here is also 0.4. Alright, I'm just going to do one more. I think on the prior slide we had that this value here was 0.9. And so now all of these connections, which tells me how h of one, sorry, the first unit affects your second unit. But each time step will be 0.9, right? So you can do that for all of these connections. And that takes my Recurrent Neural Network and unravel in time. Questions. Question is, is if I weight values are very small, isn't that really bad? Yes. So we'll talk about that in just a bit. What we'll find out that these numbers are really susceptible to vanishing and exploding gradients. Daniel's question raises another point, which is what you should have noticed also with a disability before neural network, where the number of layers is equal to the amount of time I run my number for. Every single weight connection is exactly the same. So you can think of this feedforward neural network, but all of the layers have the same exact way, which is defined by w recurrence. And now, like Daniel was saying, if W returned is small, that all of these wastes will be small and their gradients are going to vanish. And then if w wreck has any values that are both one, then after repeated application, your gradients are going to explode, vanishing and exploding gradients are really big challenge recurrent neural networks. It's tempting to train in between time steps. The first step, the question is, is w Rec, also a function of time? This W wreck ever going to be different than this w back. In return neural networks? Almost always. W Req will not change as a function of time. There have been some people who have thought about this, and there are some architectures where W direction change in time. An example is called the multiplicative recurrent neural network from Ilya Sutskever and Jasmine and James Martin's, where they do have a recurrent matrix that changes with time. But it's much more challenging problem. Other questions, Yes. Question is, when does the learning for w rank tracker? I will get to that on the next slide. So this is what the computational graph looks like for recurrent neural network. And what you see in this computational graph is that we start with an initial state and multiplies w rash, that we add a wn times X1. We add these things together and we applied our revenue, which is this function f. And I guess the H1. So this is just implementing that equation. Let me just write it down one more time. H of t equals. Instead of maybe I'm going to write f and it's going to be W times zero plus wn times, sorry, this should be h of t minus one times x, right? So this is just a computational graph for recurrent neural network. And then the hidden states are also used to compute the outputs Z1, Z2, Z3, etc. So the parameters up by recurrent neural network or this fall is there, w in W wreck and WWF? But in reality, the thing that's most challenging to learn is this w recurrent, right? Because this W recurrent is that one matrix that is applied at every single time step to update my current state. Okay? And this is the thing that is going to learn, those temporal dependencies over the history of your data. All right, any questions so far? Alright, so let's just consider a first setting where our loss is a cross-entropy loss. And we're doing next character prediction where we were, we care about the softmax probabilities and every single time. Alright, so here you would have that and l total l equals L1 plus L2, L3. In this case. This is a feed forward neural network. There are no loops. We know that when we have a feedforward neural network will have to do is back propagation the loss to the parameters to get my gradients. So if I were to start with my loss, I can backpropagate through this one path out three. That would give me a gradient with respect to z3. I can get a gradient with respect to h three. At this point. Let's say that I take this pathway to w, right? That would give me a gradient of the loss with respect to w recurrent. But I could have also at this point here, backpropagated through this pathway and then Gonstead W rack, I could have even gone one more time step ahead, and got to w. Similarly, I could have backpropagated through, Let's see if I have another. I could have backpropagated through L2 to get the gradient for w recurrence. I could have backpropagated through this capitalist to get ingredient. We know from our derivatives that you have multiple backup in the path-based object to a single parameter w. What happens is it a gradient is that they all add together. All right, So basically in this computational graph, if I want to compute DL, DW wreck, I'm going to have to be cognizant of every single pathway from the wasp to W recurrent, I'm going to have to backpropagate through every single pathway and then some other gradients. That'll give me a deal. Dw wretch. And then from there, I can just use adam or STD or my favorite optimizer to update the value of the current. Any questions here? Perfect. Yeah, The question is, are there also multiple Casper WN? The answer is yes. So Wn is gonna be the same at all of these wires. And so when we backpropagate, there'll be multiple passwords, W and skills. Great. Yeah, so this student was noticing that there is a growing, I think you say quadratic. I'm going to ask the TAs to just read about that to tell me what the complexity is. But as the number, as the amount of time steps grows, there will be a growing number of paths and that will, that will increase the complexity of the Talia is quadratic in time, but I'll perfect quadratic in time. They're trapped. Sorry, Another one at a time. The question is, when we talk about stochastic gradient descent, is that for one time or in other words, how do we use dashes here? So this DL DW rent is independent of time. So when we calculate DL DW rat will happen. You can think of it as like we're basically summing across every single possible path for the gradient to give us a single gradient of how changing w record a loss. But then because this is independent of time, then we can just use our favorite optimizer is just as GDA would be minus epsilon DL DW. Topic. Exploding gradient problem. And bring it back propagate again. We're getting great. Takeaway is asking a very deep question. Was I'm watching reserve for just the next slide. So let me first talk about what this backpropagation but looks like and then raise a problem. And then we're going to get it from right? So let's go ahead and just try to backpropagate through one of these w retry operations. So let's sa"
"So when you write tilde f of t equals f of t times delta sub tau t, or delta sub big f is f that is sampled in the frequency? Sampled in the time domain. Yeah, so f tilde here would correspond to the sequence of these impulses. Okay, thank you so much. Great. Thanks for that question. Any other questions here. And then I saw a question in chat, Jared asked with bandwidth we are saying that in time the signal contains no frequencies larger than plus minus be over saying that in the not in time but in the frequency domain omega. So the largest frequency for Omega would be to pi be. So basically it's the width, or it's half of the width of the signal spectrum."
"So you should not expect a midterm at the length of the 2017 exam. All right, and then again, see the CCLE announcement for any other details about the midterm. And I'm happy to take any questions about the midterm if people have them. Right, I see no hands raised. And so we'll, on Wednesday I'll be happy to again take any questions on the midterm if people have them as they come up."
"So you want to know what are a, f, and theta. And to answer these questions and to explore what these terms are, I have to write down a few time points. So let's say that this here, this is equal to 2. So this cosine is going to repeat every two seconds. Right. And then let's say that the amplitude here, let's say that this value is equal to 4. Right. So we know that if we don't have an a here of a is equal to 1 cosine omega T minus theta is a signal that has a peak of 1. And so when we see here, this cosine goes up to 4, that tells us that a is equal to 4. And so a is just an amplitude scaling factor is equal to 4 here. Right. That's here is again the frequency of the signal, which is 1 over big T zero. And so here, if big T zero is equal to 2, as it is, because the signal repeats every two seconds. No matter where I am, I go two seconds down in time and I'm going to be at this same point. Right. So F is going to be equal to 1 over the period, which is 1 half. So the frequency of this signal is is 0.5 hertz. Right. Okay. And then theta here, you'll see is a quantity that's going to look like it's a time scaling factor. Sorry, not a time scaling, a time shifting factor. And so if I subtract theta, right. Last lecture, we talked about how we subtract a number. What we're going to do is we're going to delay the signal. And so cosine, we know, starts at 1 goes to zero, goes to negative 1 et cetera. So this cosine is not shifted in any way. So in this case, theta is equal to zero."
"So, first, Tonmoy is going to be holding a review session this Sunday, December 13th, from 2 to 6pm, and I highly encourage you to attend that review session by Tonmoy. Tonmoy also increased his office hours during final exam week. And I made a mistake in the initial announcement."
"So, for, why can we ignore the effect of W? First off, for the, okay, I'm going to turn off my video. I think that, that, we'll hopefully help with the lag. So, um, I'm sorry, I need to do one more thing. Okay, can people hear me? I guess, very clearly, way better than with the video. Okay, great. Okay, let's go with this for now. So, uh, why can we ignore the W? Um, so first off, for the maximum likelihood solution, we didn't ignore the W at all. And so, uh, first off, that solution requires no assumption. So, ignoring the W was really a consequence of, um, the least squares, uh, intuition. And so, uh, for that intuition, the way that I would think about it is as follows. Um, will WK in the end cause a change in my regression?"
"So, for, why can we ignore the effect of W? First off, for the, okay, I'm going to turn off my video. I think that, that, we'll hopefully help with the lag. So, um, I'm sorry, I need to do one more thing. Okay, can people hear me? I guess, very clearly, way better than with the video. Okay, great. Okay, let's go with this for now. So, uh, why can we ignore the W? Um, so first off, for the maximum likelihood solution, we didn't ignore the W at all. And so, uh, first off, that solution requires no assumption. So, ignoring the W was really a consequence of, um, the least squares, uh, intuition. And so, uh, for that intuition, the way that I would think about it is as follows. Um, will WK in the end cause a change in my regression?"
"So, is that a normal distribution? Like, or is it just a shape like a normal distribution? Great question. So, uh, it turns out that when we compute this distribution, it will be a normal distribution. And that's, that's because of, um, this, um, this, um, that if we can, addition on a Gaussian random vector, it produces a Gaussian random vector. So, we haven't get proven why it's going to be Gaussian, but it ends up that it is going to be Gaussian. And we'll prove that next lecture. Yeah, gotcha."
"So, is that a normal distribution? Like, or is it just a shape like a normal distribution? Great question. So, uh, it turns out that when we compute this distribution, it will be a normal distribution. And that's, that's because of, um, this, um, this, um, that if we can, addition on a Gaussian random vector, it produces a Gaussian random vector. So, we haven't get proven why it's going to be Gaussian, but it ends up that it is going to be Gaussian. And we'll prove that next lecture. Yeah, gotcha."
"So, let's say that I had, um, data points that look like this, right? Then my least squares regression would give me a line that goes to these cut of points. Now, WK is going to be zero mean noise. And so, if I were to now take these points and perturb them by adding the noise to them, some of them would go up, some of them would go down. But on average, the effect of, of these perturbations WK to all the data points has a zero mean average. And so, um, if there isn't any, um, uh, for example, if WK had a mean of one, all of these points in general would shift up and that would change the regression line. But because WK has a mean of zero, when I average across many data points, that isn't going to change my overall new slope here."
"So, let's say that I had, um, data points that look like this, right? Then my least squares regression would give me a line that goes to these cut of points. Now, WK is going to be zero mean noise. And so, if I were to now take these points and perturb them by adding the noise to them, some of them would go up, some of them would go down. But on average, the effect of, of these perturbations WK to all the data points has a zero mean average. And so, um, if there isn't any, um, uh, for example, if WK had a mean of one, all of these points in general would shift up and that would change the regression line. But because WK has a mean of zero, when I average across many data points, that isn't going to change my overall new slope here."
"So, let's start with lecture six, where we were talking about these squares from several weeks ago, and then give you some time to also think about any project questions to talk about. So, I'm just curious as to how many people started the project. So, I'm going to put up an anonymous poll. Although, I'm not going to be able to see the responses. So, one of the students left it, one of you will have to write me in the chat. I'm just curious if you started the project and answer yes for yes and no for now. All right, and then can someone tell me what the percentages are? Hey, how can we tell that? Oh, you all can't see it either. I think like the ton, a ton, why and all the other TAs can know we can't because we're not host. Okay, that's, let me see if I can. Let me just answer yes and see for my iPad and see if. She, let me try to launch the poll for my iPad. So, I think that I can see the results on my iPad, but I cannot see it."
"So, thanks for this feedback. So it sounds like we're airing on the side of too fast for this class. I will take this into account when trying to explain this material. And again, feel free to stop me for questions. And if you have any suggestions for any particular feedback where we could help to get the pace right, feel free to let me know via email. All right. And thanks for the feedback again. Any questions on material or not material admin and announcements? All right. So we're going to continue where we left off last time. So at the end of last lecture, we were going over the intuition of the Fourier series and why we want it. And we talked about how the Fourier series, they have implications for LTI systems. But they also helped to extract different structure in the signal in particular."
"So, the concrete case is something like this right so TS more than SS minus one s minus two. So, you said is not because of law of probability, but in the class I interpreted it is because of the law of probability that we can write in term of the summation, like this. But, so yeah, any subtlety that I confused myself. Yeah, I'm not sure. I actually have in the past said that is a lot total probability but it's really just a subtlety and definition. The law of total probability takes a, a, and B, and sums over all cases of be to give just the event A happening. In this case, we have two events A and B. We aren't trying to just get the probability of one of the events, which would be the law of total probability, but we are doing something law of total probability ask, which is that we are essentially writing out all of the cases of Tn less than or equal to S out, which gives us a sum."
"So, thinking about property to from a biological standpoint, I'm confused how this makes sense because I can't mirror on spikes be affected by some biological processes, how much, how many new trends you have to continue firing after IIDs or listening to a song. If you listen to a chord, if you feel like the first note of a chord, you're expected to hear the second or stuff like memories. If you've experienced something to pass, can't you mirror on fire differently, things like that. So, I guess, come up biological point of view, how the restart property makes sense. That's a great question, Andrew. Yes, so, the restart property in general won't be true for a biological system. The restart property, which is think of as just true for this attraction, which is the Poisson process. And even though it isn't something that will be absolutely true for the biological system, still the approximate, just like, for example, that are inter spike intervals are not exponential, it's one of the modeling things that is a consequence of defining these exponential inter spike intervals that allow us to do math looking at any inter bobble Poisson process."
"So, yeah, that's right. Yeah. Okay. And then For 3D, the one where we're modeling the neurons by 50 spikes per second. I just sort of did a calculated the area under the curve for the refractory period, using exponential I just wanted to make sure that that was sort of the correct intuition. that the random variable is less than 0.001. And then I'm just a little confused. I think I'm still a little confused about the concept of combining Poisson processes for part five. So when we're saying the probability that no neurons are detected on either electrode, Would I set that up as like the probability that that n, n one, and the probability that n two equals zero, like it would be a multiplication between the two Poisson processes."
Sometimes people are interested in modeling the reverse equation which is yk equals c times xk and this equation is saying how can you construct your neural data from the kinematics alright and if you have this single equation and I now ask you to build a decoder to say given that I have neural data how can I infer kinematics this is a question that you should all be able to do because we know what the least square solution is for c right and it would be that c is equal to y times x transpose xx transpose inverse that's the least square solution that we derive last lecture and then if I wanted to decode x from y then I could just compute the pseudo inverse of c and therefore xk would equal c pseudo inverse times yk alright and so since this equation is essentially the inverse of the optimal linear estimator sometimes or not sometimes this decoder is referred to as the inverse optimal linear estimator or the inverse OLE alright and the inverse OLE is this is the exact decoder that was used by the Pittsburgh clinical trial featured on the 60 minutes video and so I said that they use an optimal linear estimator to be more detailed they use the inverse OLE so they find a c matrix that relates the kinematics to the neural data and then they decode using the pseudo inverse of that c matrix right any questions there alright so that's how we would decode neural data if this was the only equation I have but now what I'm going to say is that we have another equation and that equation relates how my kinematics evolved through time in principle the way so I'm going to have an equation like xk equals a times xk minus 1 alright and this a matrix then captures the natural evolution or some properties of the evolution of my kinematics through time alright and this is something that you all have like we've seen before so if you think back to your physics class where you learned Newton's laws of motion right we can write them as an equation xk plus 1 equals a times xk relating again the kinematics xk to the kinematics of the future time xk plus 1 alright and so let's imagine that we have additional information in this case let's say that we are modeling a ball falling on earth right and so if we model a ball following on earth falling on earth what I'm going to do is I'm going to put the x and y position of the ball and the x and y velocity of the ball into this vector and this xk plus 1 equals a xk will then summarize my laws of motion so the first equation is going to say p xk plus 1 let me write it over here the x position at time k plus 1 is going to equal one time the x position at time k right and then plus delta t time the x velocity at time k that's just my integrated velocity so plus v at time k delta t alright this ball is falling on earth and so if we rack a y velocity right we're going to have acceleration due to gravity and we know from physics that that y position should follow this equation it's your initial y position plus your integrated velocity plus 1 half a t squared right so 1 half time the acceleration due to gravity which is minus 9.81 meter per second squared times a delta t squared alright so that's just one of those equations from physics when you solve kinematic problems and so we have that equation right here the y position at time k plus 1 is the y position at time k plus the integrated y velocity and then minus 1 half a delta t squared okay and then similarly we have then an equation that relates the velocities and so if you're in a vacuum then your x velocity is just going to be the same but your y velocity is going to increase according to the acceleration due to gravity okay and so this is a long-winded way of saying I can write Newton's equations succinctly in this matrix vector multiply that tells me how the positions and velocities of my ball evolve from time k to time k plus 1 right and they're all summarized by this matrix okay any questions there all right so what I want you to notice is that in addition to this equation that gives me a relationship between neural data and kinematics xk remember xk is what I want to decode right I now have one more equation I have an equation that tells me how the kinematics at a prior time step inform the kinematics at the next time step so now I have two equations giving me information about the kinematics and so why might this matter for this I'm going to go into my slides and and show a few videos so we're going to consider this problem of the ball falling except now it's going to be a cannon ball and we're going to be shooting a cannon ball into the sky so believe it like forward so here I'm showing a video of you can imagine a cannon ball being fired off into the sky right and we know that it follows a parabolic trajectory right and we know that if you know the initial position and the initial velocity because we know the dynamics and Newton's laws of equation sorry Newton's laws of motion we can solve for the position of the ball of the cannon ball at any single point in time so if I give you the initial position and the initial velocity and I ask you what is the position of the cannon ball at a particular point in time you can solve that for me with Newton's equations of motion right any questions there all right so then let's say that you were an alien who came to visit earth and you didn't know Newton's laws you didn't know what our gravitational constant was all you could know you don't know anything about the physics of earth all right and so if you're an alien that's come to visit earth and I ask you as an alien to track the um position of the ball as it flies through the sky you can still do it because you actually get to observe the ball flying through the sky and so even though you don't know Newton's laws of motion what you can do is whenever you see the ball you can plop down an orange dot and um if you do this for every single frame in time then you're going to know the trajectory of the ball all right so there are two ways to get the trajectory of the ball one you know physics and so you can calculate it uh two you may not know physics but you can wash the ball flying through the sky and at every single point in time you can just say okay the ball is here the ball is here the ball is there etc okay now let's say you're still in the alien shoes um but now we ask you to track the location of the ball in this sky except now you as the alien can only view the ball through a noisy camera okay and so you can only view the ball through this camera and maybe you're watching a post-tot video the camera has low resolution there and so here that low resolution is from blurring this pixels by analogy and uh further the video is noisy so it turns out that uh when the light is dim and it's at a dusk there are a few photons that are entering the the camera and the photon arrival times are actually follow a Poisson process not unlike the the spike times all right and so in the analogy um the observation of this noisy low resolution video is analogous to how if we were to try to decode a cursor position from neural data that neural data is low resolution because I only get to measure a hundred neurons out of a hundred million and the neural data is also noisy because the spikes um uh have various zochastic processes and we can model that with a Poisson process right so now if I ask the alien to try to trace this ball in the sky um he can watch the video again and you can see there's a ball going there is kind of hard to make out right at the ball is almost fallen okay and so now if the alien goes from this noisy video and says okay this is where the ball was at every single point in time he gets out this blue trajectory right now this blue trajectory generally gets the position of the ball across time but there are some aspects of this trajectory that you know uh just are impossible like when the ball here is reported to fall down and then float back up right and we know that that can't happen the cannonball in space uh in the sky can't just uh fall and then float back up due to gravity right so even though the alien did a pretty good job there are some of these sub-optimalities in the observed trajectory that we can definitely improve upon because we know the laws of physics and so the idea of the common filter is to say my observations might be for example low resolution or noisy meaning that if I try to track this cannonball with just my observations only I'm going to get a trajectory that guessed a lot of the features correct but we get some features wrong okay and that's because my observations are noisy and low resolution just like when we decode from the neural data right we will get a lot of the features correct like in the optimal linear estimator but some things will be at a lower level uh incorrect like the jitteryness of the cursor however uh if we then take the trajectory and incorporate our prior knowledge that the trajectory um that the cannonball has to obey Newtonian mechanics then what we can do is say well when I saw the cannonball fall down and then float back up because my video was noisy I know that can't happen and so maybe I can denoise the trajectory to return something in orange and so this orange trajectory incorporates both my noisy observations as well as my knowledge of Newtonian mechanics right so what I'm doing is I'm taking both sources of information the low rise video camera as well as the laws of physics to get a better estimation of the actual flight of the cannonball shown there in orange okay any questions on that analogy Jonathan I have a question yeah come on yeah so for example you if you take a like the winner filter right and you get some decoding right so if you denoise that decoding results will you get something comparable to Kalman filter um it turns out and this is um something that I won't answer completely here uh it turns out that the Kalman filter as we will implement it is actually a specific case of a winner filter but uh tansher your question some way yes under some uh constraints on xk uh the winner filter uh will approximate a Kalman filter correct thank you come way all right and then let me also just show the video that I skipped before showing the performance of um our state of the art Kalman filter so one thing you can note about this video is that the cursor movements were very smooth there is no jitteriness and that smoothness is conferred by the Kalman filter modeling all right so if no questions then let me get on to the equations so what we are now saying is that instead of having just an observation equation an observation that's noisy and low resolution like the video camera in this case our low resolution noisy observation is neural data instead of just taking our noisy low-res neural data and using that to infer kinematics I'm also now going to use information about how the kinematics evolve through time in principled ways to further improve my estimation of what the decoded kinematics are so instead of having just one equation here the inverse ole I'm now going to have two equations and I'm going to use both of those equations to solve for xk all right and so these two equations together are called a linear dynamical system and there are entire classes taught on the analyses of linear dynamical systems and I highly recommend taking a class like that it both solidifies your linear algebra and then also gives you tools that help you to analyze time series data so this first equation xk plus 1 equals a xk this is typically recalled a state process in a dynamical system xk which are for us the kinematics are typically called the state and yk in a general dynamical system are usually called the observations it's what you get to observe in an experiment in this case is our neural data okay and so and this equation is therefore called the observation equation or observation process okay so our dynamical system is comprised of a state equation or state process and then observation equation or observation process this matrix a is typically called the dynamics matrix and it models how your state evolves through time which in our previous example could be things like the laws of physics or in the BMI example is going to be the inertia of the movement alright okay so just like I said before now we have an additional equation that gives us more information onto what our decoded kinematics should be all right so concretely instead of just decoding xk equals c inverse yk the optimal linear estimator I should now have more information from this equation my state process equation to better update and refine my estimate of the decoded kinematics xk okay any questions on that intuition all right I'm just going to throw out the are you following question just to make sure people are following since I haven't seen any questions yet in TAs if you can just tell me what the percent to gs is okay great looks like most are following all right any questions here all right so here's a thought that you might have if I have an equation like yk equals c times xk well what happens if or let that actually let's take this equation xk plus 1 equals axk and let's say I'm modeling a ball following right if I'm modeling a ball falling and it's falling in a vacuum on earth right this equation is almost perfect right and so if this equation is almost perfect isn't all I need to solve for the position and the velocity of the ball at any single time point and the answer is yes if one of these equations is perfect you don't need the other but the thing is that in most cases these equations are not perfect both of them are providing a noisy estimate of what xk is and if they're both noisy then we're going to have to find a way to incorporate them based off of their noise to best estimate xk and we're going to see in the common filter solution if you assume that one of these equations is perfect the common filter does tell you to just use one of the equations right so how do we get the optimal estimate of the decoded kinematics from these two equations neither of these equations are perfect because if one of them were then we should just use that one equation to solve for the kinematics in real life noise is going to corrupt these equations and so for the BMI case the reason that this equation isn't perfect is because the only thing that we're going to be modeling in this class is the inertia of the velocities so we're going to say in general our movements are smooth but it doesn't tell me what my velocity at time k plus one is given the velocity at time k all right so in real life neither of these equations are perfect they have noise and so the noise on the state update process is going to be called WK and the noise on the observation process is going to be called QK right and we bottled these noise as random additive noise and they're also going to be Gaussian noise that corrupts these equations okay any questions there all right so for the common filter we're going to make several assumptions when we get to the details of the math we're going to assume that the noises are Gaussian noise terms and so if xk were velocities right so if xk was my x and y velocity at time k right then wk would be a two-dimensional noise source with covariance with mean zero and covariance big w and then QK is a noise source on the same dimensionality as yk my neural data and so if I have 96 neurons this QK would be 96-dimensional so if the covariance matrix w was equal to zero for example right then all my wk would be zero tiny that this equation is perfect but as long as w and q are not zero then these equations have noise in them and again then we need to know how to estimate how to use both of these equations to estimate xk given my neural data and my historical kinematic data any questions there all right so then you might ask how do we get the optimal estimate of x hat of k from these two equations so in general this is an extremely complicated problem without nice closed form solutions but under certain assumptions which are linearity and Gaussian noise so under the assumption that the state update process is a linear equation which is which is which is this here is a times xk and the observation process is linear too as well as Gaussian noise at wk and qk are Gaussian then there's a recursive solution to estimate xk and that's called the Coleman filter and again at least for these starting slides we're going to just start with the high level intuition the take-home point and intuition behind the Coleman filter is that my decode equation of x hat k now instead of just being a mapping from the neural data so if I if I didn't have this m1 x hat k minus 1 x hat k would be equal to m2y k which looks like an optimal linear estimator but now that I have this dynamics equation the state update process I'm going to also have another term m1 which is another matrix times my kinematics at the prior time step all right so instead of only using neural data to decode kinematics which is what we did for both the optimal linear estimator and the wiener filter we now also use the previously decoded kinematics xk minus 1 so this term here corresponds to this term here since the previously decoded kinematics also has information about what the next kinematics should be through this dynamics equation all right and so that leads to a decoding equation of this form all right any questions there and then Brandon asked and checked and I repeat what m1 and m2 are so m1 and m2 are just going to be matrices that map the prior kinematics and the neural data into my currently decoding kinematics we haven't arrived with m1 and m2 are yet in fact m1 and m2 are going to take on pretty complicated expressions that need to be recursively calculated but and and that recursion is is called is the common filter algorithm but at least at a high level m1 will map by prior kinematics to my current kinematics influenced by this equation and then m2 will map my neural data to my kinematics so basically m1 and m2 are summarizing how to combine these two equations to get my current kinematics from past kinematics and neural data all right any questions there okay so the next part of this lecture is we're going to tell you essentially the approach of how we solve for the common filter at a high level and then we're going to write out the common filter solution so that you could code it up in your homework if you wanted and we're also going to check that the common filter solution makes intuitive sense and so this is all in terms of gaining intuition of the common filter and then after that we'll dive into the details right so the common filter will give us a recursive method to calculate m1 and m2 and m1 and m2 these two matrices here are going to be some combination of a c big w the covariance of this noise term and big q the covariance of the q k noise term so they're going to be a function of these terms and the common filter is going to tell us exactly those equations right and so the goal of the common filter is going to be to estimate the distribution p of your current kinematics xk given all of my prior neural data y1 to yk so the picture that we should have in mind is the following and I just saw a question in chat it says why are we using the covariance of the noises so the code on any given time point time k w k and q k are going to take on particular sample values from this Gaussian distribution and that's going to vary from time point to time point but knowing that these noise sources we're going to see our independence and identically distributed and so on any given time point they may take on a they'll take on a particular value to get the best estimate of xk given that w k and q k take on random values from these from these particular distributions right the way that we characterize them is by the distribution parameters which are the mean and the covariance is and so this means that we don't observe the noise at every single time points update the common filter we just know the noise is statistics and using the noise is statistics will know what's an optimal way to combine them is and a question from shall ran yeah professor just wondering for these m1 and m2 are they the same across all time points?
"Sorry, Rhett. I'm just going to write the answer that I know exactly. Can you say that sum again? The sum over D of P of D given P of B. Like sorry, P of D times P of B given A D. And I know that when we have two variables, we can say like the sum of P of C times P of A given C is like equal to P of A. Absolutely. If we could analogously do that for three variables and say that like P of D times P of B given A D summed over D is equal to P of B given A. I see. So in general, the expression for P of B given A written as a sum over D would be a sum over D of P of B comma D given A. And so when we do the summation for the law of total probability, we can think of that as taking out a variable that is in front of the conditioning sign."
"Sorry, sorry, I have a question. So when we see that yk given xk through xk minus 1 up to y1, right? So is equal to probability of yk given xk. So is xk capturing all the information that has happened in the past? Yes, that's correct. Thanks, Tomway. So xk is capturing all the information in the past about x1, x2, y1, and y2. So said differently, the reason that these are this conditional independence implies that all the information from x1, y1, x2, y2 is stored in x3. And therefore, if I know x3, knowing any of these prior, knowing any of these variables that were passed in time, doesn't give me any more additional information about y3. So all of the information from these variables is summarized systemically in x3. Right? And then a question to chat, what do xk and yk represent? Here xk are going to be our kinematics. So that's going to be the velocity of a computer cursor. And the yk is going to be our neural data. I remember then this equation tells me how, or I can model things like the inertia of the cursor that the velocity at time k minus 1 has some information about the velocity at time k. Because, for example, when I move to the right, if I'm moving to the right at a high speed, my velocity at the 200 millisecond pointing to the right implies that at the 225 millisecond, I should still be moving to the right. So this is the thing that captures inertia. And more generally, to capture any laws of physics or temporal structure in the data. And then this equation here represents how the neural data relates the kinematics and the neural data relate to each other."
"Thank you, Shishank. That's a really great point. I've drawn this wrong. So Shishank is right. D is this distance here. This is not inside and outside. The inside is just inside the cylinder. Thank you for that correction, Shishank. All right. Any other questions here? All right. So now we're going to head on to just the last topic, which is throughout these early lectures, like Kat saying, that we're going to see the action potential at different 10,000 foot view, or 1,000 foot view, et cetera. This is going to be our most in-depth view at the action potential now, and we're going to talk about the key dynamics that be to the action potential shape."
"Thank you. All right, everyone, we're going to get started for today. So, our first announcement is that homework number seven is due this Friday, uploaded to grade scope. And as we've announced for this homework questions four or five and six which are on the plus transform and inverse Laplace transform are graded based on effort. And any attempt that you make on the question will give full credit to you. In that sense, we want you to think of these questions more as practice questions for the final sensible plus transform. And this inversion is fair game on the final. And then earlier today, we sent out an announcement on CCLE and Piazza on the final exam logistics."
"Thank you. All right, everyone. We're going to get started for today. A few announcements. First a reminder homework number five is due this Friday, uploaded to grade scope by midnight. If you didn't hear the end of last lecture, or look at the cover page of the midterm exam, we announced that the final exam score will replace your midterm score if you score higher on the final exam. And then as per our syllabus, I wanna remind everyone, we don't have lecture a week from today on November 25th, 2020, the day before Thanksgiving."
"Thank you. Thank you all. Okay. Any other questions on logistics for the class. All right. So, getting back into material. Last lecture we were talking about frequency response, which was to say, prior we've talked about how LTI systems can be written as convolution between input and impulse response. And this is due to the Fourier transform of the impulse response capital H which is called the frequency response times the Fourier transform of the input x. And therefore we were able to see that the magnitude of the frequency response and the phase will be shifted, the phase of the output will be shifted by the phase of the frequency response. So last time we did one example, we'll do another example to start off this lecture on frequency response. And so last lecture we considered a system that gets an input to cosine t plus three cosine three t plus two plus cosine of two t, and the system has an impulse response, which is given by the sinc squared."
"Thanks, Chumlin. Let me give you a. So what you mean is like you you want us to put numbers on each question, right? And you're muted by the way. Sorry. So I ended up using because I forgot to set the mute all to and so thank you Ryan for raising your hand for the question as to what you need to assign. So on grade scope, we have said there's a question one A, one B, one C, etc. And so what you need to do is you need to indicate where on your submitted PDF or your submission to grade scope, which question which work refers to one A, one B, etc. Okay, thank you. All right. Any other questions? All right. So we're going to continue where we left off last time, which is we started to talk about signal models, which are the signals that we will essentially are basic signals that we will use to construct other signals in this class."
"That is just the most astounding thing I've ever seen. Can we shake hands? No, really. Yeah. Like, come right over here. Yes, you can. Okay. Oh, my goodness. Wow. And I can do a fist bump that you like. It's amazing. What are you doing, Jan? What's going on in your mind as you're moving this arm around? What are you thinking? Okay, the best way to lay is raise your arm. Uh-huh."
"That is, the last boss or the last spike came at T3, and I started waiting for my spike, my next spike at time S, or my next plus at time S. The amount of time I have to wait for in the future, the distribution of that is going to be the same as if I had not waited at all. And therefore, this inter-spike interval ends up being exponential, and if this is exponential and everything else is exponential then the line, then looking forward we have the Poisson process. All right. Any questions on the intuition there? All right. So let me just go through this slide at a high level. So at least you can read this slide, and if you're interested in understanding this proof rigorously, then this slide will give you that information. So at the top here, we just have this fact that we've been using last lecture that for an exponentially distributed random variable, the probability that big T is less than little T is each the minus lambda T."
"that look like the Poisson distribution with 1 minus p times landom times s? Yep, that's right. Okay. Yeah, so at that point it's going to be, it's going to be a bunch of algebra. I can give you a little hint which will hopefully simplify things. So, oh and sorry, this should be from n equals m to infinity because we Because we know that we know that admin is going to have fewer spikes than n. you should be able to simplify to a term where you have a sum from n equals m to infinity. And you're going to have like a p to the n minus m times a lambda s to the minus lambda st, divided by n minus m factorial. So you're going to get this expression when you do this simplification. And the key thing to realize is when you get to this expression, if I go ahead and I do a substitution, k equals, this is really bad, k equals n minus m, right? Then that changes the sum from being k equals 0 to infinity, and all these n minus m is changed to k."
"That's correct, yeah. So let me pull up this slide right now really quickly too, so that I can just make sure that we're all looking at the same thing. So I'm going to bring up this slide here, and then I'm going to share my screen. All right, so I think Grace, you're talking about this slide, right? Yes. Yeah, so, and your question was, is the reason that we have the negative 65 millivolts due to the introduction of sodium into the mix right."
"That's correct. Yeah. Think of the ellipsoid that's. Remember when we drew like, let's say that, let's say we had a 1d normal distribution where data points were well congregated here and then like maybe they're more sparse out here. If we want to model this with a normal distribution we might say that the one standard deviation point is between these two values because 67% of my data points fall in this range. But data points can exist outside of the standard deviation lines. They're just less probable. But the ellipsoids themselves don't contain the point. They just describe where most of the data points should be. So even if my data looks like maybe in each axis is a uniform distribution, then I can still have that ellipsoid."
"That's going to be the major component of this class. The first three weeks of EC-102 are not application-heavy. I mentioned in EC-102 today, that class, the first three weeks are just a lot of math and foundation-building. We're going to spend the first three weeks of this seminar just introducing the background of what brain-machine interfaces are. And then after that, we'll talk about the kind of tools that we'll want to try to solve the problem of decoding neural signals into movements. And then we'll finish off this seminar by just thinking of other problems and how we might solve them with tools we have. And so again, an overall goal of this seminar is to take a question that is probably new to all of you, think about how to approach that question, and then how to break it down so that we can tackle it with the tools that we have. Any questions in terms of course goals or how the ECE189 is structured? I had a question about what you mean by movements."
"That's how the voltage of the cell goes up for this action potential. Question from Ammona. Yeah. Hi. I wanted to ask, sorry, about the previous slide is one more responsible than the other in terms of drift current or diffusion current for the Na plus to be moving into the cell? Yeah, that's a great question. So in this case, because they are both flowing in the same direction, they're both responsible. It's actually possible to compute the current that is going to arise from, or is possible to compute the voltage at which the concentration, the diffusion current and the electric field driving force cancel out. And so they both contribute in this case because they're both pushing in, we know the answer. But the buyer to give you a separate question where I said, let's say I open up now in green here, a K plus ion channel. So in this case, for a K plus ion channel, that's through potassium, we can see K plus will want to be driven in by the electric field."
"That's okay. Or yeah, and the dopamine energy can neurons specifically in that region. Great. Perfect. Thank you for that. Great. All right. I'm not quite sure about like the page 14 of lectures, which is the. I think that I own see. Yeah, I could hold the slide up really quickly. I know exactly what site you're talking about, but just let me. Slide 14 lecture three. He's loading from you right now. Why don't you ask your question while I bring it up? Yeah, so I think you mentioned like the main driver of like how it works is the size, right? It's like the size in a pile."
"That's what we have in the solution. Let me just read the question really quickly to make sure that we stated that they're independent so that you can do that. Yep. So, in question five it says each neuron spikes independently according to a homogeneous Poisson process. So when you. And a they say what's the probability that no neurons are detected on either electrode. And so that would be a probability that parentheses. And one of t equals zero, and then because they're independent then the and can reduce to just multiplying those two together. Okay, okay, and I was a little confused, so when we set up the Poisson distribution, right? Yeah. So the S represents the number of spikes, that's the unit? No, N represents the number of spikes, and then S represents time, but lambda times S gives you a mean number of spikes."
"The channels are to K plus and plus. And so because at rest, K plus ion channels. There are many more of them at rest and plus. than any plus the weight of K plus is going to be much higher and so that's why it ends up being closer to minus 65, then to like say, minus 35 or plus 25. Okay, thank you. And, and really quickly, we'll talk about this, the equation that tells you this would be Goldman's equation. And, and, as I mentioned briefly in lecture, the PK is going to be much higher than PNA at rest and so that's why the K plus terms will dominate what the overall voltages and it ends up coming out to negative 65. Okay. Thank you. And then one more question was the increase like that, like that little red line that increases between B and C. I'm still not understanding why that increase happens. So is it because we have less positive ions outside because of the introduction of the NA plus ion channel. That's correct. Yes. So I'm going to put a negative charge in at minus 75. When I introduced this, and a plus channel and a plus float into the cell so now there's less positive charge outside the cell and more positive charge inside the cell, meaning that the voltage is going to go up. Because I mentioned the voltage from inside to outside the cell and I put more positive charge inside the cell."
The common filter would allow you to incorporate those dynamics into the estimation. And in this lecture what we're going to do is we're going to talk about the common filter first at a level of intuition since the math gets pretty involved and it's easy to get lost in the read. So we're going to talk about the common filter at a high level intuition and then after that we'll dive into the more math details. All right. So we're still in the context of the motor prosthesis where our goal is to be code the continuous movements of a computer cursor on a screen and you could generalize this to a 3D robotic arm if you're controlling the endpoint of that robotic arm. Okay. So really the common filter has been the workhorse for motor neural prostheses in the last decade and the best state of the art decoders today include a common filter. That said the common filter is of course not specific to this area of brainwashing interfaces and so one of the rewarding things for me in teaching these classes is to see students come back and tell me about how they used some of the techniques they learned in research and for this class what I hear most is how they were able to use the common filter in a particular application.
"The extra on here is sink omega T over two pie. But there is also there is also a phase component to the signal, but where it went, think of omega T over two pie is there a then the overall H of J omega will be Sarah. So for the purposes of this thought, I'm just going to leave it up since the intuition still holds, which is at these points, the sink is around. Thank you. So, um, if everyone's following that intuition, then we'll go ahead and write out the math for this. So, um, we know that if X of T is periodic. So what do we know we can write X of T as people can put in the chat also if you don't want to unmute and raise your hand. I know there's a lower activation energy to write something in the chat. How should I write X of T it's periodic. Four A series. Great. Yeah. So X of T can be written as a four A series. And so, uh, this is C K E to the J K omega not T where omega not is equal to two pie over big T and big T is the period of the signal. And so, um, we derived in class that this has four A transform X of J omega equals two pie times some from K equals minus infinity to infinity of C K and then multiplied by delta. So this is a delta omega and then the impulses occur where real may they're not great. So this would be K omega not. And so, um, then we want to know, um, let me copy and paste this onto the next slide. Um, we want that Y of J omega is equal to a constant times delta T, sorry, delta of omega. And Y of J omega is going to be equal to, um, each of J omega climbs X of J omega. But now X of J omega is going to be, um, this sum of impulses that we wrote. And then it's going to be a two pie, um, and then a sum from K equals minus infinity to infinity. And then we're going to have, um, a C K. And our delta function is going to sample the value of H at K omega not. So it's going to be H of J K omega not times a delta omega minus K omega not. So then, uh, what we need to do then is determine what, uh, what the period or, uh, when does this, when does this equal, uh, C times delta omega."
"The frequency structure, that is, when I see a signal that looks like this in the time domain on the left. And what kind of sinusoid at what frequencies are they composed of? And if I can see the structure, it can help to give me more intuition or more insight sometimes over what this signal is. And so we've given this example a few times that this is a C major chord in music. And while it looks very complex in the time domain and the frequency domain, it looks very simple and elegant. And so there could be additional information, additional insight, I mean, not information, additional insight from looking at a signal in the frequency domain. All right. So we also give the bottom line last lecture, which I'm just repeating here. We're going to derive all of these results today. And the bottom line is the following. If I have a signal FFT and it's well behaved, we talked about how last time the lecture by this we mean that it's. It doesn't have discontinuities."
"The NA plus channels which are high energy and would cause both NA plus and K plus to shed the waters of hydration. But then K plus is filtered mostly to the size, but then that filtering is not as robust. I see. So you're saying like this property is like way merged from the. Sorry, what were to use there? This is this property is what? This is an emerging property from like the. Energy like by. And the size of the. Iron channel. And the property being that the energy binding is more is better filter. The permeability is like like an observed property of like the energy and size at work. Yes, yes. That energy and the size filtering lead to these select permeabilities. I see. Okay, that makes sense."
"The question is why is W and Q symmetric? It's because they are covariance matrices. So W and Q are, where's my apple pencil? Here, W and Q are covariance matrices. And so the ijth element of W is going to be the covariance between W i and W. Let me do superscript, so it's clear there. And this is the ith element of W and the jth element of W. And this is covariance itself is a, it's a symmetric quantity. So covariance of W i w j is equal to covariance w j w i. And so for that reason then, this equals w j i. And so, I'm sorry. Can you all hear me? I'm not sure if my internet connection has frozen. Hey, oh, I'm sorry about that. Can you all hear me?"
"The second time bin. All the way up to the big K time bin. All right. And in this case, big K in the data set that we gave you is going to be equal to 16 465. And so we have 16,465 X and Y velocities. And then we also have the same thing for neural data. So for neural data, we had 192 neurons. So we have Y1, Y2 all the way to Y192. And then we have this just constant row of 1s that we talked about at the bottom. And we have this at time 1, at time 2. All the way up to time big K. So there's a Y big K. And so this matrix would be 193 by big K, which is 16 465. So then we have this big X equals big L times big Y. And then we told you that the answer, which is found in the analogous way to the least squares example."
"The slope is still going to, in the end, uh, be the effect between xk minus one and xk. Does that help with the intuition? Okay. Oh, yes. Uh, that helped a lot. And, uh, yeah, I think I'm clear on why, why we can't just, you know, in our least square solutions. So, uh, one last quick question. It's a clarification. Okay. So, at the end of the class, we have this curve, which is the probability of xk given xk all the ways to yk, right? Yeah."
"The slope is still going to, in the end, uh, be the effect between xk minus one and xk. Does that help with the intuition? Okay. Oh, yes. Uh, that helped a lot. And, uh, yeah, I think I'm clear on why, why we can't just, you know, in our least square solutions. So, uh, one last quick question. It's a clarification. Okay. So, at the end of the class, we have this curve, which is the probability of xk given xk all the ways to yk, right? Yeah."
"Then I can write F of T as a sum of complex exponentials. And last lecture, we converted as a sum of complex exponentials each weighted by some coefficient C k. Right. And so last lecture, we derived what these values C k are and how to find them. And we use this very cool proof where we use this trick where if you integrate and a complex exponential over a period, it equals zero. Except in the case when K equals when the complex exponential has a value of K equals where we're out here. So please review that proof if any of the steps were unclear. All right. So last lecture, we derived how to find these coefficients C k such that when you multiply each of the J k omega and R T, our complex exponentials, they add together to reproduce your periodic signal F of T. All right. So then we ended class last week by going over an example of a square wave where we took the square wave signal right here and we calculated its Fourier series coefficients."
"There are a few other conditions, which are called the derelict conditions that are beyond the scope of this past. But if you're interested in what well behaved means, please look into that further. If we have a well behaved periodic signal FFT, then we can write its Fourier series as the following. FFT can be written as a sum of complex exponentials each the Jk omega not to. Recall that this is a cosine plus a sign an imaginary sign a J times a sign times a Ck. And really the magic is in what are these Ck's the Ck's tell me how much of each complex exponential I need to how much I need to weigh each complex exponential by so that when they sum together, they give me my original signal FFT. Right. And today we're going to derive what those Ck are. So again, that's a high level of you of Fourier series what the rival of these results. But again, the statement is I can write my function FFT as a sum of complex exponentials. Any questions here? All right. So we last left off lecture with this concept of the eigenfunction. And eigenfunctions we said are these functions where if I have a system a function or a signal is called an eigenfunction. If when I put this signal into the system, I get back out the same signal except that it's scaled by some value a. Right. And so in this case, a is a real number and here it amplified my complex exponential in general. And time why asked this is a clarifying plus and last time a can be complex."
"There are several things about it that are, there are several things about the Poisson process that are non biological, and we'll start to address each of these later on, or we'll talk about some of them. For example, for this Poisson process, we're even assuming a constant rate lambda, right, that the firing rates have a constant rate lambda, but we know the firing rates change through time. And so, that's another place where we will have to make some updates to the model. And so, we'll talk about, for example, in homogeneous Poisson processes later today. But at face level, you're correct that in general, for a biological system, knowing about what's happened in the past could give you additional information about what's going to happen in the future. Okay. Thanks, Andrew. All right. Any other questions here?"
"There's a trial like is successful. And this is a Boolean is either one of the monkey successfully acquired a target or zero if he didn't. And so the reason that we have this is because. You can imagine if the monkey was failing a child, we may not want to use that data to learn our model. And so. What I could do is I could define the successor ray, which starts off empty. And then I'm going to loop over all the trials and then every single time. I. In the for loop, I'm going to just copy down the value of. I got it successful, which is going to be one if he was successful and zero if he wasn't. So if I run this code. It's going to loop on through. And if I look at now what success array is, what you're going to see is it's a 506 dimensional. Array or vector vector where every single element is a one. And so if I do the sum of success array to the equal 506 and that tells you that every single trial that I've given you was successful."
"There's actually a couple of CNN and another source that did a commentary on the video over the video that I watched as well as the video. Yeah. Did you watch a Professor Paul Nijikian's by any chance. No, this is getting some traction so I can send that to. He was my colleague during our we did our PhDs together and he goes into a lot of details. If you could send us the link. Thank you. Yeah, let me get that link right now. So that's a Paul new chickens at deconstruction of this. We zoom in on the monkey and this is all the hardware that record from the electrode array and then that sends it out to computers later on to be processed."
"This constraint means that the order of the polynomial in the numerator is strictly less than that in the denominator. And then we had talked last time about nomenclature, namely, A of S and B of S has m roots, which are called zeros of F of S. A of S has n roots, which are called the poles of F of S. And the poles are denoted lambda 1 up to lambda n, since we have an nth order polynomial, we'll have n roots. And then these numbers are one to rn are the residues. Any questions on this recap thus far. because it's easy for me to invert this Laplace transform. All of these are the inverse Laplace transform of 1 over s minus lambda, and we know from our Laplace transform table that the inverse Laplace transform of 1 over s minus a number is just e to that number t. So in the case where F of s has no repeated polls and m less than n. Our solution is going to be some sum of exponentials."
"This is not my first year at UCLA. This is now my fourth year. And it's my seventh time teaching this class. And so I, I've taught it at Stanford twice with my PhD advisor there and previously taught at UCLA four times. This is a class related very closely to the research that my research group does. And so we care a lot about improving the performance of brain machine interfaces. And then other classes that teach at UCLA include ECU 102 in the fall quarter, so that signals and systems. And then in the winter quarter, I teach a class on neural networks and deep learning. And so with that, I'm going to pass it off to one of our TAs, Tom, why so Tom, why if you can go ahead and unmute yourself. Yeah. Hi, so hello everyone. So my name is Tom, I'm on tour. I will be one of your keys for this quarter. So this is my third time, Tying for this course on neural signal processing and my fifth time, Tying for Jonathan. All of my Tying experience at UCLA has been for Jonathan. I really like Tying for Jonathan and especially for this course. So this course is one of my favorite courses here at UCLA. I took this course first when it was offered for the first time and back in 2017. And I really enjoyed the course material. I also hope that you also enjoyed this course too. So I am a PhD student at UCLA working with Professor Rajudhary and my research in interest, primary lies in reinforcement learning and pattern, exception and dynamic networks. So that's all about me and I hope to have a very enjoyable quarter with all of you. Thank you very much. Great. Thank you, Tom Boyd. And then we have our second TA, Shishank, who won't be able to give an introduction today. We'll ask him to give an introduction on Monday. But this will be Shishank's first time Tying this class here at UCLA and he's of course taken this class before and done, absolutely. And so we'll leave Shishank's introduction for the start of next class on Monday."
"This is not my first year at UCLA. This is now my fourth year. And it's my seventh time teaching this class. And so I, I've taught it at Stanford twice with my PhD advisor there and previously taught at UCLA four times. This is a class related very closely to the research that my research group does. And so we care a lot about improving the performance of brain machine interfaces. And then other classes that teach at UCLA include ECU 102 in the fall quarter, so that signals and systems. And then in the winter quarter, I teach a class on neural networks and deep learning. And so with that, I'm going to pass it off to one of our TAs, Tom, why so Tom, why if you can go ahead and unmute yourself. Yeah. Hi, so hello everyone. So my name is Tom, I'm on tour. I will be one of your keys for this quarter. So this is my third time, Tying for this course on neural signal processing and my fifth time, Tying for Jonathan. All of my Tying experience at UCLA has been for Jonathan. I really like Tying for Jonathan and especially for this course. So this course is one of my favorite courses here at UCLA. I took this course first when it was offered for the first time and back in 2017. And I really enjoyed the course material. I also hope that you also enjoyed this course too. So I am a PhD student at UCLA working with Professor Rajudhary and my research in interest, primary lies in reinforcement learning and pattern, exception and dynamic networks. So that's all about me and I hope to have a very enjoyable quarter with all of you. Thank you very much. Great. Thank you, Tom Boyd. And then we have our second TA, Shishank, who won't be able to give an introduction today. We'll ask him to give an introduction on Monday. But this will be Shishank's first time Tying this class here at UCLA and he's of course taken this class before and done, absolutely. And so we'll leave Shishank's introduction for the start of next class on Monday."
"This is not my first year at UCLA. This is now my fourth year. And it's my seventh time teaching this class. And so I, I've taught it at Stanford twice with my PhD advisor there and previously taught at UCLA four times. This is a class related very closely to the research that my research group does. And so we care a lot about improving the performance of brain machine interfaces. And then other classes that teach at UCLA include ECU 102 in the fall quarter, so that signals and systems. And then in the winter quarter, I teach a class on neural networks and deep learning. And so with that, I'm going to pass it off to one of our TAs, Tom, why so Tom, why if you can go ahead and unmute yourself. Yeah. Hi, so hello everyone. So my name is Tom, I'm on tour. I will be one of your keys for this quarter. So this is my third time, Tying for this course on neural signal processing and my fifth time, Tying for Jonathan. All of my Tying experience at UCLA has been for Jonathan. I really like Tying for Jonathan and especially for this course. So this course is one of my favorite courses here at UCLA. I took this course first when it was offered for the first time and back in 2017. And I really enjoyed the course material. I also hope that you also enjoyed this course too. So I am a PhD student at UCLA working with Professor Rajudhary and my research in interest, primary lies in reinforcement learning and pattern, exception and dynamic networks. So that's all about me and I hope to have a very enjoyable quarter with all of you. Thank you very much. Great. Thank you, Tom Boyd. And then we have our second TA, Shishank, who won't be able to give an introduction today. We'll ask him to give an introduction on Monday. But this will be Shishank's first time Tying this class here at UCLA and he's of course taken this class before and done, absolutely. And so we'll leave Shishank's introduction for the start of next class on Monday."
"This is of course coordinated within the class. This will be Sunday, November 8 from 12 to 2pm, and it will be at Tom boys usual office hours link and discussion section link Tom boy is going to record this review session and also post the solutions for this review session. OK, that was a lot. Any questions on announcements or questions on things related to the midterm? So I saw in the chat, Rodrigo says, ask so we don't have to log into class to take the midterm. That's correct. We're not going to proctor this exam over Zoom. Okay, any other questions?"
"This is the signal one over square root of T, that is in the homework. When you square the signal, it becomes one over T. And we know from high school or college calculus that the harmonic series where you're summing one over T as T goes from one to infinity, even though that decays towards zero, that has an infinite sum or if you were to integrate an infinite area under the curve. And so that's an example of a signal that has infinite energy but zero power. And so last lecture, I said that there are some signals that are neither energy nor power signals. And this is another example of one of those. Okay, any questions on any course logistics before we dive back into material? Excuse me for like assigning the question by outline, just the question number or like part A or part B also? It also includes part A and B. So we created the outline, we'll say it will say one A, one B, etc. And so please assign all of this."
This is what their wiener filter did and then they also implemented a velocity based common filter so a common filter that just because velocity and you can see on the individual trial the trajectories are much cleaner and if you were to take the average across trials you would get straight lines each target. And so at the time this is a very compelling demonstration that the velocity based common filter should be used over the wiener filter although today with some more hyperparameter optimization there's evidence that the wiener filter actually the vanilla wiener filter is better than the vanilla common filter. However after this paper once in the field worked on common filters and we may optimize into the common filter such that the state of the art common filter is indeed the state of the art algorithm out there today. Any questions here? Alright so I'm going to show this video in this a bit since I have to go to keynote to do that so let me first talk about the intuition. Alright so let's see that we have the following equation this is following our conventions from last time xk are the kinematics and yk is the neural data alright and you'll recall last lecture we talked about the optimal linear estimator where we did the relationship being xk equals L times yk so that we take neural data and use that to decode the kinematics xk alright.
"This means that my neuron fires many spikes per second, then I rightly should see that the expected the average time between spikes will decrease. If lambda is big my PDF right is going to go up to a larger value of lambda. This will be lambda big. Right. And then remember for the probability density function, the area under the curve has to equal one. So if for lambda being big is starts off higher, it better decay to zero more quickly. And so, if you look at this distribution here in blue, and you ask what's the average value of time, right, then let's say lambda was equal to this value. The distribution starts at a lower value, and the area under the curve is one and so when it exponentially decays it's going to have a more drawn out tail. And here is expected value would be larger."
"This simplifies to a k pi in the numerator. Oh, I see that works because it's of the shifting property then. Correct. I just have to think about it. I wasn't sure if it would work and why, but I think I see it now. Thank you. Great. I wonder if that's actually a further part of the question. Yeah, this is actually, oh, you know what? Sorry. So yeah, for 2b, I shouldn't have stopped here. We researched simplified according to what Daniel, Daniel brought up. So let's go ahead and do that. So we're going to take these 2 impulses and I'm going to carry over this work to the next page."
"Though I know that this is a required class for many of you. I hope that you'll still have a lot of fun learning the material in this class. I'm obviously biased, but I think that the material here is really interesting and will dovetail well into a lot of your future classes in the right. And again, as I was just saying, please ask questions, even questions that you may think are elementary. Those are the most important ones. All right. All right. So a first question that you might have is this class is called signals and systems. So what are signals and what are systems? To unpack this question, I wanted to take a high level view of what signals and systems are. And so what do we mean when we say signals and systems? Well, at a very high level. We know that our world and our society relies on being able to represent information, to represent the knowledge that we have. And then to to communicate that information knowledge, others to process that information and to operate on that information. And one way in which we can see the importance of this in our society is through technology. And technology is correlated to our ability to do these things to represent information and then also to communicate information. And so we can go back as far as prehistoric times by dining times before we had recorded history to ages of country gatherers where predominantly there, the means of communicating information was through personal interaction. And so you might be able to get some information by meeting up with someone, reading their facial expressions or if they had some form of language by word of math. All right. As civilization developed, we had a major advantage, which was the knowledge or the ability to write things down. And here I'm showing a picture of Egyptian hieroglyphics. But as we were able to write things down, now knowledge could be written down for future generations. And so this allowed information within, gained within a specific generation to propagated future generations."
"Times of probability of the second neuron, the second element, etc. And that's this expression that I've written right here. Okay, I just think it was just a little bit confusing for me because when I was deriving it for the Gaussian model, but looking at the summation line and read, I would use J as sort of a subscript rather than a superscript in that case. Yeah. And for these notes, I intentionally put J in a superscript. Actually, these are from Monday's office hours notes. I originally was using I, but then I saw that in the question statement, we use I as a subscript for the neuron number. And so that's why that's why I changed all of these to j's and have used it the trial to be the superscript parentheses to differentiate from the subscript number, which is the number the neuron number. Okay. Yeah. And so what you should be able to do then is if this is the probability of this probability is P of Y, the entire YJ, which is the entire vector in RD, then you could take this product. And so you know from the problem statement that the I neuron given the class CK is going to have a pause on distribution with mean lambda K I. That means that each of the P of Y I given CK can be written as the mean lambda K I raise to the spike counts, which is Y I times E to the minus mean divided by the spike counts, which is Y I factorial. So this this line here, all I've done is I've taken the plus on a lamp a K I and written that written the actual probability mass function in place here. Okay. I'm I guess I'm just wondering that at that point, you're taking we're optimizing over lambda. Was it lambda K I or lambda K J? Okay. So you just have one of these in the in the product."
"To get started for today. Our announcements, our one I reminded you that homework number two is due tonight. Upload it to grade scope, and please budget time for submission will be done a lot of sabbatical about problems with submission. Compiling or printing PDFs. Or I'll put into grade scope. So please just pleasant time if there's an issue for the homework, and then be sure to submit your code violence Notebooks in your doc PY files as well. The TAs will be uploading homework number three later after class today. And instead of it being due a week from today on Monday, we decided to give two more days on it, so it'll be due Wednesday, February 2023. Any questions on course logistics? The question is, will this affect the due Jacob future homeworks? The answer is yes. So I believe in conversation with the TAs. Homework number four will also be due the Wednesday after that, and then homework number five will now be two after the midterm. That's how we started clearing. Alright? Any other questions? Alright, we're gonna get back to material. So today's lecture is going to be on backpropagation. And this covers Section 6.5 is 0.6 and the deep learning textbook. And you'll recall that at the end of lecture last Wednesday, we just finished talking about the neural network architecture. We've talked about the activation function f that we would use for it. And we've talked about how for classification we would use the cross entropy loss. So that's great. We have our model architecture, we have our loss function. So we know that at the end of a neural network, we will pass this to some cross entropy loss to do classification. And now we're basically at the last part, being able to actually train and deploy neural networks. Which is we need to learn how to set the weights of the neural network. So remember that in-between every single layer we have, which is there'll be new and biases B. And so in this three layer neural network, we would have three sets of weights and biases. And we need to be able to know how to update these weights and biases. And we'll do that via gradient descent. But to do gradient descent, you know how to update the weights and biases. We need to compute gradients. Dl DW three, DL DW to, and DL DW one. And because this network looks a bit more complicated at least and other things, it's still an open question as to how we compute these gradients. Alright? So this lecture is going to be on backpropagation, which is the algorithm that we're going to use to be able to take the gradient of the loss with respect to W1, W2, and W3, basically any parameters and network, we're going to be able to take the gradient of the loss with respect to a few things that nomenclature. There's something called forward propagation. So forward propagation is just the act of starting at your inputs x and from them doing the forward computation. So linear layers then followed by your ReLu during the forward computation through the network to eventually calculate your loss. So for propagation goes from your inputs to calculate your loss. And this term backpropagation, is essentially the opposite. So at backpropagation, what we're going to do, we'll see is we're going to start off with a gradient of the loss with respect to our output. Then we're going to develop rules to basically calculate all of these gradients by taking our gradient at the end and basically back-propagating it all the way until the end. So that's where the name backprop comes from. Any questions on the motivation of why we need that competition. Alright, and so after we get these gradients, all we have to do is use gradient descent and we'll be able to change these weights to optimize this neural network. Alright, so these slides here, just putting it into text, what we, what we described on the prior slide, as well as the nomenclature before it and backpropogation. There is a question that you might have, which is, why do we need backpropagation? So if you look at the neural network architecture and you'd write out this equations, there is conceivably a way where you can analytically compute the gradients of the loss with respect to the weights, just like you did in homework number two for the softmax and for the hinge loss. Alright, so why did we do back propagation? The first thing is that back-propagation will find out is computationally efficient. Because before every backpropagation stuff, we're going to do a forward propagation step. We calculate all of the values and the number on route from the input to the loss. And if we just pass those values, will find that we can reuse them when we calculate backpropagated gradients. And so you don't have to do extra computation. Let me do back propagation. And then another reason that you might want to do back propagation is basically operationalizes the gradient computation procedure. So while you could write out the equation of the neural network and analytically calculate ingredients. At least for me, it takes quite a bit of time and it takes a bunch of mental effort. It's possible to do. But when you do backpropagation, but we're going to see is that we want to break down this gradient into many sub-problems, each of which are simple to solve. And so just like how decomposing code into functions is a good idea. So two, is decomposing your gradients into these small complications. We find out that when that happens, it also operationalizes gradient calculation, which means that it gives us a very simple algorithm to compute gradients in a neural network. And that is what makes software like PyTorch and TensorFlow is so powerful because there'll be able to compute gradients of any functions that we pass them using this backpropagation algorithm. Alright? Any questions before we get into the mathematical details here? Alright? So we're going to start off with just the simplest example where you can calculate an analytical gradient. But we'll show you, you how backpropagation works in this novel graduate to tougher examples. So we may want to compute the gradient with respect to our inputs x, y, and z, where f implements this function, alright? And this is not difficult to do, right? If I want to compute DS DZ, alright? We know that that's equal to x plus y. And we can also compute ds dx, that equals df dx and df dy y both equal to z. You can just do that by inspection. So this is super simple. We're going to use this example to motivate back propagation. So before backpropagation always comes forward propagation. So what I want to do is I'm just going to assign some dummy values to x, y, and z. There'll be 23.4. And then if we were to do this computation, x plus y times z, what I'm want to do is I'm going to draw what is called a computational graph. Alright? So basically what I do is I take this operation and I break it down where the sub operations are these nodes in the graph. And then our variables are the inputs. So here we have x plus y being implemented by this part of the computational graph. And then the result is multiplied by Z, and that's this multiplication here. And then that gives hopefully straightforward. If you plug in the values 234, you can go through these operations and get that at people's. What happens in backpropagation. Backpropagation always happens when we do it or some settings of x, y, and z. So what we do in backpropagation is we're going to start off with the upstream gradient. What do I mean by upstream gradient? I just need a gradient at the very end of our graphs. So here I'm just going to assume that DL df equals one. And I'm going to draw what the gradient of the loss with respect to each wire and the graph is. The graph as something in red. The wire. Alright, we'll talk about why we do it this way. You may say, where does the LDF come from? So at the output of a neural network, you'll have a softmax classifier. And from homework number two, you already know how to differentiate the loss with respect to the softmax classifier parameters. And so that would be like you're starting gradient at the end of the graph. Alright? And then what we wanna do is we want to take this gradient backpropagated to compute the gradients with respect to every other node in the networks. So we just assume for simplicity that we're going to have our upstream gradient equal to one in this example, but we will always be starting off with some gradients at the output. Any questions so far? All right, let's start to do the operations then to backpropagate. So that propagation really is the chain rule for calculus or the chain rule for derivatives. So let's say that I want to backpropagate the LDF to compute the gradient with respect to DC. If I want to compute the LDC, right, what I would do is I would use my chain rule for calculus, where DLD z would equal D LDF times df. Dz. Remember, DL, DFT is known to us, is the value of our upstream gradient that's just gonna be equal to one in this example. What is the FTC? This is something that we're going to call a local gradient, and I'll have a slide later that describes that more. But DFT is a gradient that we can compute because if we look at F, F is equal to z times the value on this wire over here. Let me just call this wire w. I'm gonna go ahead and call this wire w. Alright? So we know that F equals w times z. That's what this part of the computation graph tells us. If I want to compute d, f, d z, I would differentiate this equation with respect to w, and that will give me that ds z equals w. Remember w is a value of disquiet. If I want to compute DLD, I take my upstream gradient whose value is equal to one. And then I take the value of d f d z is w, the value on this wire, and that's equal to five. So this is equal to five. And therefore DLD z equals Any questions. Alright, and so similarly, I keep backpropagate through this wire by computing DL DW. So if I wanted the gradient at this wire, this would be DL DW. Dl DW would equal D L ds times ds dW, df dW equals z, right? Because remember, you put w times e. So if I differentiate this with respect to w, I just get x0 out. So in this example, the LDF is equal to one and d l, sorry, the FTW equals z and the value of z is equal to four. Alright, so then the gradient here DO w equals same operation as Bob for DLD z, but this is just for the other wire. Finally, backpropagate the gradient to find what DLD x and DLD YR. So if I want to find the L dx and dy l v y, then what I would do is I would write up the computation of this node, which is that w equals x plus y. Therefore, dw dx equals one, and d w d y equals one. Let's say I wanted to get dy dx. So let's say I want to get this gradient here. This is dy, dx. The gradient that I go at. This is DL DW, right? So this here is DLT W sacrilegious DL DW times dw, dx, dw, dx, I know from here, is equal to one. This thing equals one. And then DL DW, this thing equals four. So this equals four. And therefore the gradients, the L dx is equal to four. And for the same reason, DLD y will also equal four. All right, so you see just by using the simple chain rule, but doing this procedure where we just focused on one wire at a time and write the chain rule for that wider. You see that we were able to backpropagate this gradient to compute d l with respect to every single input variable x, y, and z, as well as intermediate. Any questions there? Rupture. And then that button. And so Ross's question is, can rewrite this as DLD F times d FTW. Like to swap these two because these are scalars in this case you can. But later on in this class we're going to talk about the factor multivariate chain rule in which case the order matters. And that's why I'll generally write the chain rule going from right to left. Because you'll see for the nominator layout, our chain rule for vectors and matrices will go from right to left. All right, you guys have had to be followed this example. Alright, so this is the idea of back propagation. We did it for a very shipboard example and now we're going to make it a bit more difficult. But first, some intuitions. The basic idea, as you've seen us play it out, is. We're going to always take whatever computation we're doing and we're going to break it down into a computational graph. That's this drawing over here. The forward pass, we will plug in the values of the inputs to compute our output. And then the backward pass. We'll first take the derivative of the output with respect to our last wire. That'll give us a gradient. In this case, we just said that radius is one. Then basically at every single node, what we can do is we can backpropagate the derivative by using the chain rule associated with the computation at this nerve, which we call a vocal. So let me just tell you more explicitly what I mean by local gradient. So if I start off with an upstream derivative CLTS, so the identity of this wire is, if I want to compute d L dx, so this graph, X and Y are these wires. And so d L, dx would be the gradients. Under this top input. We know that dy, dx is going to equal our upstream gradient. That's the gradient on the wire that's ahead of me. So this D LDS is called My upstream gradient. Then there's going to be a gradient. Dy dx, df, dx, and df dx is going to be called a local gradient. Because df dx is the gradient of this vocal computation or the football function f. So when I refer to a local gradient that refers to df, dx, and that reflects what the gradient is. For this function. In the backward pass, I can always compute the gradient of the wires just ahead at the input. As long as I know what the upstream gradient is and I know what the local gradient is, e, I know what the derivative of this function is with respect to its end points. Any questions there? Alright, so this is a slides just talk to you about what we talked about. We take a derivative. And as long as I know how to differentiate this function with respect to its inputs, I can always backpropagate through that function. This is how it around you'll see pie charts and TensorFlow operationalized gradient calculation. Basically whenever you call an operation like multiply or even something like cross-entropy. Pytorch or TensorFlow will always store what the gradient is of the function that you're calling. It will know how to compute this gradient. So that if you want to backpropagate, it knows to just take your upstream gradient, multiply it by the gradient of d f with respect to its inputs and then you can backpropagate the gradient to it. These foreigners question. Yeah. Yeah, the question is, is the order of these gradients because we used the denominator. So when these are multivariate quantities, yes, it will be because we use the denominator. Other questions, and we'll talk about that in more detail with Tom way says, are there, are there any roles to drawing a computational graph like e.g. x plus y times z. You wrote three independent variables as XYZ and then decompose. All right? This one way, another way. I see, yes. I'm always asking basically when I drew this computational graph, we draw every single operation, the addition and the multiplication has its own nodes. This is the way I think that you should all do it. You could conceivably no, ignored this phosphine and have some other way to draw a computational graph that confusing and it'll probably be two mistakes. And so when you draw these computational data, at least for this class, it will be most helpful for you to really write out every single operation. When you, when you draw these computational gas. That's gonna be a large part of getting, getting things ready. Alright? So just to nail it, the basic intuition of backpropagation, right? Is that we break up the calculation of a gradient into small and simple steps by focusing at one node at a time in the ultimate computation that we're doing. Each of the nodes. And these graphs correspond to a function. As long as I know the derivative of the function with respect to its inputs, then I will always be able to take an upstream gradient backpropagated to the inputs. Alright? And if I just keep composing these operations, I'll be able to take the gradient at the output of a neural network and back propagated to every single layer, the layer that occurs before it. So composing all of these gradients together then returns the overall group. Any questions there? Alright, so let's go ahead and talk about some operations where we're going to see this so often. And there's basically a role for backpropagating that makes it really straightforward. So if you have two inputs, like x and y and they are added together, we know that the local gradients, so in this case, equals x plus y. So the local gradients will be df dx and df dy y. And these things are able to work. So because these things are equal to one, and the gradient here is the upstream gradient times local gradient, which is one. When you see a plus sign and you're doing backpropagation, what that means is that a plus sign just passes through the gradient. So if you see a plus sign, you can always just write that this gradient here, dy dx will just equal d LDF, the upstream gradient, also equal TO. So if we think of these as gates, basically the AG gave the add operation and the neural network will distribute your gradient, the LDF to these two wires. We also will have multiplication gate. So let's say that we have two inputs, x and y. And the output is going to be S equals x times y. And we know the local gradients are df dx equals y and df dy y equals x. Again, since that propagation tells me to take my upstream gradients and multiply it by the local gradient. And the gradient at this wire is going to be the local gradient, y times dy LDF. So this is going to equal y times dy LDF. And then the gradients on this wire is going to be equal to x times DL DS. So when you see a multiplication gate, you can think of it as like a radiant couldn't put switcher where switcher just means I'm going to take my upstream gradient backpropagated to y. Then I multiply by x, by backpropagate to excellent, I have to multiply by y. So whenever you backpropagate through, you're just multiplying the upstream gradient by the value on the other wire. Flushing said, alright, let's continue on then. Um, we're gonna do this, the max operation because we're going to find out that max is going to come up a lot in our neural networks because we are going to be using the ReLu operations, which is max of zero comma x. So in this case, let's say that df dx, sorry, must first day that ash is equal to max x comma y. Then df dx is going to be an important one. If x is bigger than y. Because if x is bigger than y, then f equals x and dx, dx equals one. Then if y is bigger than x, then x equals y. Differentiate with respect to x, I get zero error. So this is going to equal one if x is greater than y and zero. Otherwise. We can write this as the indicator function, that x is bigger than y. So then when I got off it through a max operation, I'm gonna get what is essentially a gradient router. So one of these conditions, x is bigger than Y or Y is bigger than x is going to be true. And so x is bigger than y. Then the gradient comes here, has DLD f, and the gradient on this wire will be zero. If y is bigger than x, then the gradient d LDF comes onto this wire. But over here it's going to be equal to zero. So the max operation will route the LDF to whichever wire as a bigger value. Daniel. Daniel's question is what happens when x equals y? That gets asked every single year. So I always say, practically, first-off, x will not be equal y in our networks because we practically implement them because they'll be like double or single precision. If x equals y, then it depends on how you define your max operation. So if you said S is equal to y, if we return x, then that, then you could assign the gradient to be one when x is bigger than or equal to one. Question. Other questions. Yeah. I'm not sure I understood the question. So you said when we say DY DX before. When I say that, sorry, so I'm not making any assumption about independence of X and Y. Just looking at the function, you differentiate it. You get an x and y depend on each other in some way when we take the partial derivatives of the assumed that the other variables are all constants. The question is, when do we use the max case? When we have ReLu, we know that ReLu of x equals max of x and zero. And so in our computational graph, we're going to have a bunch of bees, or we can have a max where the inputs are x and zero. Yeah, the student's question is, we don't want to backpropagate to zero, right? Yeah, this case it would be useless to backpropagate to zero. But we still need to know how to backpropagate to x. And that depends on the value of X being bigger than or less than zero. We're going to continue on with this. What we're gonna do is we're gonna do a more involved scalar example. The scalar example we're going to be using a bunch of big clinical case we talked about already and multiplication and addition. So I'm going to be using a bunch of other gates as well so that we can make sure that we understand this operation. So what I have here is the sigmoid function. We have 1/1 plus exponential of minus WE 00 plus W1, X1 plus W2. And so I've drawn out the computational graph. So all the way up until this node, this node em, implements this sum here. Then I multiply that by minus one, That's this node. Then I exponentiate the results, you get this. So that's exponential. Then to get the entire denominator, I add one, that's this node right here. And then to get f, I take the inverse of what's in the denominator and I guess the one over the denominator. And so that's this computational graph that we see. And this is where we've drawn out every single operation again, just for the sake of being careful and it'll it'll unsure a reduced headaches and mistakes later on. So what I've done here is I've just arbitrarily chosen some values of w zero, et cetera, et cetera. I've done the forward hopper for propagation through this graph. So if you assign these values and you do all these operations, you will get that f equals zero points sudden the root. Alright? Now we're gonna start with backpropagation. So we're going to assume that we're going to start off with an upstream gradient. Dld. Yes. That is equal to warn. And the first thing that I want to do is a backpropagation through an operation we haven't talked about yet. So what I want to back propagate from one to the gradient over here. I'm just going to call this wire. Okay? So let's say the value of this wire is a and in this particular instantiation, a equals 1.37. Firstly, I want you to think about it for 20 to 30 s. Feel free to talk to your neighbor as to how do I back propagate from the LDF here to the LDA, which is backpropagating through this inverse operation. Alright, if there's some discussion, does someone want to raise her, someone raise their hand and tell me how I back propagate through the inverse operation. So this student told us the answer to exactly. Remember that I break down my computation into my upstream gradient, which is DLT F. And that equals one times my vocal gradient, which is ds, da. Alright? In this case is the inverse operation. So f equals one over a. And therefore, if I want to compute my local gradient, which is the gradient of just the inverse operation. I would get that DF da equals minus one over a squared. Therefore, if I want to compute what the LDA is, all I have to do is I had to do the LDA equals my vocal gradient by local gradient in this case is going to be minus one over the value of a. So a is 1.37 squared times by upstream gradient which is born. And this thing simplifies to be equal to -0.53. So the gradient over here is gonna be -0.5. I guess that's just taking gradient, which is the gradient of f with respect to a and then multiplying it by my gradients of one. Any questions? So does anyone want me to go over this guy? Alright, we'll continue on then. I have, this is the word for the one that we just did. We have the gradient -0.53 here. Next we're backpropagating through a plus sign and no for plus sign, the video just passes through. So if I want the gradient at this node here, it would be just -0.53 times the gradient passes through a plus sign. So we have -0.1 by three here. Now we want to backpropagate through this exponent exponential function. And to do this, we would do exactly what we've been talking about. So if I call the values on this wire, if I call this C and I call this one D wire, which has the value of one, that will also get minus point. Yeah, great. I see a tomboy saying that the gradient at this wire would be -0.53. So if you were to just follow our rules, that would be true. But really those gate rules apply it. This is like some variable because really DEF d1 is equal to zero. So if this were a variable that is gradient would also be -0.5. Alright, so we're going to backpropagate through this exponential. Here. I will write up the computation. So if I call these wires b and c, Then the local computation is that d equals EXP of C. And if I want my local gradient, right, I need to be able to compute DB DC. And the derivative of this function is exponential of C. So if I want the gradient here, I would take my upstream gradient, which is -0.53 and multiply it by the local gradient, which is exponential of c and c is equal to minus one. So the gradient over here as -0.53 times exponential of minus one equals -0.2. Any questions on this local gradient? Backpropagation? Alright, if we've made it this far, the rest is easy. Because the rest are just multiplications and additions, or for those we have our, alright. So if I want to backpropagate to this value over here, we know when you backpropagate through a multiplication, you take the upstream gradient and you multiply it by the value of the other wire. So this would be -0.2 times minus one. That gives me 0.20. And then 0.20 here, I can start to backpropagate through these plus signs, which just pass through the gradient. So the gradient DLD W2, It's going to be 0.20. This is going to be 0.20. These are going to be 0.2, 0.0, 0.20 plus signs are super easy. And then the multiplication for just a tiny bit more complicated. So backpropagating from 0.20 to x one, I would multiply 0.20 by the wire value here, which is two. So this gradient would be 0.40. This would be 0.20 times the value of X1, which is two, that's 0.00 would be 0.20 and DL DW zero would be -0.20. And that's using the rules that we developed further multiplication gates. Here you've seen that by using this backdrop roll, we've taken the LDF. We have compute all the gradients with respect to the inputs W0, W1, X1 and believes you. Alright? Any questions on this example or any step in this example, and then use it as a proposition. So yes, this is scaling up when we're differentiating whether you're doing. Great. Yeah, so his question is about pi torch and how it keeps track of what the gradients are. So if you're multiplying two matrices and Py Torch, right, you wouldn't use Pi torch, his version of matrix multiply. Because pi torches version of matrix multiply, doesn't just multiply two matrices, but it also knows what the local gradient is so that it can backpropagate through that operation. So basically in Py Torch, whenever you make a computational graph, every single node in our computational graph is going to be a defined by torch function. But only that, not only does the operation, but stores or local gradients, and then you backpropagate through it. And then if you want to implement a function that's not implied works, you can define your own function and all you have to do is pass it what the local gradient is. So then pie charts will still know how to backpropagate through it. We'll get more into this in week seven or week eight when we talk about the deep-learning likers. Any other questions? Yeah, question is, can I go over the inverse again? Yes. So for the inverse, remember that we always, when we calculate the gradient at the input of the function here, we take our upstream gradient, g LDF, and we multiply that by the local gradient of this inverse function. The inverse function I'm calling f. So that would be the FDA where f equals one over there. All I have to do to get the local gradient is compute the FDA. And the gradient of y over a is minus one over k squared. So now I know that the LDA will be DLD F, which is one times df dy, which is minus one over a squared. A is the value of this wire. So a here is the value 1.37. And so my overall answer is gonna be one times -1/1, 0.3 s squared. Any other questions here? Yes. Thank you. So the student is asking, when I call somebody the upstream gradient, is it different for every wire? Yes. So when I back propagate e.g. through this exponential function, my oxygen gradient is going to be -0.53 and my local gradient is the gradient of this exponential functions. So basically, I always look at just one operation. The upstream gradient is the gradient at the output. The local gradient is the gradient of that function. And then I want to get the back propagated gradient, which is a gradient as the important. Thanks for letting me clarify Other questions. Alright. Can you raise your hand to to follow? That's awesome. I think that's almost the entire class. So given that, that's how backpropagation works at the scalar level. Now we're going to move on to do back-propagation at the multivariate model. Oh, sorry. Before that, there's one more rule that we have to talk about. So first off, I want to say what backpropagation? As long as you can draw this computational graph where you know the local gradients, you can take the gradient of anything. Alright, so I think on question number two of the homework, which will be a homework number three, which will be optional for C1, 47 students will be mandatory. Proceed to 47 students. He took his paper from Europe's in 2004. And I remember reading this paper. And they just wrote down some gradients of the loss function with respect to weights. But oftentimes in textbooks and in papers, they don't show you all the mathematical steps to get from D L to do just to complete that gradient. And so as a grad student, I was working on this gradient for an entire day and I can never get the analytical solution correct. And then I remembered, oh wait, I could do this with backpropagation. And that's what you're gonna do in homework number three, pushing to wherever you read the computational graph can do with backpropagation, you get the gradient fairly easily, alright? So that's one advantage I talked about where formally for me it's just easier to think of gradients when you break them down into these bite-size chunks. Alright, there's one more thing that we have to talk about, which is what you do when you have two paths converging on a node. So I'm gonna call this wire x. I'm going to call this wire one. I'm going to call this wire Q2. And let's say I have upstream gradients. I know what DL, dq, then I know what DL, dq2. And then in this case, Q1 equals h of x, equals h of x. What I want to know is, if I know my upstream gradients, DLT, dq, q1, and q2. Now there are two of them. I want to compute what dy, dx is. So here's a new situation. We haven't been captured where to gradient paths converge on one question for you all is, what is dy, dx in terms of DLD Q1 and DLP T2. So this is a tricky question. I'll give you 3 s to think about it. Feel free to talk to your neighbors. Does anyone have the answer? I think it happened yesterday. Danielle. That's exactly tracks. Yeah. So Daniel's answer is that this will equal, um, so I'm gonna write this as just a general sum for the general answer, or the system will just be over two terms. So I'm gonna write a sum from I equals one to n. Little n is the number of converging passband. And we're going to have the upstream gradients D L, D Q Pi. And then we have the local gradients dq dy, dx. So you may recall that this is the law of total derivatives. And the reason it makes sense is because x changes through Q1 and Q2 because they are functions of each other. And so if I want the total change of the loss with respect to x, I have to sum up the contributions due to Q1 and Q2. Alright? So if the summation of this with I equals one is how much the loss changes if I wiggle x did acute one. When I equals two is how much the loss changes with respect to x when I recall Q2. And then a total change of the loss with respect to x will be the sum of all"
"Um, I don't think that's. Uh, sorry, me. I, I broke up. I don't think that's correct. Yeah, that's all I said. Okay, got it. Uh, okay, let me, uh, let me, sorry, let me just pull up the homework. All right. All right. So, um, is this for, uh, notebook number three? Yeah. So it's, yeah, for three C, it says calculate the means for error. I got a certain number. Yes, it says does it do better worse than the optimal linear estimator?"
"Um, I don't think that's. Uh, sorry, me. I, I broke up. I don't think that's correct. Yeah, that's all I said. Okay, got it. Uh, okay, let me, uh, let me, sorry, let me just pull up the homework. All right. All right. So, um, is this for, uh, notebook number three? Yeah. So it's, yeah, for three C, it says calculate the means for error. I got a certain number. Yes, it says does it do better worse than the optimal linear estimator?"
"Um, well, it looks like, um, E, but also A, but I'm not sure if A is correct. I'm saying there's two different graphs. Great, yeah, so, um, A is split up into the real part and the imaginary part of X of j omega is also even. And so if they're both even then their sum is even. Some is even. And more precisely, when we take x, when we take real part of x of j omega plus j times the imaginary part of x of j omega, both the imaginary and the real parts are even and so therefore the entire signal must the there. When we combine them into a complex number that will also be even. So a is correct. It's even and then Sal also said that E is even. That's also correct. That one you can just look. X of j omega is clearly a symmetric about the y-axis there. There's one more of these that is even. Can anyone tell me what that is?"
"Viana. Hi. So you mentioned that those three problems for homework three are still fair game for the exam, even though that they're not graded. And I was wondering, is it possible that even though the homeworks are shorter, we have access to maybe the original problems that would have been on them so we can have additional practice for the exams? That's a great question. I'll discuss that with the TAs. I guess one potential thing that we could do is we could release what the original homework was going to be but then dedicate a subset of the questions to the optional. And so not sure exactly what we'll do there. I'll discuss that with the TAs to figure out what's the best thing going forward. But thanks for that suggestion. Viana. Any other suggestions? All right. And so with that, we're going to get back to material. So in last lecture, we had talked about this concept of the impulse response. And we had this fact that if I have any linear time invariant system, then as long as I know the impulse response, which is how that system responds when I input a delta into the system. And as long as I know this impulse response, I can calculate the output of the system, Y of T. For any input X of T. Again, as long as I know the impulse response. So this impulse response is a whole characterization of the system. And last lecture, we did ride that the way this happens is via this convolution integral. And the solution integral is the manifestation of this fact on the prior slide, where, as long as I know the impulse response, H of T. I can calculate my Y of T."
"We also recognize that some of you may not be in the United States or maybe you're on the East Coast and if you're in a different time zone such that taking the exam during class time or during our final exam slot would be difficult, please email me by the end of this week so that we can have a list of the students and we can organize a separate exam time. Okay, so many are saying they aren't able to connect. That's peculiar. The lecture will be recorded. I do see a lot of people who don't have their mic or video set up. So, here's what I'm going to do. I'm going to end this meeting right now, and then I'm going to start it again within the next minute, and then we'll see if people can connect them."
"We asked you record your question, sorry, that you post your question publicly to everyone so that it would be for the benefit of everyone. And then I just also wanted to mention something about asking questions. This is a large class. I think we have right now on zoom. We have 158 students in here. And I remember being an undergrad and having questions during lecture, but oftentimes being afraid to ask them because I fear that I might ask a stupid question or others might judge me. So I want to say that in this class, the likelihood of stupid questions is very rare. So for the purposes of practice for this class, I'm going to say there are no stupid questions. And I think that if you have a question like that, it's really important to ask it for two reasons. The first is that it's very, there are going to be concepts in this class that builds on top of prior knowledge. And if you aren't following something that's more fundamental, then it's really important to get that addressed so that we can build on that. And you can understand later material. The second is that if you have a question about something that you don't understand is very likely that others in the class also may not have picked that up. And that might be because I didn't explain it well. And so if I ask you a question, you give me an additional opportunity to try to explain a concept. All right. And then the TAs are also going to be operating on this quarter. I put all of their links here for their office hours, as well as for their discussion sections. And we'll talk about logistics there later on in class. All right. Any questions before we start off? All right. Great. So in this first lecture of signals and systems, I wanted to give an overview of what this class is all about. And so we'll also after talking about the high level details of this class, go into the course logistics and the syllabus."
"We can see that for different values of sigma and omega and j omega, it's going to have very different behavior. If sigma is bigger than zero, it's going to be a growing complex exponential. If sigma is less than zero, it's going to be a decaying exponential. If omega is small, it's going to be a slow, it's going to be a sinusoid that changes very slowly, where it's omega is a very large number that's going to be a high frequency sinusoid. It's going to have faster oscillations. Right. And so what we can do is we can conceptualize this in a plane where on the x axis, we have the value of sigma. And on the y axis, we have the value of omega. And so like we just said, if you're in the right hand side of the plane, which means that sigma is greater than zero. So here, sigma is greater than zero. This means that the complex exponential grows with time."
"We didn't 1D is going to be XY transpose times YY transpose inverse. All right. And we showed you how to do this in that lab. L is equal to X times Y. This apostrophe is a transpose. And then this inverse is this big minus 1. And we do Y times Y transpose. And so in the 189 project code, that is this right here. L equals, so another way to write it is X times pseudo inverse of Y. This PN of Y is equal to this expression Y transpose inverse YY transpose. And so here in the code, we do that exactly L equals X bin times the pseudo inverse of Y bin. And so this gives us our matrix L. And then that matrix L tells us how do we decode new neural data coming in. So now if we were to run an experiment forward, we're trying to decode the intention of someone who's paralyzed. We read out their neural data. And from that neural data, we get a 193 dimensional vector that has the number of spikes on the 192 neuros and our constant one."
"We do that so that we could fix his head during the experiments, because we want, because essentially the experiment has to be very precisely calibrated the monkey centers to be in a particular position and we also put a juice tube into his mouth so that they receive a juice reward. And the monkey doesn't feel discomfort during this. His head is just fixed into a single location. Neuralink released a video last week where they actually did everything freely and wirelessly. And they had already trained the monkey to hold onto a juice tube, which essentially is the monkey holding his head still in a single area. And so they removed this whole implant part, which is remarkable."
"We have overshoot here at the extremities and then we have under shoot along the edges of this square wave. Right. But now if I go ahead and I let you have three complex exponentials of frequencies. So now I have a sum from K equals minus three to plus three of our C ke to the J K omega not T. Now with these additional frequencies, I can start to approximate even better. And as to add more and more complex exponential frequencies all the way up until here I'm showing you 100. You can see the sum of all of these signs and cosines are going to give me a fairly good approximation of the square wave. All right. Okay. So I want to pause and say any questions here. I know we went over this rather quickly at the end of last lecture."
"We sent out instructions on how to sign up for Piazza and Grayscope. And so please do so if you haven't already. And then we will send out an announcement shortly on the consolidation of discussion sections. And so, Tonmoi, one of our TAs was able to find three sections that everyone who wanted to attend the live discussions can make. And so we'll be able to do that consolidation. All right. I see the chat lighting up. If there are any problems, TAs, can you just unmute yourselves and let me know. I'm going to just forward to have here. So I just wanted to mention on the syllabus, it appears when we write homework, we release that they're released on Wednesday, but we release them typically one week before the due date. Unless they are a longer assignment. So we, these homework assignments are all due on Fridays and we're going to release them on Fridays. But we will announce all of that in class. And so hopefully none of that will be ambiguous."
"We're going to derive the answer in 1D. And then I'm going to tell you the answer when you have multiple dimensions like 2 and 192. The reason we're going to derive the answer in 1D and then I'm going to tell you the answer for 2 and 192 is because in the 1D case, we're just working with scalars, which we can differentiate. It turns out that you can differentiate with respect to vectors and matrices also, but this requires a bit more formalism. And so in this class, we're going to develop the principles to derive it in 1D. And I'm going to tell you the answer in multiple B, but you'll see that the answer in multiple B is like a multiple dimensional generalization of the 1D example. So in the 1D example, let's say that we just had neural data from neuron 1. And so that's why super-ship 1 of k. This is the neural data from neuron 1 at time k. And on the y-axis, we have the x-flossy at time k."
"We're going to start lecture now. So a few answers before we begin. The first to reminder that homework number four is do this Friday and then we will also release homework number five then. And homework number five is going to be a pen and paper homework on graphical models. The other announcement is a reminder that midterm read grades are going to close on Monday, May 17th and so be sure to get your read grades in by then. Are there any questions regarding the course? All right, so we'll get back to material. So we're going to finish up graphical models today and then after that we're going to get into continuous decoding for brain machine interfaces. So recap from last lecture is we introduce this notion of graphical models where we have nodes that represent random variables and links that connect random variables. And when we have a graphical model, what this does is it gives us a factorization of the probability density of all of the random variables."
"We're going to talk about a few encoding models in this class, especially some tuning models. We'll see in later slides. But that'll be the extent to which we talk about neural encoding. And this class will be focused more on neural decoding, which is the map from the neural responses, like spikes, to the stimuli. And the stimuli in this case could be the motor action. So I want to decode how a monkey intends to move his or her arm so that the monkey can move a cursor on the screen or play pong like in that most recent neural link video. All right. So in this case, we attempt to reconstruct this stimulus or the motor action from the spike sequence. And this is what will spend more time extensively discussing during this class. So before that, we'll just talk a bit at a high level about neural encoding. And that's what these next slides will go through. Before I move on, any questions here?"
"We're not going to go in depth into some of the proofs because some of the proofs look very similar to ones we've done in the past. However, we'll do a fair amount of these proofs. I copy and pasted the proofs into these lecture notes, even if we're not going to do them or we're going to go to them more quickly. But if you look at the formal notes, hand out number 10. There will be a derivation of every single one of these properties there. So we're going to start off with time reversal for today. And time reversal tells me that if I take my FFT and I time reverses. So I do F of negative T. Then what happens is that the spectrum is also reversed. Alright, this is actually really easy to prove to prove this. All we have to do is use the scaling result here setting a equals negative one. And so if we take the time scaling result, we already proved and set a equals minus one. Then we achieve this result. And so we don't even have to do a formal proof here. So let's do an example of how this time reversal property works. So we're going to ask here to find the Fourier transform of each of the negative a and then absolute value of T. So this signal is one that looks like this at time T equals zero."
"We're starting at time P plus 1. So my error is this should be an L4. So this should be an L4. Since P is equal to 4, so it goes from L0 all the way up to L4. And then this goes to K minus P. K is equal to P plus 1 is equal to 5, so 5 minus 4 is equal to 1. So it should have been this. If you follow the pattern, good. OK. Thanks for catching that Caleb. I'm going to re-upload these notes now so that I have the correct version uploaded to CCLE. So then while this is happening, I'm happy to take the next question. Edwin. Hello, Professor."
"Well, you take the line. So and then you pick out the the one with the I one. That's correct. That's correct exactly. So yeah, you you put this. I'll just say it just in case to be clear to everyone off on the call. What what Caleb was saying is that we take this product, we plug it in here, right? And then that's just into a sum of log of this expression. And then from there, you can differentiate with respect to lambda K I. Because that's the parameter of this model. And as Caleb was also mentioning, this son is going to have. So that's the lambda K I's for many case and many I's where you're different shooting with respect to a particular particular lambda K I. And so that should simplify the calculus and. To. To make sure that you've done the the math correctly, I can tell you the intuitive answer here, which is that lambda K I ends up being. The average firing rate for neuron I in class K. So when you actually derive what lambda K I is. You should get an expression that looks like lambda K I is equal to the average firing rate of neuron I in class K."
"What did you think about and he did that? Well, not very much. I do it all the time. It's not a matter. Is that hard work? Are you having to concentrate? All right. So with that video, I want to highlight a few things. First, you can see that she could move the robotic arm. Through the space, she was able to open and close the hand and even give. Scott tell you a fist bump. Interestingly, um, Scott tell you asked her, what are you thinking about when you. When you when you move the arm and her response was essentially, uh, I don't really think about anything. I'm or I'm naturalistic."
When we did this integral we had this expression here evaluated at infinity minus this expression evaluated at zero. And we saw to get to the answer 1 over a plus s we have to have that as time went to infinity this term over here this term would go to zero. All right. And this is critical because this defines the region of convergence of the Laplace transform. So again for this integral here to equal 1 over a plus s this term here has to go to zero as time here t goes to infinity. All right. And so we have to then find a condition under which this term goes to zero as t goes to infinity. And that's how we did that at the end of last lecture where we saw that as long as as long as so this is the term e to the minus a plus s t goes to zero as long as e to the minus a plus sigma t goes to zero or sigma remember a s is equal to sigma plus j omega. So sigma is the real part of s.
"Yeah, because I was wondering, like, or that question was just in general, but for like 5b, I was wondering, like, what does it mean when it says, like, is it worth it? It's like, are we comparing, like, do we want species A versus species B? Or I was just confused about the wording of that. Yeah, so for 5B, a potential con of myelinating is that it takes up more space. And so, looking at the answer in 5A, we wanted you to see that, because we know that action speed is going to increase if we increase the diameter as well. And so both of these things take up more space but which one is better. And from the calculation five a you should have seen that it was better to have myelination, then just the compared to the wider diameter neuron. Yeah. And so therefore, myelin helps."
"Yeah, great. Yeah. So William and then Grace wrote something similar in chat, which is looks like they're over shooting. That's exactly right. So it turns out that with the common filter, when we incorporate the inertia, we reduce it incorporated. So when you look at the movements, they are relatively smooth towards the target. You don't see the jitteryness that we saw with optimal linear estimator, for example. The problem is that it's in a way too smooth as if there's too much inertia. So usually when there's a target that's far away, you see that he overshoots it. Although actually two minutes in, which is what let me restrict the video. It turns out that the monkey will learn a good control strategy. So two minutes in, he kind of compensation for this over shooting by by not going as fast. But you can see early on when he's trying to get to a target. He often overshoots it. And that's because there's too much smoothness in the velocity. And so while this incorporates inertia, it causes you to be so much inertia sometimes that that the monkey will overshoot. And then when there are small movements that he needs to make, you can see he goes back and forth around that again, because as far as it's kind of stopped the inertia of the common filter. Any questions there? All right, so we saw this problem of the common filter and we decided to make an innovation on it to make it higher performance. And so we had two ideas, two main ideas to do this. And these comprise what is now called the refit common filter control algorithm. And this is something that we published in Nature Neuroscience in 2012. And so the first idea is less related to the common filter. But it has to do with the training data. So let me talk about that innovation first and then it's related to our second innovation, which helps with this over shooting problem. So let's say that the monkey is starting at a center target and wants to reach to a target that is up into the right. When the monkey makes such a reach, the trajectory isn't the straight line to the target, but when we reach up into the right, we make a curve trajectory because of the biomechanics. And so if you think about how in the homework, the way that we calculated the velocities that we read rest to is that we just took adjacent positions. And then we just did a first order or error approximation of the velocity. So we took the two adjacent positions subtracted them and then divided by T. What this means is that at the very start of the trial early on, even though the monkey intends to go up into the right, the velocities are actually pointing upwards. And at the end of the trial as he gets this target, even though he's again trying to go up into the right, the velocities point just to the right. Whereas we believe the neural data reflects that he wants to always go up into the right, but the movement itself has some of these suboptimalities just due to the way of biomechanics how hands naturally move."
"Yeah, great. Yeah. So William and then Grace wrote something similar in chat, which is looks like they're over shooting. That's exactly right. So it turns out that with the common filter, when we incorporate the inertia, we reduce it incorporated. So when you look at the movements, they are relatively smooth towards the target. You don't see the jitteryness that we saw with optimal linear estimator, for example. The problem is that it's in a way too smooth as if there's too much inertia. So usually when there's a target that's far away, you see that he overshoots it. Although actually two minutes in, which is what let me restrict the video. It turns out that the monkey will learn a good control strategy. So two minutes in, he kind of compensation for this over shooting by by not going as fast. But you can see early on when he's trying to get to a target. He often overshoots it. And that's because there's too much smoothness in the velocity. And so while this incorporates inertia, it causes you to be so much inertia sometimes that that the monkey will overshoot. And then when there are small movements that he needs to make, you can see he goes back and forth around that again, because as far as it's kind of stopped the inertia of the common filter. Any questions there? All right, so we saw this problem of the common filter and we decided to make an innovation on it to make it higher performance. And so we had two ideas, two main ideas to do this. And these comprise what is now called the refit common filter control algorithm. And this is something that we published in Nature Neuroscience in 2012. And so the first idea is less related to the common filter. But it has to do with the training data. So let me talk about that innovation first and then it's related to our second innovation, which helps with this over shooting problem. So let's say that the monkey is starting at a center target and wants to reach to a target that is up into the right. When the monkey makes such a reach, the trajectory isn't the straight line to the target, but when we reach up into the right, we make a curve trajectory because of the biomechanics. And so if you think about how in the homework, the way that we calculated the velocities that we read rest to is that we just took adjacent positions. And then we just did a first order or error approximation of the velocity. So we took the two adjacent positions subtracted them and then divided by T. What this means is that at the very start of the trial early on, even though the monkey intends to go up into the right, the velocities are actually pointing upwards. And at the end of the trial as he gets this target, even though he's again trying to go up into the right, the velocities point just to the right. Whereas we believe the neural data reflects that he wants to always go up into the right, but the movement itself has some of these suboptimalities just due to the way of biomechanics how hands naturally move."
"Yeah, I have a question. So since the integral is not Evaluable, but it does turn into that like 2 pi times Dirac Delta Does that mean like that integral like does? Equal that end result and that like if like we find this integral and like another problem We can like use that result Yeah, I have eaten negative JW omega t. Can we say that that just equals to pi direct. Yeah, we can. Yeah, so, I'm solving this integral, including duality and the fact that the Fourier transform of delta t is one allows us to, to, to give this value to the Fourier transform of one, and so the answer is yes, and then she is a bit of subtlety that I missed there please chime in and let me know. Cool, thanks. And I'm just going to take this opportunity really quickly to make my TAs co-hosts so that they can unmute themselves."
"Yeah, I would do the latter. Yeah. But let me look at what we do in the, in the homework in, in sorry, in the solutions. Yep, that's what we do. Yeah. So the reason I said the latter is because maybe you have some trials where like, there are fewer spikes, then you would be up waiting their contribution to the average. Whereas we just want to look at all of the ISIS and then see like a cross all ISIS that happen across all trials for reach. What was the average yet? Yeah, same for the CV as well. So if we, as I'm trying to separate each of the reaches, there's a hundred trials and each trial has data. What the CVB of again, all the data if we create, put everything in a given reach angle. So there's a hundred trials times how many ISIS will we copy the CV of like the total of that? Yeah, so we should take the standard deviation and the mean across all the ISIS and then the CV will be one number for each wish condition. But it's a standard deviation of all of your concatenated all of the ISIS across all the trials divided by the mean of all the ISIS across all the trials."
"Yeah, it's on one. So since this is a continuous decoding task, right, we are doing it for 25 millisecond time bins. So for that 25 millisecond time bins, how do you record the kinematics? Like for example, the position might vary over to 25 milliseconds or is it fixed? Yeah, great. So over the 25 milliseconds, the monkey will be moving. So if you imagine, let's say the monkey is holding a center target and then he makes a reschedulatory out to one of the targets, let's say he reaches out to here. So this would be time zero and then in the homework, we'll tell you how to do this, but basically every 25 milliseconds. So actually, let me fight it like this. In between these two time points, we'll be 25 milliseconds in space. And so this will be here. The x and y position here will be p of x1 and p of y1."
"Yeah, so I'm looking at this chart we see that k plus is minus 75 for equilibrium potential so if only k plus was there, the equilibrium potential would be minus 75. If only NA plus was there, then the resting potential would be plus 55. Now, when K plus and NA plus are both there, the resting potential is gonna be in between these two numbers because K plus is gonna pull it down to minus 75, NA plus is gonna try to pull it up to plus 55. Why does it end up at minus 65? Is that essentially what happens is that it's going to be a weighted sum of K plus equilibrium potential and any plus and the weight is going to be based off of how permeable."
"Yeah, that's a picture that you should have here like a when when this NA plus gets sufficiently close. Oh, minus is so attractive that it's going to come down to bind to it, but to bind to it, it has to shed these waters around it so it can make it close bond to it. And so. Yeah. And so. That's that's how the binding site works. And. And the reason. K plus doesn't pass through a high energy binding site is site. Yeah, I believe size is the biggest factor. There may be also things related to confirmation, but. But some textbooks say primarily size. There's one other thing interesting you'll note, which is. I can find it. Yeah, we wrote that K plus channels are 100x more permeable, both the K plus and NA plus. Whereas NA plus channels are only 10 to 20 times more permeable to NA plus and K plus. And so it seems also based off of this that. The selection via the energy site is a stronger filter."
"Yeah, that's, that's all my questions. Thank you very much. Thanks, thanks, too. All right, hopefully internet has stabilized a bit. If it ends up being bad again, can someone, uh, please let me know. And then, um, and then I will try to reset things. I have a question about the homework. Great. I'm just trying to sanity check, uh, my answers. Okay. And I got, for the means for error, for the optimal linear estimator, the weiner filter, I think I have pretty much the same number. Okay."
"Yeah, that's, that's all my questions. Thank you very much. Thanks, thanks, too. All right, hopefully internet has stabilized a bit. If it ends up being bad again, can someone, uh, please let me know. And then, um, and then I will try to reset things. I have a question about the homework. Great. I'm just trying to sanity check, uh, my answers. Okay. And I got, for the means for error, for the optimal linear estimator, the weiner filter, I think I have pretty much the same number. Okay."
"Yeah, we will take some time but you you have all the results you need to show it. Got it. So I'm going to wrap up with questions based off of some of the derivations we've already done with the Fourier transform. In terms of, in terms of manipulating the Fourier transform, and we'll, we'll go over more on on Monday. We have our next lecture. I see. All right. Thank you. I would ask. Cool. Thanks. And just, I think everyone knows this, but this homework is not due tomorrow. It's due a week from tomorrow."
"Yeah. And we could have defined this more clearly on the homework. So let's do 4B here. So when we say that the Fourier series has only odd harmonics, that means that the only coefficients that are not zero are C1, C-1, C3, C-3, etc. These are not zero. But then C2, C-2, C4, C-4, etc. are zero. And so I've seen already an answer posted on Piazza that uses this property to derive a relationship for X of T. Because ultimately, we're asking you all to plot what X of T is going to look like. So we can go ahead and try to get that property and then we can see what X of T looks like. So can anyone who saw for this property tell us your approach here? Sorry, what exactly did you ask me? Oh, sorry. Let me clarify the question. So we're told in this question that the signal X of T only has odd harmonics and isn't even function. So we're just going to focus on the odd harmonics property. And I'm going to actually just write what I've seen on Piazza."
"Yeah. So for the PLT dot bar, you'll have like an, uh, a vector of x look x values and y values, right? Yeah. So for the x values, you could just make that the like zero, 20, 40, 60, 80, 100, all the way up to 1000. And then that'll, that'll plot the y values at those x locations. I see. Um, along. I see. I'll try that. Do you mind if I try it right now? No problem. Yeah. Yeah. Yeah."
"Yeah. So I share my screen. Yes. Well, awesome. Um, will you be showing code for the, uh, for the problem? Oh, yeah. I'm going to just pause recording then so that, uh, all right. So anyone watching this postdoc, we just returned from looking at Ian's code. And, uh, a reminder that. So if you're plotting, if you're using PLT dot bar. And the time bins are 20 milliseconds. So you're giving a time sample the x locate the x locations of PLT dot bar are like 20, 40, 60, 80. Um, there's some odd behavior in PLT dot bar. So you have to sit the width of the bar to 12 to see all the bars."
"Yeah. So we're going to do a purple example next. I'm going to focus on green just for now. So the green interval corresponds to the region of convergence which far the values of s for which the Laplace transform exists. And so for the Laplace transform to exist we have to choose a value of s such that this exponential goes to zero as t went to infinity. And we derived that the condition for this exponent, this exponential to go to zero as t went to infinity was that it's real part sigma had to be bigger than minus a. And so what I'm drawing here is I'm drawing I'm highlighting here in green the entire region for which these particular values of s anywhere in this green lead to you being able to compute a Laplace transform for for e to the minus e to the minus a t times u of t. So as long as you choose an s that is in this green region, then the Laplace transform will exist. I'm going to meet you one more time."
"Yep, that's right. But to do that you needed to pump and the pump isn't there, and leak channels and sodium channels would still be present, I'm guessing. Yeah. Wouldn't the neuron just equalize or equiliberalize slightly higher? It wouldn't. There would be a...or... It depends. Let me pull up the slides, just so that I can have the diagram to point you to. All right. So Okay. So the reason that we that we still need the pump is that for the K plus channels these three on the left."
"Yes, in this particular example, that is correct. So this one. This T here that says the T in the argument of H is going to be the same T as the upper bound of this integral. I'm sorry, I think my internet went out for like a split second. I think it just did it again. Okay, it's going the T here is the same as the T here. So that's correct. I also just realized that I have a little type of it. I want to correct actually make this actually tell detail. So what you said is correct one. Thank you. And Marga. So why is the in the system is X of TOW detail? How come we're not integrating with respect to T? In this system, we are saying we want to calculate the area under the curve until a time T. So when I do an integral, I could call this whatever variable I want. TOW are just a dummy variable that tells me I'm going to be integrating along the X axis up until sometime two. Okay. Alright. Alright, so then at the end of last lecture, we were also describing a time invariant impulse response for a system that is time invariant. This impulse response will of course also be time invariant, meaning that if I put adults into the system and I get out an impulse response that looks like this. If I have a delay by time TOW sorry for these typos."
"Yes, so you mentioned that the brain machine interfaces have applications for complete paralysis, but deep does that also have an application for things like essential tremor, where it's like they can move, but it's impaired? That's a great question. It could have application there. If the tremor is due to, for example, Parkinson's disease, then there are better treatments, a treatment alternative, like deep brain stimulation. And I'll show some examples of that in lecture today. In theory, it could be, but we'll also learn, at least for what we'll talk about in lecture, to record from neurons, you have to do neurosurgery. And so there are a lot of costs and risks associated with the system. And to justify those costs and risks, the ability to decode has to be very high, or the tremor has to be completely removed. And I'm not aware of anything aside from deep brain stimulation to address Parkinsonian tremor right now."
"Yes. Also, a question about the last assignment. I'm not sure if you can answer that. I'm not sure if you can answer that. I'm not sure if you can answer that. I'm not sure if you can answer that. I'm not sure if you can answer that. I'm not sure if you can answer that. I'm not sure if you can answer that. I'm not sure if you can answer that."
"You can also generalize that to control the 3D position of a robotic arm. And so what this participant here, Jan, which I'm not Jan, a capy, what capy is thinking about is she's going to be controlling the 3D endpoint of this robotic arm. It turns out that this arm is fairly complex. You can see it has several joints and angles. She's not controlling those joints and angles. She's not thinking I want to have the angle, have the first 20 at 45 degrees in the second joint, the BS 7 degrees. She's not thinking that she's only thinking about the 3D endpoint. And this is actually something reasonable because when you think about how we move, we are in thinking I'm going to move with my elbow at, you know, a 90 degree angle. We think about the endpoint that we're trying to reach to and the biomechanics of our arm takes care of the rest. And so that's kind of the goal for this, that is the goal for these robotic, very machine interfaces as well, which is that we would decode the endpoint. And then a biomechanical model to solve all the inverse kinematics of what these joints should be. There's going to be one more thing she controls in here."
"You don't have to do your work on the exam. And then you need to stop working at 3.50 PM and then we give you 10 minutes to upload your exam to Gradescope. All right? So those details will all be on CCLE. The TAs and I will also be in a Zoom room, not this room, but a different link that we have put in that CCLE announcement. So please drop by that link if you have any questions about the exam and we'll be posting any updates or clarifications to Piazza."
"You may feel the pace of the class is somewhat slow. And then we're going to keep the pace because a lot of people in the class don't have background in machine learning. And I want to make sure that everyone can learn and master the material. And so if you're coming in with the prior background in machine learning, just be aware that the class may feel a bit slower for you. And you can factor that into your decision as you decide what class is to take. And then I like we talked last time. I think we're going to take halfway through lecture because the lectures are fairly long here at UCLA. And in our discussion sections, which we're going to be run live by Tonloi and Shashank. First off, you can attend whatever discussion section you would like. And the TAs will also be recording at least one of the discussion sections to put it on CCLA. And then we have already sent out the P.O.T.S.ing page to send up information so this full of points should not be here. All right, a lot of logistics I just went through any questions on anything that I went through. Okay, so I just want to do brief introductions. So about me and sorry. This is not updated well. I did update the other parts of the slide, but I didn't update this introduction."
"You may feel the pace of the class is somewhat slow. And then we're going to keep the pace because a lot of people in the class don't have background in machine learning. And I want to make sure that everyone can learn and master the material. And so if you're coming in with the prior background in machine learning, just be aware that the class may feel a bit slower for you. And you can factor that into your decision as you decide what class is to take. And then I like we talked last time. I think we're going to take halfway through lecture because the lectures are fairly long here at UCLA. And in our discussion sections, which we're going to be run live by Tonloi and Shashank. First off, you can attend whatever discussion section you would like. And the TAs will also be recording at least one of the discussion sections to put it on CCLA. And then we have already sent out the P.O.T.S.ing page to send up information so this full of points should not be here. All right, a lot of logistics I just went through any questions on anything that I went through. Okay, so I just want to do brief introductions. So about me and sorry. This is not updated well. I did update the other parts of the slide, but I didn't update this introduction."
"You may feel the pace of the class is somewhat slow. And then we're going to keep the pace because a lot of people in the class don't have background in machine learning. And I want to make sure that everyone can learn and master the material. And so if you're coming in with the prior background in machine learning, just be aware that the class may feel a bit slower for you. And you can factor that into your decision as you decide what class is to take. And then I like we talked last time. I think we're going to take halfway through lecture because the lectures are fairly long here at UCLA. And in our discussion sections, which we're going to be run live by Tonloi and Shashank. First off, you can attend whatever discussion section you would like. And the TAs will also be recording at least one of the discussion sections to put it on CCLA. And then we have already sent out the P.O.T.S.ing page to send up information so this full of points should not be here. All right, a lot of logistics I just went through any questions on anything that I went through. Okay, so I just want to do brief introductions. So about me and sorry. This is not updated well. I did update the other parts of the slide, but I didn't update this introduction."
"You would have the access to going from zero to 1000. You would have 50 bins and then you would sample the actual function. Also every 20 milliseconds. So you would have 50 times points to plot and Python. Okay. Yeah. That definitely looks good. Follow questions to that then is when I was plotting the bins. I was like labeling the bins. So like I was labeling them like zero to 20, 21 to 40 and so on. So like I don't know how to like make the x axis like time like this, but also show 50 bins. Um, actually, when you, uh, what function are you using to plot the, uh, the bars are using the PLT dot bar function. Yeah. Yeah."
"Your output will have either amplitude distortion if the magnitude response is not just equal to k, or phase distortion if the phase response is not a straight line. So what do these distortions look like intuitively. Well for amplitude distortion. It's relatively straightforward. So, the amplitude response is going to be a constant with the value, the amplitude K. And so when the amplitude deviates from an impulse, for example, if we try to make an impulse, but we didn't do a good job and because an impulse is really hard to make in real life, and so, or it's impossible to make in real life. So instead of an impulse, we have something that looks kind of like an impulse, but has some, has some, isn't perfectly an impulse."