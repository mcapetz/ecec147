{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EKOTlwcmxmej"
   },
   "source": [
    "# GPT-2 Fine-Tuning Tutorial with PyTorch & Huggingface in Colab\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NKGBoVwuhM4H"
   },
   "source": [
    "This is a simplified script for fine-tuning GPT2 using Hugging Face's [Transformers library](https://huggingface.co/transformers/) and PyTorch.\n",
    "\n",
    "You should understand the basics of PyTorch and how a training loop works before getting started. [This official PyTorch tutorial](https://pytorch.org/tutorials/beginner/nn_tutorial.html) serves as an excellent introduction. Familiarity with the workings of GPT2 might be useful but isn't required. The code has been written for clarity and not re-use. I'd advise refactoring it for actual projects. I've liberally taken bits from [Chris McCormick's BERT fine-tuning tutorial](https://mccormickml.com/2019/07/22/BERT-fine-tuning/), [Ian Porter's GPT2 tutorial](https://snappishproductions.com/blog/2020/03/01/chapter-9.5-text-generation-with-gpt-2-and-only-pytorch.html.html) and the [Hugging Face Language model fine-tuning script](https://huggingface.co/transformers/v2.0.0/examples.html#language-model-fine-tuning) so full credit to them. Chris' code has pretty much provided the basis for this script - you should definitely check out his [blog](https://mccormickml.com/tutorials/).\n",
    "\n",
    "I should mention what the script doesn't cover:\n",
    "\n",
    "- Using the [nlp](https://huggingface.co/nlp/) library to load in the dataset and setting up the training workflow, which looks to streamline things rather nicely.\n",
    "- [Accumulated gradients](https://medium.com/huggingface/training-larger-batches-practical-tips-on-1-gpu-multi-gpu-distributed-setups-ec88c3e51255) - this gives larger effective batch sizes than Colab allows (GPT2 is a large model, and anything more than a batch size of 2 would be enough to get a CUDA out of memory error on Colab).\n",
    "- [Freezing layers](https://github.com/huggingface/transformers/issues/1431). This is the process of only changing the parameters in selected layers, made famous by the [ULMFit](https://arxiv.org/abs/1801.06146) process.\n",
    "- [Using 'past'](https://huggingface.co/transformers/quickstart.html#using-the-past) when generating text. This takes in the previous state when generating successive items of text. I didn't need it.\n",
    "- [Tensor packing](https://snappishproductions.com/blog/2020/03/01/chapter-9.5-text-generation-with-gpt-2-and-only-pytorch.html.html). This is a neat way of fitting in as much training data in each batch. \n",
    "- [Hyperparameter search](https://discuss.huggingface.co/t/using-hyperparameter-search-in-trainer/785/10). I settled quickly on values that seemed to produce decent values, without checking if they were optimal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xf3Qw77SZGbS"
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JCCeyhuDHdOu",
    "outputId": "d630a22a-68d6-44e9-8d40-e20800da100d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\email\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import datetime\n",
    "# from google.colab import drive\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# % matplotlib inline\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, RandomSampler, SequentialSampler\n",
    "torch.manual_seed(42)\n",
    "\n",
    "from transformers import GPT2LMHeadModel,  GPT2Tokenizer, GPT2Config, GPT2LMHeadModel\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'pwd' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = r\"D:\\Python\\kaoculator\\\\\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZfdCML6Parvv"
   },
   "source": [
    "# Create Training Set\n",
    "\n",
    "We're using the SQUAD dataset for question and answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "VPMgkpJwpu6I"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "data = json.load(open(f\"{data_path}train-v2.0.json\", \"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['qas', 'context'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['data'][0]['paragraphs'][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'qas': [{'question': 'When did Beyonce start becoming popular?',\n",
       "   'id': '56be85543aeaaa14008c9063',\n",
       "   'answers': [{'text': 'in the late 1990s', 'answer_start': 269}],\n",
       "   'is_impossible': False},\n",
       "  {'question': 'What areas did Beyonce compete in when she was growing up?',\n",
       "   'id': '56be85543aeaaa14008c9065',\n",
       "   'answers': [{'text': 'singing and dancing', 'answer_start': 207}],\n",
       "   'is_impossible': False},\n",
       "  {'question': \"When did Beyonce leave Destiny's Child and become a solo singer?\",\n",
       "   'id': '56be85543aeaaa14008c9066',\n",
       "   'answers': [{'text': '2003', 'answer_start': 526}],\n",
       "   'is_impossible': False},\n",
       "  {'question': 'In what city and state did Beyonce  grow up? ',\n",
       "   'id': '56bf6b0f3aeaaa14008c9601',\n",
       "   'answers': [{'text': 'Houston, Texas', 'answer_start': 166}],\n",
       "   'is_impossible': False},\n",
       "  {'question': 'In which decade did Beyonce become famous?',\n",
       "   'id': '56bf6b0f3aeaaa14008c9602',\n",
       "   'answers': [{'text': 'late 1990s', 'answer_start': 276}],\n",
       "   'is_impossible': False},\n",
       "  {'question': 'In what R&B group was she the lead singer?',\n",
       "   'id': '56bf6b0f3aeaaa14008c9603',\n",
       "   'answers': [{'text': \"Destiny's Child\", 'answer_start': 320}],\n",
       "   'is_impossible': False},\n",
       "  {'question': 'What album made her a worldwide known artist?',\n",
       "   'id': '56bf6b0f3aeaaa14008c9604',\n",
       "   'answers': [{'text': 'Dangerously in Love', 'answer_start': 505}],\n",
       "   'is_impossible': False},\n",
       "  {'question': \"Who managed the Destiny's Child group?\",\n",
       "   'id': '56bf6b0f3aeaaa14008c9605',\n",
       "   'answers': [{'text': 'Mathew Knowles', 'answer_start': 360}],\n",
       "   'is_impossible': False},\n",
       "  {'question': 'When did Beyoncé rise to fame?',\n",
       "   'id': '56d43c5f2ccc5a1400d830a9',\n",
       "   'answers': [{'text': 'late 1990s', 'answer_start': 276}],\n",
       "   'is_impossible': False},\n",
       "  {'question': \"What role did Beyoncé have in Destiny's Child?\",\n",
       "   'id': '56d43c5f2ccc5a1400d830aa',\n",
       "   'answers': [{'text': 'lead singer', 'answer_start': 290}],\n",
       "   'is_impossible': False},\n",
       "  {'question': 'What was the first album Beyoncé released as a solo artist?',\n",
       "   'id': '56d43c5f2ccc5a1400d830ab',\n",
       "   'answers': [{'text': 'Dangerously in Love', 'answer_start': 505}],\n",
       "   'is_impossible': False},\n",
       "  {'question': 'When did Beyoncé release Dangerously in Love?',\n",
       "   'id': '56d43c5f2ccc5a1400d830ac',\n",
       "   'answers': [{'text': '2003', 'answer_start': 526}],\n",
       "   'is_impossible': False},\n",
       "  {'question': 'How many Grammy awards did Beyoncé win for her first solo album?',\n",
       "   'id': '56d43c5f2ccc5a1400d830ad',\n",
       "   'answers': [{'text': 'five', 'answer_start': 590}],\n",
       "   'is_impossible': False},\n",
       "  {'question': \"What was Beyoncé's role in Destiny's Child?\",\n",
       "   'id': '56d43ce42ccc5a1400d830b4',\n",
       "   'answers': [{'text': 'lead singer', 'answer_start': 290}],\n",
       "   'is_impossible': False},\n",
       "  {'question': \"What was the name of Beyoncé's first solo album?\",\n",
       "   'id': '56d43ce42ccc5a1400d830b5',\n",
       "   'answers': [{'text': 'Dangerously in Love', 'answer_start': 505}],\n",
       "   'is_impossible': False}],\n",
       " 'context': 'Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny\\'s Child. Managed by her father, Mathew Knowles, the group became one of the world\\'s best-selling girl groups of all time. Their hiatus saw the release of Beyoncé\\'s debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['data'][0]['paragraphs'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86821\n"
     ]
    }
   ],
   "source": [
    "processed_data = []\n",
    "for sample in data['data']:\n",
    "    sample = sample['paragraphs']\n",
    "    for paragraph in sample:\n",
    "        for qa_pairs in paragraph['qas']:\n",
    "            for answer in qa_pairs['answers']:\n",
    "                entry = \"<|startoftext|>\" + paragraph['context'] + \" \" + qa_pairs['question'] + \"<|answer|>\" + answer['text'] + \"<|endoftext|>\"\n",
    "                processed_data.append(entry)\n",
    "print(len(processed_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 140
    },
    "id": "G_DWAMe1FopX",
    "outputId": "b6199fe3-2e62-4af7-9f89-59edb551b055"
   },
   "outputs": [],
   "source": [
    "class QADataset(Dataset):\n",
    "    def __init__(self, path:str, tokenizer, max_length=768):\n",
    "        self.data = json.load(open(path, \"r\"))\n",
    "\n",
    "        self.X = []\n",
    "        self.Y = []\n",
    "        for sample in self.data['data']:\n",
    "            sample = sample['paragraphs']\n",
    "            for paragraph in sample:\n",
    "                for qa_pairs in paragraph['qas']:\n",
    "                    for answer in qa_pairs['answers']:\n",
    "                        context_question = \"<|startoftext|>\" + paragraph['context'] + \" \" + qa_pairs['question'] + \"<|answer|>\"\n",
    "                        self.X.append(context_question)\n",
    "                        self.Y.append(answer['text'])\n",
    "                        \n",
    "        \n",
    "        indices = np.random.permutation(5000) #change to number of samples you want \n",
    "        self.X, self.Y = np.array(self.X), np.array(self.Y)\n",
    "        self.X, self.Y = self.X[indices], self.Y[indices]\n",
    "        self.X, self.Y = list(self.X), list(self.Y)\n",
    "        \n",
    "        print(self.X[0])\n",
    "\n",
    "        self.X_encoded = tokenizer(self.X, truncation=True, padding=\"max_length\", return_tensors=\"pt\", max_length=max_length)\n",
    "        self.Y_encoded = tokenizer(self.Y, truncation=True, padding=\"max_length\", return_tensors=\"pt\", max_length=max_length)\n",
    "        self.input_ids = self.X_encoded['input_ids']\n",
    "        self.attention_mask = self.X_encoded['attention_mask']\n",
    "        self.label_ids = self.Y_encoded['input_ids']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.input_ids[idx], self.attention_mask[idx], self.label_ids[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hQ1oK0kXaV5p"
   },
   "source": [
    "We need to get an idea of how long our training documents are.\n",
    "\n",
    "I'm not going to use the same tokenizer as the GPT2 one, which is a [byte pair encoding tokenizer](https://blog.floydhub.com/tokenization-nlp/). Instead, I'm using a simple one just to get a rough understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 455
    },
    "id": "cKsH2sU0OCQA",
    "outputId": "a89e5c66-a5ec-404f-d88e-5b596c358887"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:03<00:00, 1520.23it/s]\n",
      "C:\\Users\\email\\AppData\\Local\\Temp/ipykernel_17516/4274368390.py:13: UserWarning: \n",
      "\n",
      "`distplot` is a deprecated function and will be removed in seaborn v0.14.0.\n",
      "\n",
      "Please adapt your code to use either `displot` (a figure-level function with\n",
      "similar flexibility) or `histplot` (an axes-level function for histograms).\n",
      "\n",
      "For a guide to updating your code to use the new functions, please see\n",
      "https://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751\n",
      "\n",
      "  sns.distplot(doc_lengths)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot: ylabel='Density'>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "doc_lengths = []\n",
    "\n",
    "for d in tqdm(processed_data[:5000]):\n",
    "\n",
    "    # get rough token count distribution\n",
    "    tokens = nltk.word_tokenize(d)\n",
    "\n",
    "    doc_lengths.append(len(tokens))\n",
    "\n",
    "doc_lengths = np.array(doc_lengths)\n",
    "count = 0\n",
    "sns.distplot(doc_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u6P6bTItJEIj",
    "outputId": "c2e57fbd-95cb-4bea-d866-33ff7c08806e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the max token length   \n",
    "len(doc_lengths[doc_lengths > 768])/len(doc_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "63t_69HjlwAj",
    "outputId": "89b73719-b57f-4898-b869-116e37131c93"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "162.7892"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.average(doc_lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tuq5bqdr4_a6"
   },
   "source": [
    "Even though these token counts won't match up to the BPE tokenizer's, I'm confident that most bios will be fit under the 768 embedding size limit for the small GPT2 model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cMml12FJGjPW"
   },
   "source": [
    "# GPT2 Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ANJhbBwdxN-b"
   },
   "source": [
    "Although the defaults take care of this,I thought I'd show that you can specify some of the special tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z474sSC6oe7A",
    "outputId": "37d040ab-b13f-4f00-d623-5a0526237fd2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the GPT tokenizer.\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2', bos_token='<|startoftext|>', eos_token='<|endoftext|>', pad_token='<|pad|>') #gpt2-medium\n",
    "tokenizer.add_tokens([\"<|answer|>\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sh0XKuDvnryn",
    "outputId": "089f70f3-2300-48b0-a6b3-2dc33dec7b31"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The max model length is 1024 for this model, although the actual embedding size for GPT small is 768\n",
      "The beginning of sequence token <|startoftext|> token has the id 50257\n",
      "The end of sequence token <|endoftext|> has the id 50256\n",
      "The padding token <|pad|> has the id 50258\n"
     ]
    }
   ],
   "source": [
    "print(\"The max model length is {} for this model, although the actual embedding size for GPT small is 768\".format(tokenizer.model_max_length))\n",
    "print(\"The beginning of sequence token {} token has the id {}\".format(tokenizer.convert_ids_to_tokens(tokenizer.bos_token_id), tokenizer.bos_token_id))\n",
    "print(\"The end of sequence token {} has the id {}\".format(tokenizer.convert_ids_to_tokens(tokenizer.eos_token_id), tokenizer.eos_token_id))\n",
    "print(\"The padding token {} has the id {}\".format(tokenizer.convert_ids_to_tokens(tokenizer.pad_token_id), tokenizer.pad_token_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ex5O1eV-Pfct"
   },
   "source": [
    "# PyTorch Datasets & Dataloaders\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3lgZoOYkxZfx"
   },
   "source": [
    "GPT2 is a large model. Increasing the batch size above 2 has lead to out of memory problems. This can be mitigated by accumulating the gradients but that is out of scope here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "scqrzmqhV__z"
   },
   "outputs": [],
   "source": [
    "batch_size = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lqGMee7Isfpx"
   },
   "source": [
    "I'm using the standard PyTorch approach of loading data in using a [dataset class](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html).\n",
    "\n",
    "I'm passing in the tokenizer as an argument but normally I would  instantiate it within the class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "89Z7aYUgpWrd"
   },
   "source": [
    "To understand how I've used the tokenizer, it's worth reading [the docs](https://huggingface.co/transformers/main_classes/tokenizer.html). I've wrapped each bio in the bos and eos tokens.\n",
    "\n",
    "Every tensor passed to the model should be the same length.\n",
    "\n",
    "If the bio is shorter than 768 tokens, it will be padded to a length of 768 using the padding token. In addition, an attention mask will be returned that needs to be passed to the model to tell it to ignore the padding tokens. \n",
    "\n",
    "If the bio is longer than 768 tokens, it will be truncated without the eos_token. This isn't a problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xza_O1_rD7yh",
    "outputId": "c8e1197f-2b6d-4f78-c6b3-34c5cc32c432"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|startoftext|>Executive vice governor Wei Hong confirmed on November 21, 2008 that more than 90,000 people in total were dead or missing in the earthquake. He stated that 200,000 homes had been rebuilt, and 685,000 were under reconstruction, but 1.94 million households were still without permanent shelter. 1,300 schools had been reconstructed, with initial relocation of 25 townships, including Beichuan and Wenchuan, two of the most devastated areas. The government spent $441 billion on relief and reconstruction efforts. Who spoke about the dead and missing people on November 21, 2008?<|answer|>\n",
      "4,500 training samples\n",
      "  500 validation samples\n"
     ]
    }
   ],
   "source": [
    "dataset = QADataset(f\"{data_path}train-v2.0.json\", tokenizer, max_length=768)\n",
    "\n",
    "# Split into training and validation sets\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "print('{:>5,} training samples'.format(train_size))\n",
    "print('{:>5,} validation samples'.format(val_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "x0WeP5PREUuy"
   },
   "outputs": [],
   "source": [
    "# Create the DataLoaders for our training and validation datasets.\n",
    "# We'll take training samples in random order. \n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,  # The training samples.\n",
    "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
    "            batch_size = batch_size # Trains with this batch size.\n",
    "        )\n",
    "\n",
    "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
    "validation_dataloader = DataLoader(\n",
    "            val_dataset, # The validation samples.\n",
    "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
    "            batch_size = batch_size # Evaluate with this batch size.\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D6TKgyUzPIQc"
   },
   "source": [
    "# Finetune GPT2 Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "gFsCTp_mporB"
   },
   "outputs": [],
   "source": [
    "# I'm not really doing anything with the config buheret\n",
    "configuration = GPT2Config.from_pretrained('gpt2', output_hidden_states=False)\n",
    "\n",
    "# instantiate the model\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\", config=configuration)\n",
    "\n",
    "# this step is necessary because I've added some tokens (bos_token, etc) to the embeddings\n",
    "# otherwise the tokenizer and model tensors won't match up\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Tell pytorch to run this model on the GPU.\n",
    "device = torch.device(\"cuda\")\n",
    "model.cuda()\n",
    "\n",
    "# Set the seed value all over the place to make this reproducible.\n",
    "seed_val = 42\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "pBEVY2PYSTXJ"
   },
   "outputs": [],
   "source": [
    "# some parameters I cooked up that work reasonably well\n",
    "\n",
    "epochs = 50\n",
    "learning_rate = 5e-4\n",
    "warmup_steps = 1e2\n",
    "epsilon = 1e-8\n",
    "\n",
    "# this produces sample output every 100 steps\n",
    "sample_every = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GLs72DuMODJO",
    "outputId": "5ef8d020-506d-43c3-a252-ca6c73b50847"
   },
   "outputs": [],
   "source": [
    "# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n",
    "optimizer = torch.optim.AdamW(model.parameters(),\n",
    "                  lr = learning_rate,\n",
    "                  eps = epsilon\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "-p0upAhhRiIx"
   },
   "outputs": [],
   "source": [
    "# Total number of training steps is [number of batches] x [number of epochs]. \n",
    "# (Note that this is not the same as the number of training samples).\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "# This changes the learning rate as the training loop progresses\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = warmup_steps, \n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "gpt6tR83keZD"
   },
   "outputs": [],
   "source": [
    "def format_time(elapsed):\n",
    "    return str(datetime.timedelta(seconds=int(round((elapsed)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "mUHW6xbJefWZ"
   },
   "outputs": [],
   "source": [
    "# MODEL_PATH = f'{data_path}model_save/gpt2-base.pt'\n",
    "\n",
    "# checkpoint = torch.load(MODEL_PATH)\n",
    "# model.load_state_dict(checkpoint['model_state_dict'])\n",
    "# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "# epoch = checkpoint['epoch']\n",
    "# loss = checkpoint['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vCPohrZ-CTWu",
    "outputId": "4b565d63-4400-4841-cc7e-44e70824845d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 50 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4500/4500 [16:09<00:00,  4.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 0.04\n",
      "  Validation took: 0:00:24\n",
      "\n",
      "======== Epoch 2 / 50 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4500/4500 [13:55<00:00,  5.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 0.04\n",
      "  Validation took: 0:00:23\n",
      "\n",
      "======== Epoch 3 / 50 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4500/4500 [15:58<00:00,  4.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 0.04\n",
      "  Validation took: 0:00:25\n",
      "\n",
      "======== Epoch 4 / 50 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4500/4500 [14:39<00:00,  5.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 0.04\n",
      "  Validation took: 0:00:24\n",
      "\n",
      "======== Epoch 5 / 50 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4500/4500 [14:38<00:00,  5.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 0.04\n",
      "  Validation took: 0:00:25\n",
      "\n",
      "======== Epoch 6 / 50 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4500/4500 [18:06<00:00,  4.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 0.04\n",
      "  Validation took: 0:00:33\n",
      "\n",
      "======== Epoch 7 / 50 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4500/4500 [15:28<00:00,  4.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 0.04\n",
      "  Validation took: 0:00:22\n",
      "\n",
      "======== Epoch 8 / 50 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4500/4500 [13:26<00:00,  5.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 0.04\n",
      "  Validation took: 0:00:22\n",
      "\n",
      "======== Epoch 9 / 50 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4500/4500 [13:26<00:00,  5.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 0.04\n",
      "  Validation took: 0:00:22\n",
      "\n",
      "======== Epoch 10 / 50 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4500/4500 [14:08<00:00,  5.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 0.04\n",
      "  Validation took: 0:00:24\n",
      "\n",
      "======== Epoch 11 / 50 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4500/4500 [14:57<00:00,  5.02it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 0.04\n",
      "  Validation took: 0:00:24\n",
      "\n",
      "======== Epoch 12 / 50 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4500/4500 [15:41<00:00,  4.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 0.04\n",
      "  Validation took: 0:00:35\n",
      "\n",
      "======== Epoch 13 / 50 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4500/4500 [17:41<00:00,  4.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 0.04\n",
      "  Validation took: 0:00:28\n",
      "\n",
      "======== Epoch 14 / 50 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4500/4500 [17:49<00:00,  4.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 0.04\n",
      "  Validation took: 0:00:29\n",
      "\n",
      "======== Epoch 15 / 50 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4500/4500 [16:14<00:00,  4.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 0.04\n",
      "  Validation took: 0:00:26\n",
      "\n",
      "======== Epoch 16 / 50 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 457/4500 [01:43<15:14,  4.42it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17516/2917269264.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     90\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 92\u001b[1;33m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     93\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m         \u001b[0mscheduler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\email\\anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m                 \u001b[0minstance\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_step_count\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m                 \u001b[0mwrapped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m             \u001b[1;31m# Note that the returned function here is no longer a bound method,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\email\\anaconda3\\lib\\site-packages\\torch\\optim\\optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m                     \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\email\\anaconda3\\lib\\site-packages\\torch\\autograd\\grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\email\\anaconda3\\lib\\site-packages\\torch\\optim\\adamw.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    143\u001b[0m                 \u001b[0mstate_steps\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'step'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 145\u001b[1;33m             F.adamw(params_with_grad,\n\u001b[0m\u001b[0;32m    146\u001b[0m                     \u001b[0mgrads\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m                     \u001b[0mexp_avgs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\email\\anaconda3\\lib\\site-packages\\torch\\optim\\_functional.py\u001b[0m in \u001b[0;36madamw\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m         \u001b[1;31m# Perform stepweight decay\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 137\u001b[1;33m         \u001b[0mparam\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlr\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    138\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m         \u001b[0mbias_correction1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mbeta1\u001b[0m \u001b[1;33m**\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#torch.cuda.empty_cache()\n",
    "from tqdm import tqdm\n",
    "MODEL_PATH = f'{data_path}model_save/gpt2-qa2.pt'\n",
    "\n",
    "total_t0 = time.time()\n",
    "\n",
    "training_stats = []\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# try:\n",
    "#     checkpoint = torch.load(MODEL_PATH)\n",
    "#     model.load_state_dict(checkpoint['model_state_dict'])\n",
    "#     optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "#     epoch = checkpoint['epoch']\n",
    "#     loss = checkpoint['loss']\n",
    "# except:\n",
    "#     epoch = 0\n",
    "epoch = 0\n",
    "\n",
    "for epoch_i in range(epoch, epochs):\n",
    "\n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    total_train_loss = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for step, batch in enumerate(tqdm(train_dataloader)):\n",
    "\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        b_masks = batch[1].to(device)\n",
    "\n",
    "        model.zero_grad()        \n",
    "\n",
    "        outputs = model(  b_input_ids,\n",
    "                          labels=b_labels, #you're basically training the model to be able to output your inputs\n",
    "                          attention_mask = b_masks,\n",
    "                          token_type_ids=None\n",
    "                        )\n",
    "\n",
    "        loss = outputs[0]  \n",
    "\n",
    "        batch_loss = loss.item()\n",
    "        total_train_loss += batch_loss\n",
    "\n",
    "        # Get sample every x batches.\n",
    "        if step % sample_every == 0 and not step == 0:\n",
    "\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            print('  Batch {:>5,}  of  {:>5,}. Loss: {:>5,}.   Elapsed: {:}.'.format(step, len(train_dataloader), batch_loss, elapsed))\n",
    "\n",
    "            model.eval()\n",
    "\n",
    "            sample_outputs = model.generate(\n",
    "                                    bos_token_id=random.randint(1,30000),\n",
    "                                    do_sample=True,   \n",
    "                                    top_k=50, \n",
    "                                    max_length = 200,\n",
    "                                    top_p=0.95, \n",
    "                                    num_return_sequences=1\n",
    "                                )\n",
    "            for i, sample_output in enumerate(sample_outputs):\n",
    "                  print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))\n",
    "            \n",
    "            model.train()\n",
    "\n",
    "            # Create output directory if needed\n",
    "            if not os.path.exists(f\"{data_path}/model_save\"):\n",
    "                os.makedirs(f\"{data_path}/model_save\")\n",
    "\n",
    "            print(\"Saving model to %s\" % MODEL_PATH)\n",
    "\n",
    "            torch.save({\n",
    "              'epoch': epoch_i,\n",
    "              'model_state_dict': model.state_dict(),\n",
    "              'optimizer_state_dict': optimizer.state_dict(),\n",
    "              'loss': loss,\n",
    "            }, MODEL_PATH)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)       \n",
    "    \n",
    "    # Measure how long this epoch took.\n",
    "    training_time = format_time(time.time() - t0)\n",
    "\n",
    "    # print(\"\")\n",
    "    # print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    # print(\"  Training epoch took: {:}\".format(training_time))\n",
    "        \n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in validation_dataloader:\n",
    "        \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        b_masks = batch[1].to(device)\n",
    "        \n",
    "        with torch.no_grad():        \n",
    "\n",
    "            outputs  = model(b_input_ids, \n",
    "#                            token_type_ids=None, \n",
    "                             attention_mask = b_masks,\n",
    "                            labels=b_labels)\n",
    "          \n",
    "            loss = outputs[0]  \n",
    "            \n",
    "        batch_loss = loss.item()\n",
    "        total_eval_loss += batch_loss        \n",
    "\n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "    \n",
    "    validation_time = format_time(time.time() - t0)    \n",
    "\n",
    "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "    print(\"  Validation took: {:}\".format(validation_time))\n",
    "\n",
    "    # Record all statistics from this epoch.\n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Valid. Loss': avg_val_loss,\n",
    "            'Training Time': training_time,\n",
    "            'Validation Time': validation_time\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "Uyip-x1Clogy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to D:\\Python\\kaoculator\\\\model_save/gpt2-qa2.pt\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(f\"{data_path}/model_save\"):\n",
    "    os.makedirs(f\"{data_path}/model_save\")\n",
    "\n",
    "print(\"Saving model to %s\" % MODEL_PATH)\n",
    "\n",
    "torch.save({\n",
    "  'epoch': 5,\n",
    "  'model_state_dict': model.state_dict(),\n",
    "  'optimizer_state_dict': optimizer.state_dict(),\n",
    "  'loss': loss,\n",
    "}, MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VQTvJ1vRP7u4"
   },
   "source": [
    "Let's view the summary of the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 252
    },
    "id": "6O_NbXFGMukX",
    "outputId": "d20449f5-0720-4527-d437-f7b807ff663c"
   },
   "outputs": [],
   "source": [
    "# Display floats with two decimal places.\n",
    "pd.set_option('precision', 2)\n",
    "\n",
    "# Create a DataFrame from our training statistics.\n",
    "df_stats = pd.DataFrame(data=training_stats)\n",
    "\n",
    "# Use the 'epoch' as the row index.\n",
    "df_stats = df_stats.set_index('epoch')\n",
    "\n",
    "# A hack to force the column headers to wrap.\n",
    "#df = df.style.set_table_styles([dict(selector=\"th\",props=[('max-width', '70px')])])\n",
    "\n",
    "# Display the table.\n",
    "df_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 448
    },
    "id": "68xreA9JAmG5",
    "outputId": "c1321022-0133-4916-c1da-c11707572e6c"
   },
   "outputs": [],
   "source": [
    "# Use plot styling from seaborn.\n",
    "sns.set(style='darkgrid')\n",
    "\n",
    "# Increase the plot size and font size.\n",
    "sns.set(font_scale=1.5)\n",
    "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
    "\n",
    "# Plot the learning curve.\n",
    "plt.plot(df_stats['Training Loss'], 'b-o', label=\"Training\")\n",
    "plt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation\")\n",
    "\n",
    "# Label the plot.\n",
    "plt.title(\"Training & Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.xticks([1, 2, 3, 4])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GfjYoa6WmkN6"
   },
   "source": [
    "# Display Model Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8PIiVlDYCtSq",
    "outputId": "83594ae4-2836-42db-e64b-5fc2c7c530ce"
   },
   "outputs": [],
   "source": [
    "# Get all of the model's parameters as a list of tuples.\n",
    "params = list(model.named_parameters())\n",
    "\n",
    "print('The GPT-2 model has {:} different named parameters.\\n'.format(len(params)))\n",
    "\n",
    "print('==== Embedding Layer ====\\n')\n",
    "\n",
    "for p in params[0:2]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== First Transformer ====\\n')\n",
    "\n",
    "for p in params[2:14]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== Output Layer ====\\n')\n",
    "\n",
    "for p in params[-2:]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZLf6rbRglYhQ"
   },
   "source": [
    "# Generate Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v4XhewaV93-_",
    "outputId": "7368cc46-f03a-44b1-bdfb-8c5b4d633ee2"
   },
   "outputs": [],
   "source": [
    "def predict(model, prompt):\n",
    "    model.eval()\n",
    "\n",
    "    prompt = \"<|startoftext|>\" + prompt + \"<|answer|>\"\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    sample_outputs = model.generate(\n",
    "                                    **inputs, \n",
    "                                    #bos_token_id=random.randint(1,30000),\n",
    "                                    do_sample=True,   \n",
    "                                    # top_k=50, \n",
    "                                    max_length = 300,\n",
    "                                    top_p=0.1, \n",
    "                                    num_return_sequences=3\n",
    "                                    )\n",
    "\n",
    "    for i, sample_output in enumerate(sample_outputs):\n",
    "        print(\"{}: {}\\n\\n\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: Why is machine learning so hard <|answer|>\n",
      "\n",
      "\n",
      "1: Why is machine learning so hard <|answer|>\n",
      "\n",
      "\n",
      "2: Why is machine learning so hard <|answer|>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predict(model, \"Why is machine learning so hard\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t4LrX5H-0nAU"
   },
   "source": [
    "These aren't bad at all!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SMMoM-evQVeW"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
